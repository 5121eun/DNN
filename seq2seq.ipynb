{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b550053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import sequence\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c1fabd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seqlm:\n",
    "    \n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        \n",
    "        rn = np.random.randn\n",
    "        enc_embed = (rn(V, D) / 100).astype('f')\n",
    "        enc_lstmWx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        enc_lstmWh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        enc_lstmb = np.zeros(4 * H).astype('f')\n",
    "        \n",
    "        dec_embed = (rn(V, D) / 100).astype('f')\n",
    "        dec_lstmWx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        dec_lstmWh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        dec_lstmb = np.zeros(4 * H).astype('f')\n",
    "        dec_affineW = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        dec_affineb = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.params = [enc_embed, enc_lstmWx, enc_lstmWh, enc_lstmb, \\\n",
    "            dec_embed, dec_lstmWx, dec_lstmWh, dec_lstmb, dec_affineW, dec_affineb]\n",
    "        \n",
    "        self.grads = []\n",
    "        self.enc_lstm = [] \n",
    "        self.dec_lstm = []\n",
    "        \n",
    "        self.time_idx = 0\n",
    "        self.vocab_size = vocab_size\n",
    "        self.wordvec_size = wordvec_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "\n",
    "        \n",
    "    def forward(self, enc_xs, ts):\n",
    "        enc_embed, enc_lstmWx, enc_lstmWh, enc_lstmb, \\\n",
    "            dec_embed, dec_lstmWx, dec_lstmWh, dec_lstmb, dec_affineW, dec_affineb = self.params\n",
    "        \n",
    "        self.enc_lstm = []\n",
    "        self.dec_lstm = []\n",
    "        \n",
    "        dec_xs = ts[:, :-1]\n",
    "        dec_ts = ts[:, 1:]\n",
    "        \n",
    "        batch_size, enc_time_size = enc_xs.shape\n",
    "        dec_time_size = dec_xs.shape[1]\n",
    "        \n",
    "        hidden_size = self.hidden_size\n",
    "               \n",
    "        ####################### ENCODER #######################\n",
    "        \n",
    "        h_prev = np.zeros((batch_size, hidden_size), dtype='f')\n",
    "        c_prev = np.zeros((batch_size, hidden_size), dtype='f')\n",
    "        enc_hs = np.zeros((batch_size, enc_time_size, hidden_size), dtype='f')\n",
    "        for t in range(enc_time_size):\n",
    "            # embed\n",
    "            emb_out = enc_embed[enc_xs[:, t]]\n",
    "\n",
    "            # lstm\n",
    "            A = np.matmul(emb_out, enc_lstmWx) + np.matmul(h_prev, enc_lstmWh) + enc_lstmb\n",
    "            f = A[:, :hidden_size]\n",
    "            g = A[:, hidden_size: 2*hidden_size]\n",
    "            i = A[:, 2*hidden_size: 3*hidden_size]\n",
    "            o = A[:, 3*hidden_size:]\n",
    "\n",
    "            f = self.sigmoid(f)\n",
    "            g = np.tanh(g)\n",
    "            i = self.sigmoid(i)\n",
    "            o = self.sigmoid(o)\n",
    "\n",
    "            c_next = f * c_prev + g * i\n",
    "            h_next = o * np.tanh(c_next)\n",
    "            \n",
    "            enc_hs[:, t, :] = h_next\n",
    "            \n",
    "            self.enc_lstm.append((emb_out, h_prev, c_prev, f, g, i, o, c_next, h_next))\n",
    "            \n",
    "            c_prev = c_next\n",
    "            h_prev = h_next\n",
    "            \n",
    "        \n",
    "        ####################### DECODER #######################\n",
    "        \n",
    "        c_prev = np.zeros((batch_size, hidden_size), dtype='f')\n",
    "        dec_hs = np.zeros((batch_size, dec_time_size, hidden_size), dtype='f')\n",
    "        for t in range(dec_time_size):\n",
    "            # embed\n",
    "            emb_out = dec_embed[dec_xs[:, t]]\n",
    "            \n",
    "            # lstm\n",
    "            A = np.matmul(emb_out, dec_lstmWx) + np.matmul(h_prev, dec_lstmWh) + dec_lstmb\n",
    "            f = A[:, :hidden_size]\n",
    "            g = A[:, hidden_size: 2*hidden_size]\n",
    "            i = A[:, 2*hidden_size: 3*hidden_size]\n",
    "            o = A[:, 3*hidden_size:]\n",
    "\n",
    "            f = self.sigmoid(f)\n",
    "            g = np.tanh(g)\n",
    "            i = self.sigmoid(i)\n",
    "            o = self.sigmoid(o)\n",
    "\n",
    "            c_next = f * c_prev + g * i\n",
    "            h_next = o * np.tanh(c_next)\n",
    "            \n",
    "            dec_hs[:, t, :] = h_next\n",
    "            \n",
    "            self.dec_lstm.append((emb_out, h_prev, c_prev, f, g, i, o, c_next, h_next))\n",
    "            c_prev = c_next\n",
    "            h_prev = h_next\n",
    "            \n",
    "            \n",
    "        # affine\n",
    "        dec_hs = dec_hs.reshape(batch_size * dec_time_size, -1)\n",
    "        dec_affine_out = np.matmul(dec_hs, dec_affineW) + dec_affineb\n",
    "        \n",
    "        # softmax\n",
    "        y = self.softmax(dec_affine_out.reshape(batch_size, dec_time_size, -1))\n",
    "        loss = self.getLoss(y, dec_ts)\n",
    "        \n",
    "        self.xs = enc_xs, enc_hs, dec_xs, dec_hs, dec_affine_out, y\n",
    "        self.ts = dec_ts\n",
    "        return loss\n",
    "        \n",
    "    def backward(self):\n",
    "        enc_embed, enc_lstmWx, enc_lstmWh, enc_lstmb, \\\n",
    "            dec_embed, dec_lstmWx, dec_lstmWh, dec_lstmb, dec_affineW, dec_affineb = self.params\n",
    "        enc_xs, enc_hs, dec_xs, dec_hs, dec_affine_out, y = self.xs\n",
    "        dec_ts = self.ts\n",
    "        \n",
    "        vocab_size = self.vocab_size        \n",
    "        wordvec_size = self.wordvec_size\n",
    "        batch_size, enc_time_size = enc_xs.shape\n",
    "        dec_time_size = dec_xs.shape[1]\n",
    "        \n",
    "        ####################### DECODER #######################\n",
    "        \n",
    "        # softmax\n",
    "        y = y.reshape(batch_size * dec_time_size, -1)\n",
    "        y[np.arange(batch_size * dec_time_size), dec_ts.reshape(batch_size * dec_time_size)] -= 1\n",
    "        soft_dout = y\n",
    "        \n",
    "        # affine\n",
    "        dec_hs = dec_hs.reshape(batch_size * dec_time_size, -1)\n",
    "        affine_dout = np.matmul(soft_dout, dec_affineW.T).reshape(batch_size, dec_time_size, -1) # (b, t, h)\n",
    "        affinedW = np.matmul(dec_hs.T, soft_dout) # (h, v)\n",
    "        affinedb = np.sum(soft_dout, axis=0)       \n",
    "\n",
    "        # lstm\n",
    "        dec_lstmdWx = np.zeros_like(dec_lstmWx)\n",
    "        dec_lstmdWh = np.zeros_like(dec_lstmWh)\n",
    "        dec_lstmdb = np.zeros_like(dec_lstmb)\n",
    "        \n",
    "        dec_lstm_douts = np.empty((batch_size, dec_time_size, wordvec_size), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "        for t in reversed(range(dec_time_size)):            \n",
    "            emb_out, h_prev, c_prev, f, g, i, o, c_next, _ = self.dec_lstm[t]\n",
    "            dh_next = affine_dout[:, t, :] + dh\n",
    "            dc_next = dc\n",
    "\n",
    "            tanh_c_next = np.tanh(c_next)\n",
    "            \n",
    "            ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "            \n",
    "            dc_prev = ds * f\n",
    "            \n",
    "            di = ds * g\n",
    "            df = ds * c_prev\n",
    "            do = dh_next * tanh_c_next\n",
    "            dg = ds * i\n",
    "            \n",
    "            di *= i * (1 - i)\n",
    "            df *= f * (1 - f)\n",
    "            do *= o * (1 - o)\n",
    "            dg *= (1 - g ** 2)\n",
    "            \n",
    "            dA = np.hstack((df, dg, di, do))\n",
    "            \n",
    "            dWh = np.matmul(h_prev.T, dA)\n",
    "            dWx = np.matmul(emb_out.T, dA)\n",
    "            db = np.sum(dA, axis=0)\n",
    "            \n",
    "            dec_lstm_douts[:, t, :] = np.matmul(dA, dec_lstmWx.T)\n",
    "            dh_prev = np.matmul(dA, dec_lstmWh.T)\n",
    "        \n",
    "            dec_lstmdWx += dWx\n",
    "            dec_lstmdWh += dWh\n",
    "            dec_lstmdb += db\n",
    "            dh = dh_prev\n",
    "            dc = dc_prev\n",
    "            \n",
    "        # embed\n",
    "        dec_embed_dout = np.zeros_like(dec_embed)\n",
    "        for t in range(dec_time_size):\n",
    "            np.add.at(dec_embed_dout, dec_xs[:, t], dec_lstm_douts[:, t, :])\n",
    "            \n",
    "        ####################### ENCODER #######################\n",
    "        \n",
    "        # lstm\n",
    "        enc_lstmdWx = np.zeros_like(enc_lstmWx)\n",
    "        enc_lstmdWh = np.zeros_like(enc_lstmWh)\n",
    "        enc_lstmdb = np.zeros_like(enc_lstmb)\n",
    "        \n",
    "        enc_lstm_douts = np.empty((batch_size, enc_time_size, wordvec_size), dtype='f')\n",
    "        dc = 0\n",
    "        for t in reversed(range(enc_time_size)):            \n",
    "            emb_out, h_prev, c_prev, f, g, i, o, c_next, _ = self.enc_lstm[t]\n",
    "            dh_next = dh\n",
    "            dc_next = dc\n",
    "\n",
    "            tanh_c_next = np.tanh(c_next)\n",
    "            \n",
    "            ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "            \n",
    "            dc_prev = ds * f\n",
    "            \n",
    "            di = ds * g\n",
    "            df = ds * c_prev\n",
    "            do = dh_next * tanh_c_next\n",
    "            dg = ds * i\n",
    "            \n",
    "            di *= i * (1 - i)\n",
    "            df *= f * (1 - f)\n",
    "            do *= o * (1 - o)\n",
    "            dg *= (1 - g ** 2)\n",
    "            \n",
    "            dA = np.hstack((df, dg, di, do))\n",
    "            \n",
    "            dWh = np.matmul(h_prev.T, dA)\n",
    "            dWx = np.matmul(emb_out.T, dA)\n",
    "            db = np.sum(dA, axis=0)\n",
    "            \n",
    "            enc_lstm_douts[:, t, :] = np.matmul(dA, enc_lstmWx.T)\n",
    "            dh_prev = np.matmul(dA, enc_lstmWh.T)\n",
    "        \n",
    "            enc_lstmdWx += dWx\n",
    "            enc_lstmdWh += dWh\n",
    "            enc_lstmdb += db\n",
    "            dh = dh_prev\n",
    "            dc = dc_prev        \n",
    "        \n",
    "        # embed\n",
    "        enc_embed_dout = np.zeros_like(enc_embed)\n",
    "        for t in range(enc_time_size):\n",
    "            np.add.at(enc_embed_dout, enc_xs[:, t], enc_lstm_douts[:, t, :])\n",
    "        \n",
    "        self.grads = enc_embed_dout, enc_lstmdWx, enc_lstmdWh, enc_lstmdb, \\\n",
    "            dec_embed_dout, dec_lstmdWx, dec_lstmdWh, dec_lstmdb, affinedW, affinedb\n",
    "        \n",
    "    def softmax(self, y):\n",
    "        y = y - np.max(y)\n",
    "        y = np.exp(y)\n",
    "        y = y / y.sum(axis=-1, keepdims=True)\n",
    "        return y\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def getLoss(self, y, t):\n",
    "        N, T, V = y.shape\n",
    "\n",
    "        y = y.reshape(N * T, V)\n",
    "        t = t.reshape(N * T)\n",
    "\n",
    "        ls = np.log(y[np.arange(N * T), t])\n",
    "        return -np.sum(ls) / (N * T)\n",
    "    \n",
    "    def update(self, lr):\n",
    "        for i in range(len(self.params)):\n",
    "            self.params[i] -= self.grads[i] * lr\n",
    "            \n",
    "    def updateAdam(self, lr):\n",
    "        params = self.params\n",
    "        grads = self.grads\n",
    "        \n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "                \n",
    "        self.iter += 1\n",
    "        lr_t = lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "        \n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n",
    "        self.params = params\n",
    "        \n",
    "    def clip_grads(self, max_norm):\n",
    "        grads = self.grads\n",
    "        total_norm = 0\n",
    "        \n",
    "        for grad in grads:\n",
    "            total_norm += np.sum(grad**2)\n",
    "            \n",
    "        total_norm = np.sqrt(total_norm)\n",
    "        \n",
    "        rate = max_norm / (total_norm + 1e-6)\n",
    "        \n",
    "        if rate  < 1:\n",
    "            for grad in grads:\n",
    "                grad *= rate\n",
    "        self.grads = grads\n",
    "            \n",
    "    def generate(self, xs, start_id, sample_size):\n",
    "        enc_embed, enc_lstmWx, enc_lstmWh, enc_lstmb, \\\n",
    "            dec_embed, dec_lstmWx, dec_lstmWh, dec_lstmb, dec_affineW, dec_affineb = self.params\n",
    "        \n",
    "        \n",
    "        enc_time_size = xs.shape[1]\n",
    "        hidden_size = self.hidden_size\n",
    "        \n",
    "        ####################### ENCODER #######################\n",
    "        \n",
    "        h_prev = np.zeros((hidden_size), dtype='f')\n",
    "        c_prev = np.zeros((hidden_size), dtype='f')\n",
    "        for t in range(enc_time_size):\n",
    "            # encoder embed\n",
    "            emb_out = enc_embed[xs[:, t]]\n",
    "\n",
    "            # encoder lstm\n",
    "            A = np.matmul(emb_out, enc_lstmWx) + np.matmul(h_prev, enc_lstmWh) + enc_lstmb\n",
    "            f = A[:, :hidden_size]\n",
    "            g = A[:, hidden_size: 2*hidden_size]\n",
    "            i = A[:, 2*hidden_size: 3*hidden_size]\n",
    "            o = A[:, 3*hidden_size:]\n",
    "\n",
    "            f = self.sigmoid(f)\n",
    "            g = np.tanh(g)\n",
    "            i = self.sigmoid(i)\n",
    "            o = self.sigmoid(o)\n",
    "\n",
    "            c_next = f * c_prev + g * i\n",
    "            h_next = o * np.tanh(c_next)\n",
    "            \n",
    "            c_prev = c_next\n",
    "            h_prev = h_next\n",
    "                        \n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "\n",
    "        ####################### DECODER #######################\n",
    "\n",
    "        c_prev = np.zeros((hidden_size), dtype='f')\n",
    "        for _ in range(sample_size):\n",
    "            \n",
    "            # embed\n",
    "            emb_out = dec_embed[sample_id]\n",
    "\n",
    "            # lstm\n",
    "            A = np.matmul(emb_out, dec_lstmWx) + np.matmul(h_prev, dec_lstmWh) + dec_lstmb\n",
    "            f = A[:, :hidden_size]\n",
    "            g = A[:, hidden_size: 2*hidden_size]\n",
    "            i = A[:, 2*hidden_size: 3*hidden_size]\n",
    "            o = A[:, 3*hidden_size:]\n",
    "\n",
    "            f = self.sigmoid(f)\n",
    "            g = np.tanh(g)\n",
    "            i = self.sigmoid(i)\n",
    "            o = self.sigmoid(o)\n",
    "\n",
    "            c_next = f * c_prev + g * i\n",
    "            h_next = o * np.tanh(c_next)\n",
    "\n",
    "            c_prev = c_next\n",
    "            h_prev = h_next\n",
    "            \n",
    "            # affine\n",
    "            dec_affine_out = np.matmul(h_next, dec_affineW) + dec_affineb\n",
    "            \n",
    "            # softmax\n",
    "            y = self.softmax(dec_affine_out)\n",
    "            sample_id = np.argmax(y.flatten())\n",
    "            sampled.append(int(sample_id))\n",
    "        \n",
    "        return sampled\n",
    "            \n",
    "    def eval_seq2seq(self, question, correct, id_to_char,\n",
    "                     verbos=False, is_reverse=False):\n",
    "        correct = correct.flatten()\n",
    "        # 머릿글자\n",
    "        start_id = correct[0]\n",
    "        correct = correct[1:]\n",
    "        guess = self.generate(question, start_id, len(correct))\n",
    "\n",
    "        # 문자열로 변환\n",
    "        question = ''.join([id_to_char[int(c)] for c in question.flatten()])\n",
    "        correct = ''.join([id_to_char[int(c)] for c in correct])\n",
    "        guess = ''.join([id_to_char[int(c)] for c in guess])\n",
    "\n",
    "        if verbos:\n",
    "            if is_reverse:\n",
    "                question = question[::-1]\n",
    "\n",
    "            print('Q', question)\n",
    "            print('T', correct)\n",
    "\n",
    "            if correct == guess:\n",
    "                mark = 'O'\n",
    "                print(mark + ' ' + guess)\n",
    "            else:\n",
    "                mark = 'X'\n",
    "                print(mark + ' ' + guess)\n",
    "            print('---')\n",
    "\n",
    "        return 1 if guess == correct else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c6b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f77ac798",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 128\n",
    "batch_size = 128\n",
    "max_epoch = 5\n",
    "max_grad = 5.0\n",
    "data_size = len(x_train)\n",
    "learning_rate = 0.01\n",
    "data_size = len(x_train)\n",
    "max_iters = data_size // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87cfdf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2seqlm(vocab_size, wordvec_size, hidden_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0ffdf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 2.56\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 0[s] | 손실 2.05\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 0[s] | 손실 1.87\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 1[s] | 손실 1.81\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 1[s] | 손실 1.77\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 2[s] | 손실 1.74\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 2[s] | 손실 1.70\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 3[s] | 손실 1.60\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 3[s] | 손실 1.49\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 4[s] | 손실 1.39\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 4[s] | 손실 1.34\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 5[s] | 손실 1.31\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 5[s] | 손실 1.27\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 6[s] | 손실 1.25\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 6[s] | 손실 1.25\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 7[s] | 손실 1.20\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 7[s] | 손실 1.19\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 8[s] | 손실 1.17\n",
      "Q 77+85  \n",
      "T 162 \n",
      "X 155 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "X 1143\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "O 666 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "X 142 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "X 419 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "X 846 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "X 1043\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "X 1376\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "X 856 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "X 270 \n",
      "---\n",
      "검증 정확도2.100%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 11[s] | 손실 1.14\n",
      "| 에폭 2 |  반복 21 / 351 | 시간 12[s] | 손실 1.17\n",
      "| 에폭 2 |  반복 41 / 351 | 시간 12[s] | 손실 1.19\n",
      "| 에폭 2 |  반복 61 / 351 | 시간 13[s] | 손실 1.14\n",
      "| 에폭 2 |  반복 81 / 351 | 시간 14[s] | 손실 1.10\n",
      "| 에폭 2 |  반복 101 / 351 | 시간 14[s] | 손실 1.12\n",
      "| 에폭 2 |  반복 121 / 351 | 시간 15[s] | 손실 1.11\n",
      "| 에폭 2 |  반복 141 / 351 | 시간 15[s] | 손실 1.09\n",
      "| 에폭 2 |  반복 161 / 351 | 시간 16[s] | 손실 1.09\n",
      "| 에폭 2 |  반복 181 / 351 | 시간 16[s] | 손실 1.08\n",
      "| 에폭 2 |  반복 201 / 351 | 시간 17[s] | 손실 1.08\n",
      "| 에폭 2 |  반복 221 / 351 | 시간 17[s] | 손실 1.06\n",
      "| 에폭 2 |  반복 241 / 351 | 시간 18[s] | 손실 1.08\n",
      "| 에폭 2 |  반복 261 / 351 | 시간 18[s] | 손실 1.08\n",
      "| 에폭 2 |  반복 281 / 351 | 시간 19[s] | 손실 1.05\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 19[s] | 손실 1.04\n",
      "| 에폭 2 |  반복 321 / 351 | 시간 20[s] | 손실 1.02\n",
      "| 에폭 2 |  반복 341 / 351 | 시간 21[s] | 손실 1.03\n",
      "Q 77+85  \n",
      "T 162 \n",
      "X 161 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "X 1144\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "X 694 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "X 151 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "X 404 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "X 826 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "X 1049\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "X 1394\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "X 854 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "X 234 \n",
      "---\n",
      "검증 정확도3.840%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 24[s] | 손실 1.02\n",
      "| 에폭 3 |  반복 21 / 351 | 시간 25[s] | 손실 1.01\n",
      "| 에폭 3 |  반복 41 / 351 | 시간 25[s] | 손실 1.02\n",
      "| 에폭 3 |  반복 61 / 351 | 시간 26[s] | 손실 1.03\n",
      "| 에폭 3 |  반복 81 / 351 | 시간 26[s] | 손실 1.00\n",
      "| 에폭 3 |  반복 101 / 351 | 시간 27[s] | 손실 1.01\n",
      "| 에폭 3 |  반복 121 / 351 | 시간 28[s] | 손실 1.01\n",
      "| 에폭 3 |  반복 141 / 351 | 시간 28[s] | 손실 0.99\n",
      "| 에폭 3 |  반복 161 / 351 | 시간 29[s] | 손실 0.98\n",
      "| 에폭 3 |  반복 181 / 351 | 시간 29[s] | 손실 1.01\n",
      "| 에폭 3 |  반복 201 / 351 | 시간 30[s] | 손실 1.00\n",
      "| 에폭 3 |  반복 221 / 351 | 시간 30[s] | 손실 1.04\n",
      "| 에폭 3 |  반복 241 / 351 | 시간 31[s] | 손실 0.98\n",
      "| 에폭 3 |  반복 261 / 351 | 시간 32[s] | 손실 0.97\n",
      "| 에폭 3 |  반복 281 / 351 | 시간 32[s] | 손실 0.95\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 33[s] | 손실 0.99\n",
      "| 에폭 3 |  반복 321 / 351 | 시간 33[s] | 손실 0.98\n",
      "| 에폭 3 |  반복 341 / 351 | 시간 34[s] | 손실 0.96\n",
      "Q 77+85  \n",
      "T 162 \n",
      "X 161 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "X 1092\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "X 652 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "X 177 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "X 421 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "X 832 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "X 1002\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "X 1382\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "X 852 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "X 242 \n",
      "---\n",
      "검증 정확도2.860%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 38[s] | 손실 1.02\n",
      "| 에폭 4 |  반복 21 / 351 | 시간 38[s] | 손실 1.00\n",
      "| 에폭 4 |  반복 41 / 351 | 시간 39[s] | 손실 0.99\n",
      "| 에폭 4 |  반복 61 / 351 | 시간 39[s] | 손실 0.95\n",
      "| 에폭 4 |  반복 81 / 351 | 시간 40[s] | 손실 0.94\n",
      "| 에폭 4 |  반복 101 / 351 | 시간 40[s] | 손실 0.97\n",
      "| 에폭 4 |  반복 121 / 351 | 시간 41[s] | 손실 0.95\n",
      "| 에폭 4 |  반복 141 / 351 | 시간 42[s] | 손실 0.92\n",
      "| 에폭 4 |  반복 161 / 351 | 시간 42[s] | 손실 0.93\n",
      "| 에폭 4 |  반복 181 / 351 | 시간 43[s] | 손실 0.93\n",
      "| 에폭 4 |  반복 201 / 351 | 시간 43[s] | 손실 0.95\n",
      "| 에폭 4 |  반복 221 / 351 | 시간 44[s] | 손실 0.94\n",
      "| 에폭 4 |  반복 241 / 351 | 시간 45[s] | 손실 0.95\n",
      "| 에폭 4 |  반복 261 / 351 | 시간 45[s] | 손실 0.93\n",
      "| 에폭 4 |  반복 281 / 351 | 시간 46[s] | 손실 0.91\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 46[s] | 손실 0.93\n",
      "| 에폭 4 |  반복 321 / 351 | 시간 47[s] | 손실 0.97\n",
      "| 에폭 4 |  반복 341 / 351 | 시간 47[s] | 손실 0.95\n",
      "Q 77+85  \n",
      "T 162 \n",
      "X 164 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "X 1135\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "X 678 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "X 164 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "X 430 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "X 866 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "X 1059\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "X 1404\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "X 871 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "X 248 \n",
      "---\n",
      "검증 정확도4.680%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 51[s] | 손실 0.90\n",
      "| 에폭 5 |  반복 21 / 351 | 시간 52[s] | 손실 0.93\n",
      "| 에폭 5 |  반복 41 / 351 | 시간 52[s] | 손실 0.93\n",
      "| 에폭 5 |  반복 61 / 351 | 시간 53[s] | 손실 0.92\n",
      "| 에폭 5 |  반복 81 / 351 | 시간 54[s] | 손실 0.93\n",
      "| 에폭 5 |  반복 101 / 351 | 시간 54[s] | 손실 0.95\n",
      "| 에폭 5 |  반복 121 / 351 | 시간 55[s] | 손실 0.92\n",
      "| 에폭 5 |  반복 141 / 351 | 시간 56[s] | 손실 0.90\n",
      "| 에폭 5 |  반복 161 / 351 | 시간 56[s] | 손실 0.89\n",
      "| 에폭 5 |  반복 181 / 351 | 시간 57[s] | 손실 0.91\n",
      "| 에폭 5 |  반복 201 / 351 | 시간 57[s] | 손실 0.92\n",
      "| 에폭 5 |  반복 221 / 351 | 시간 58[s] | 손실 0.92\n",
      "| 에폭 5 |  반복 241 / 351 | 시간 59[s] | 손실 0.92\n",
      "| 에폭 5 |  반복 261 / 351 | 시간 59[s] | 손실 0.92\n",
      "| 에폭 5 |  반복 281 / 351 | 시간 60[s] | 손실 0.91\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 61[s] | 손실 0.89\n",
      "| 에폭 5 |  반복 321 / 351 | 시간 62[s] | 손실 0.91\n",
      "| 에폭 5 |  반복 341 / 351 | 시간 63[s] | 손실 0.89\n",
      "Q 77+85  \n",
      "T 162 \n",
      "O 162 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "X 1159\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "X 657 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "X 157 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "X 418 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "X 859 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "X 1059\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "X 1399\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "X 877 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "X 245 \n",
      "---\n",
      "검증 정확도5.440%\n"
     ]
    }
   ],
   "source": [
    "acc_list = []\n",
    "loss_list = []\n",
    "start_time = time.time()\n",
    "total_loss, loss_count = 0, 0\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    idx = np.random.permutation(np.arange(data_size))\n",
    "    x_train = x_train[idx]\n",
    "    t_train = t_train[idx]\n",
    "    \n",
    "    for iters in range(max_iters):\n",
    "        batch_x = x_train[(iters * batch_size) : (iters + 1) * batch_size]\n",
    "        batch_t = t_train[(iters * batch_size) : (iters + 1) * batch_size]\n",
    "        \n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        model.clip_grads(max_grad)\n",
    "        model.updateAdam(learning_rate)\n",
    "        \n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "        \n",
    "        if iters % 20 == 0:\n",
    "            avg_loss = total_loss / loss_count\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 손실 %.2f'\n",
    "                % (epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "            loss_list.append(avg_loss)\n",
    "            total_loss, loss_count = 0, 0\n",
    "            \n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += model.eval_seq2seq(question, correct, id_to_char, verbose)\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('검증 정확도%.3f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "093f0a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:, ::-1]\n",
    "x_test = x_test[:, ::-1]\n",
    "model = Seq2seqlm(vocab_size, wordvec_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "604234f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 2.56\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 0[s] | 손실 2.05\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 0[s] | 손실 1.86\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 1[s] | 손실 1.84\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 1[s] | 손실 1.77\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 2[s] | 손실 1.72\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 2[s] | 손실 1.68\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 3[s] | 손실 1.58\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 3[s] | 손실 1.47\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 4[s] | 손실 1.38\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 5[s] | 손실 1.32\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 5[s] | 손실 1.28\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 6[s] | 손실 1.23\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 6[s] | 손실 1.19\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 7[s] | 손실 1.16\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 7[s] | 손실 1.13\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 8[s] | 손실 1.09\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 8[s] | 손실 1.07\n",
      "Q   58+77\n",
      "T 162 \n",
      "O 162 \n",
      "---\n",
      "Q 461+579\n",
      "T 1139\n",
      "X 1173\n",
      "---\n",
      "Q  48+285\n",
      "T 666 \n",
      "X 650 \n",
      "---\n",
      "Q   551+8\n",
      "T 163 \n",
      "X 170 \n",
      "---\n",
      "Q  55+763\n",
      "T 422 \n",
      "X 429 \n",
      "---\n",
      "Q 752+006\n",
      "T 857 \n",
      "X 890 \n",
      "---\n",
      "Q 292+167\n",
      "T 1053\n",
      "X 1007\n",
      "---\n",
      "Q 795+038\n",
      "T 1427\n",
      "X 1372\n",
      "---\n",
      "Q  838+62\n",
      "T 864 \n",
      "X 855 \n",
      "---\n",
      "Q  39+341\n",
      "T 236 \n",
      "X 238 \n",
      "---\n",
      "검증 정확도3.240%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 12[s] | 손실 1.06\n",
      "| 에폭 2 |  반복 21 / 351 | 시간 13[s] | 손실 1.02\n",
      "| 에폭 2 |  반복 41 / 351 | 시간 13[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 61 / 351 | 시간 14[s] | 손실 0.95\n",
      "| 에폭 2 |  반복 81 / 351 | 시간 14[s] | 손실 0.92\n",
      "| 에폭 2 |  반복 101 / 351 | 시간 15[s] | 손실 0.93\n",
      "| 에폭 2 |  반복 121 / 351 | 시간 15[s] | 손실 0.89\n",
      "| 에폭 2 |  반복 141 / 351 | 시간 16[s] | 손실 0.89\n",
      "| 에폭 2 |  반복 161 / 351 | 시간 17[s] | 손실 0.86\n",
      "| 에폭 2 |  반복 181 / 351 | 시간 17[s] | 손실 0.83\n",
      "| 에폭 2 |  반복 201 / 351 | 시간 18[s] | 손실 0.83\n",
      "| 에폭 2 |  반복 221 / 351 | 시간 19[s] | 손실 0.77\n",
      "| 에폭 2 |  반복 241 / 351 | 시간 19[s] | 손실 0.74\n",
      "| 에폭 2 |  반복 261 / 351 | 시간 20[s] | 손실 0.75\n",
      "| 에폭 2 |  반복 281 / 351 | 시간 21[s] | 손실 0.73\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 21[s] | 손실 0.72\n",
      "| 에폭 2 |  반복 321 / 351 | 시간 22[s] | 손실 0.73\n",
      "| 에폭 2 |  반복 341 / 351 | 시간 23[s] | 손실 0.72\n",
      "Q   58+77\n",
      "T 162 \n",
      "O 162 \n",
      "---\n",
      "Q 461+579\n",
      "T 1139\n",
      "X 1144\n",
      "---\n",
      "Q  48+285\n",
      "T 666 \n",
      "X 667 \n",
      "---\n",
      "Q   551+8\n",
      "T 163 \n",
      "X 162 \n",
      "---\n",
      "Q  55+763\n",
      "T 422 \n",
      "O 422 \n",
      "---\n",
      "Q 752+006\n",
      "T 857 \n",
      "X 856 \n",
      "---\n",
      "Q 292+167\n",
      "T 1053\n",
      "X 1047\n",
      "---\n",
      "Q 795+038\n",
      "T 1427\n",
      "X 1428\n",
      "---\n",
      "Q  838+62\n",
      "T 864 \n",
      "X 867 \n",
      "---\n",
      "Q  39+341\n",
      "T 236 \n",
      "X 234 \n",
      "---\n",
      "검증 정확도11.860%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 28[s] | 손실 0.70\n",
      "| 에폭 3 |  반복 21 / 351 | 시간 29[s] | 손실 0.69\n",
      "| 에폭 3 |  반복 41 / 351 | 시간 29[s] | 손실 0.70\n",
      "| 에폭 3 |  반복 61 / 351 | 시간 30[s] | 손실 0.68\n",
      "| 에폭 3 |  반복 81 / 351 | 시간 31[s] | 손실 0.68\n",
      "| 에폭 3 |  반복 101 / 351 | 시간 31[s] | 손실 0.66\n",
      "| 에폭 3 |  반복 121 / 351 | 시간 32[s] | 손실 0.66\n",
      "| 에폭 3 |  반복 141 / 351 | 시간 33[s] | 손실 0.67\n",
      "| 에폭 3 |  반복 161 / 351 | 시간 33[s] | 손실 0.64\n",
      "| 에폭 3 |  반복 181 / 351 | 시간 34[s] | 손실 0.64\n",
      "| 에폭 3 |  반복 201 / 351 | 시간 35[s] | 손실 0.65\n",
      "| 에폭 3 |  반복 221 / 351 | 시간 36[s] | 손실 0.63\n",
      "| 에폭 3 |  반복 241 / 351 | 시간 36[s] | 손실 0.63\n",
      "| 에폭 3 |  반복 261 / 351 | 시간 37[s] | 손실 0.64\n",
      "| 에폭 3 |  반복 281 / 351 | 시간 39[s] | 손실 0.60\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 39[s] | 손실 0.60\n",
      "| 에폭 3 |  반복 321 / 351 | 시간 40[s] | 손실 0.63\n",
      "| 에폭 3 |  반복 341 / 351 | 시간 41[s] | 손실 0.62\n",
      "Q   58+77\n",
      "T 162 \n",
      "X 158 \n",
      "---\n",
      "Q 461+579\n",
      "T 1139\n",
      "X 1137\n",
      "---\n",
      "Q  48+285\n",
      "T 666 \n",
      "X 667 \n",
      "---\n",
      "Q   551+8\n",
      "T 163 \n",
      "X 160 \n",
      "---\n",
      "Q  55+763\n",
      "T 422 \n",
      "X 423 \n",
      "---\n",
      "Q 752+006\n",
      "T 857 \n",
      "O 857 \n",
      "---\n",
      "Q 292+167\n",
      "T 1053\n",
      "X 1050\n",
      "---\n",
      "Q 795+038\n",
      "T 1427\n",
      "X 1420\n",
      "---\n",
      "Q  838+62\n",
      "T 864 \n",
      "X 867 \n",
      "---\n",
      "Q  39+341\n",
      "T 236 \n",
      "O 236 \n",
      "---\n",
      "검증 정확도15.020%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 46[s] | 손실 0.61\n",
      "| 에폭 4 |  반복 21 / 351 | 시간 47[s] | 손실 0.60\n",
      "| 에폭 4 |  반복 41 / 351 | 시간 47[s] | 손실 0.60\n",
      "| 에폭 4 |  반복 61 / 351 | 시간 48[s] | 손실 0.59\n",
      "| 에폭 4 |  반복 81 / 351 | 시간 49[s] | 손실 0.61\n",
      "| 에폭 4 |  반복 101 / 351 | 시간 49[s] | 손실 0.60\n",
      "| 에폭 4 |  반복 121 / 351 | 시간 50[s] | 손실 0.58\n",
      "| 에폭 4 |  반복 141 / 351 | 시간 51[s] | 손실 0.58\n",
      "| 에폭 4 |  반복 161 / 351 | 시간 51[s] | 손실 0.60\n",
      "| 에폭 4 |  반복 181 / 351 | 시간 52[s] | 손실 0.59\n",
      "| 에폭 4 |  반복 201 / 351 | 시간 53[s] | 손실 0.57\n",
      "| 에폭 4 |  반복 221 / 351 | 시간 53[s] | 손실 0.56\n",
      "| 에폭 4 |  반복 241 / 351 | 시간 54[s] | 손실 0.56\n",
      "| 에폭 4 |  반복 261 / 351 | 시간 55[s] | 손실 0.59\n",
      "| 에폭 4 |  반복 281 / 351 | 시간 55[s] | 손실 0.57\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 56[s] | 손실 0.55\n",
      "| 에폭 4 |  반복 321 / 351 | 시간 57[s] | 손실 0.57\n",
      "| 에폭 4 |  반복 341 / 351 | 시간 57[s] | 손실 0.55\n",
      "Q   58+77\n",
      "T 162 \n",
      "X 160 \n",
      "---\n",
      "Q 461+579\n",
      "T 1139\n",
      "X 1141\n",
      "---\n",
      "Q  48+285\n",
      "T 666 \n",
      "O 666 \n",
      "---\n",
      "Q   551+8\n",
      "T 163 \n",
      "X 162 \n",
      "---\n",
      "Q  55+763\n",
      "T 422 \n",
      "O 422 \n",
      "---\n",
      "Q 752+006\n",
      "T 857 \n",
      "X 856 \n",
      "---\n",
      "Q 292+167\n",
      "T 1053\n",
      "O 1053\n",
      "---\n",
      "Q 795+038\n",
      "T 1427\n",
      "X 1428\n",
      "---\n",
      "Q  838+62\n",
      "T 864 \n",
      "X 862 \n",
      "---\n",
      "Q  39+341\n",
      "T 236 \n",
      "X 239 \n",
      "---\n",
      "검증 정확도20.480%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 62[s] | 손실 0.54\n",
      "| 에폭 5 |  반복 21 / 351 | 시간 62[s] | 손실 0.52\n",
      "| 에폭 5 |  반복 41 / 351 | 시간 63[s] | 손실 0.54\n",
      "| 에폭 5 |  반복 61 / 351 | 시간 64[s] | 손실 0.53\n",
      "| 에폭 5 |  반복 81 / 351 | 시간 64[s] | 손실 0.50\n",
      "| 에폭 5 |  반복 101 / 351 | 시간 65[s] | 손실 0.55\n",
      "| 에폭 5 |  반복 121 / 351 | 시간 66[s] | 손실 0.54\n",
      "| 에폭 5 |  반복 141 / 351 | 시간 66[s] | 손실 0.53\n",
      "| 에폭 5 |  반복 161 / 351 | 시간 67[s] | 손실 0.53\n",
      "| 에폭 5 |  반복 181 / 351 | 시간 68[s] | 손실 0.53\n",
      "| 에폭 5 |  반복 201 / 351 | 시간 68[s] | 손실 0.51\n",
      "| 에폭 5 |  반복 221 / 351 | 시간 69[s] | 손실 0.53\n",
      "| 에폭 5 |  반복 241 / 351 | 시간 70[s] | 손실 0.52\n",
      "| 에폭 5 |  반복 261 / 351 | 시간 70[s] | 손실 0.53\n",
      "| 에폭 5 |  반복 281 / 351 | 시간 71[s] | 손실 0.52\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 72[s] | 손실 0.52\n",
      "| 에폭 5 |  반복 321 / 351 | 시간 72[s] | 손실 0.50\n",
      "| 에폭 5 |  반복 341 / 351 | 시간 73[s] | 손실 0.57\n",
      "Q   58+77\n",
      "T 162 \n",
      "X 160 \n",
      "---\n",
      "Q 461+579\n",
      "T 1139\n",
      "X 1136\n",
      "---\n",
      "Q  48+285\n",
      "T 666 \n",
      "O 666 \n",
      "---\n",
      "Q   551+8\n",
      "T 163 \n",
      "O 163 \n",
      "---\n",
      "Q  55+763\n",
      "T 422 \n",
      "O 422 \n",
      "---\n",
      "Q 752+006\n",
      "T 857 \n",
      "X 856 \n",
      "---\n",
      "Q 292+167\n",
      "T 1053\n",
      "X 1052\n",
      "---\n",
      "Q 795+038\n",
      "T 1427\n",
      "X 1426\n",
      "---\n",
      "Q  838+62\n",
      "T 864 \n",
      "X 862 \n",
      "---\n",
      "Q  39+341\n",
      "T 236 \n",
      "X 234 \n",
      "---\n",
      "검증 정확도21.960%\n"
     ]
    }
   ],
   "source": [
    "reverse_acc_list = []\n",
    "loss_list = []\n",
    "start_time = time.time()\n",
    "total_loss, loss_count = 0, 0\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    idx = np.random.permutation(np.arange(data_size))\n",
    "    x_train = x_train[idx]\n",
    "    t_train = t_train[idx]\n",
    "    \n",
    "    for iters in range(max_iters):\n",
    "        batch_x = x_train[(iters * batch_size) : (iters + 1) * batch_size]\n",
    "        batch_t = t_train[(iters * batch_size) : (iters + 1) * batch_size]\n",
    "        \n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        model.clip_grads(max_grad)\n",
    "        model.updateAdam(learning_rate)\n",
    "        \n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "        \n",
    "        if iters % 20 == 0:\n",
    "            avg_loss = total_loss / loss_count\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 손실 %.2f'\n",
    "                % (epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "            loss_list.append(avg_loss)\n",
    "            total_loss, loss_count = 0, 0\n",
    "            \n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += model.eval_seq2seq(question, correct, id_to_char, verbose)\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    reverse_acc_list.append(acc)\n",
    "    print('검증 정확도%.3f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b753cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = np.arange(len(acc_list))\n",
    "reverse_acc = np.arange(len(reverse_acc_list))\n",
    "plt.plot(acc, acc_list, reverse_acc, reverse_acc_list, label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
