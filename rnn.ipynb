{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "901b4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from dataset import ptb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5923d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rnnlm:\n",
    "    \n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        \n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed = (rn(V, D) / 100).astype('f')\n",
    "        rnnWx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
    "        rnnWh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
    "        rnnb = np.zeros(H).astype('f')\n",
    "        affineW = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affineb = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.params = [embed, rnnWx, rnnWh, rnnb, affineW, affineb]\n",
    "        self.grads = []\n",
    "        self.rnn = []\n",
    "        \n",
    "        self.wordvec_size = wordvec_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.time_idx = 0\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        embed, rnnWx, rnnWh, rnnb, affineW, affineb = self.params\n",
    "                \n",
    "        batch_size, time_size = x.shape\n",
    "        wordvec_size = self.wordvec_size\n",
    "        hidden_size = self.hidden_size\n",
    "        \n",
    "        self.rnn = []\n",
    "        \n",
    "        h_prev = np.zeros((batch_size, hidden_size), dtype='f')\n",
    "        hs = np.empty((batch_size, time_size, hidden_size), dtype='f')\n",
    "        for t in range(time_size):\n",
    "            # embed\n",
    "            emb_out = embed[x[:, t]]\n",
    "            \n",
    "            #rnn\n",
    "            h_next = np.matmul(h_prev, rnnWh) + np.matmul(emb_out, rnnWx) + rnnb\n",
    "            h_next = np.tanh(h_next)\n",
    "            \n",
    "            self.rnn.append([emb_out, h_prev, h_next])\n",
    "            h_prev = h_next\n",
    "            hs[:, t, :] = h_prev\n",
    "        \n",
    "        # affine\n",
    "        affine_out = np.matmul(hs, affineW) + affineb\n",
    "        \n",
    "        # softmax\n",
    "        y = self.softmax(affine_out)\n",
    "        \n",
    "        loss = self.getLoss(y, batch_t)\n",
    "        self.xs = x, hs, affine_out, y\n",
    "\n",
    "        return y, loss\n",
    "    \n",
    "    def backward(self, t):\n",
    "        embed, rnnWx, rnnWh, rnnb, affineW, affineb = self.params\n",
    "        x, hs, affine_out, y = self.xs\n",
    "        \n",
    "        wordvec_size = self.wordvec_size\n",
    "        vocab_size = self.vocab_size\n",
    "        batch_size, time_size = x.shape\n",
    "        \n",
    "        # softmax\n",
    "        y = y.reshape(batch_size * time_size, -1)\n",
    "        t = t.reshape(batch_size * time_size)\n",
    "        y[np.arange(batch_size * time_size), t] -= 1\n",
    "        soft_dout = y\n",
    "        \n",
    "        # affine\n",
    "        affine_dout = np.matmul(soft_dout, affineW.T).reshape(batch_size, time_size, -1) \n",
    "        affinedW = np.matmul(hs.reshape(batch_size * time_size, -1).T, soft_dout) \n",
    "        affinedb = np.sum(soft_dout, axis=0)\n",
    "        \n",
    "        # rnn\n",
    "        rnndWx = np.zeros_like(rnnWx)\n",
    "        rnndWh = np.zeros_like(rnnWh)\n",
    "        rnndb = np.zeros_like(rnnb)\n",
    "        \n",
    "        rnn_douts = np.empty((batch_size, time_size, wordvec_size), dtype='f')\n",
    "        dh = 0\n",
    "        for t in reversed(range(time_size)):\n",
    "            emb_out, h_prev, h_next = self.rnn[t]\n",
    "            \n",
    "            dh_next = affine_dout[:, t, :] + dh\n",
    "            tanh_dout = dh_next * (1 - h_next ** 2)\n",
    "            \n",
    "            dWx = np.matmul(emb_out.T, tanh_dout)\n",
    "            rnn_dout = np.matmul(tanh_dout, rnnWx.T)\n",
    "            rnn_douts[:, t, :] = rnn_dout\n",
    "            \n",
    "            dWh = np.matmul(h_prev.T, tanh_dout)\n",
    "            dh_prev = np.matmul(tanh_dout, rnnWh.T)\n",
    "            \n",
    "            db = np.sum(tanh_dout, axis=0)\n",
    "            \n",
    "            rnndWx += dWx\n",
    "            rnndWh += dWh\n",
    "            rnndb += db\n",
    "            \n",
    "            dh = dh_prev\n",
    "        \n",
    "        # embed\n",
    "        embed_dout = np.zeros_like(embed)\n",
    "        for t in range(time_size):\n",
    "            np.add.at(embed_dout, x[:, t], rnn_douts[:, t, :])\n",
    "        \n",
    "        self.grads = embed_dout, rnndWx, rnndWh, rnndb, affinedW, affinedb\n",
    "        \n",
    "        \n",
    "    def softmax(self, y):\n",
    "        y = y - np.max(y)\n",
    "        y = np.exp(y)\n",
    "        y = y / y.sum(axis=2, keepdims=True)\n",
    "        return y\n",
    "    \n",
    "    def update(self, lr):\n",
    "        for i in range(len(self.params)):\n",
    "            self.params[i] -= self.grads[i] * lr      \n",
    "            \n",
    "    def getLoss(self, y, t):\n",
    "        N, T, V = y.shape\n",
    "\n",
    "        y = y.reshape(N * T, V)\n",
    "        t = t.reshape(N * T)\n",
    "\n",
    "        ls = np.log(y[np.arange(N * T), t])\n",
    "        return -np.sum(ls) / (N * T)\n",
    "    \n",
    "    def get_batch(self, x, t, batch_size, time_size):\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "\n",
    "        data_size = len(x)\n",
    "        jump = data_size // batch_size\n",
    "        offsets = [i * jump for i in range(batch_size)]\n",
    "\n",
    "        for time in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, time] = x[(offset + self.time_idx) % data_size]\n",
    "                batch_t[i, time] = t[(offset + self.time_idx) % data_size]\n",
    "            self.time_idx += 1\n",
    "        return batch_x, batch_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b52e1b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_size = 1000  \n",
    "corpus = corpus[:corpus_size]\n",
    "vocab_size = int(max(corpus) + 1)\n",
    "xs = corpus[:-1] \n",
    "ts = corpus[1:]  \n",
    "\n",
    "batch_size = 10\n",
    "wordvec_size = 100\n",
    "hidden_size = 100  \n",
    "time_size = 5  \n",
    "lr = 0.01\n",
    "max_epoch = 100\n",
    "max_iters = len(xs) // (batch_size * time_size)\n",
    "eval_interval = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b15a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Rnnlm(vocab_size, wordvec_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2cd95d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 | time 0[s] | loss 6.03 | ppl 417.69\n",
      "| epoch 2 | time 0[s] | loss 5.25 | ppl 307.44\n",
      "| epoch 3 | time 0[s] | loss 5.61 | ppl 236.34\n",
      "| epoch 4 | time 0[s] | loss 5.53 | ppl 214.06\n",
      "| epoch 5 | time 0[s] | loss 5.07 | ppl 203.11\n",
      "| epoch 6 | time 0[s] | loss 5.26 | ppl 192.38\n",
      "| epoch 7 | time 1[s] | loss 5.16 | ppl 176.55\n",
      "| epoch 8 | time 1[s] | loss 5.29 | ppl 167.06\n",
      "| epoch 9 | time 1[s] | loss 5.07 | ppl 152.21\n",
      "| epoch 10 | time 1[s] | loss 5.04 | ppl 138.19\n",
      "| epoch 11 | time 1[s] | loss 4.89 | ppl 124.06\n",
      "| epoch 12 | time 1[s] | loss 4.86 | ppl 114.28\n",
      "| epoch 13 | time 1[s] | loss 4.72 | ppl 104.96\n",
      "| epoch 14 | time 2[s] | loss 4.51 | ppl 89.09\n",
      "| epoch 15 | time 2[s] | loss 4.11 | ppl 76.86\n",
      "| epoch 16 | time 2[s] | loss 4.13 | ppl 69.69\n",
      "| epoch 17 | time 2[s] | loss 4.11 | ppl 60.19\n",
      "| epoch 18 | time 2[s] | loss 4.24 | ppl 52.98\n",
      "| epoch 19 | time 2[s] | loss 4.00 | ppl 42.30\n",
      "| epoch 20 | time 3[s] | loss 3.78 | ppl 37.76\n",
      "| epoch 21 | time 3[s] | loss 3.41 | ppl 33.22\n",
      "| epoch 22 | time 3[s] | loss 3.01 | ppl 28.60\n",
      "| epoch 23 | time 3[s] | loss 2.82 | ppl 25.54\n",
      "| epoch 24 | time 3[s] | loss 2.88 | ppl 21.80\n",
      "| epoch 25 | time 3[s] | loss 2.68 | ppl 19.11\n",
      "| epoch 26 | time 3[s] | loss 2.63 | ppl 17.51\n",
      "| epoch 27 | time 4[s] | loss 2.28 | ppl 14.71\n",
      "| epoch 28 | time 4[s] | loss 2.58 | ppl 13.39\n",
      "| epoch 29 | time 4[s] | loss 2.54 | ppl 11.69\n",
      "| epoch 30 | time 4[s] | loss 2.33 | ppl 10.26\n",
      "| epoch 31 | time 4[s] | loss 2.16 | ppl 9.36\n",
      "| epoch 32 | time 4[s] | loss 2.15 | ppl 8.33\n",
      "| epoch 33 | time 5[s] | loss 1.89 | ppl 7.53\n",
      "| epoch 34 | time 5[s] | loss 1.97 | ppl 6.89\n",
      "| epoch 35 | time 5[s] | loss 1.85 | ppl 6.28\n",
      "| epoch 36 | time 5[s] | loss 1.64 | ppl 5.80\n",
      "| epoch 37 | time 5[s] | loss 1.83 | ppl 5.42\n",
      "| epoch 38 | time 5[s] | loss 1.68 | ppl 4.99\n",
      "| epoch 39 | time 6[s] | loss 1.45 | ppl 4.53\n",
      "| epoch 40 | time 6[s] | loss 1.55 | ppl 4.31\n",
      "| epoch 41 | time 6[s] | loss 1.54 | ppl 4.05\n",
      "| epoch 42 | time 6[s] | loss 1.47 | ppl 3.81\n",
      "| epoch 43 | time 6[s] | loss 1.30 | ppl 3.49\n",
      "| epoch 44 | time 6[s] | loss 1.16 | ppl 3.48\n",
      "| epoch 45 | time 7[s] | loss 1.17 | ppl 3.42\n",
      "| epoch 46 | time 7[s] | loss 1.15 | ppl 3.06\n",
      "| epoch 47 | time 7[s] | loss 1.13 | ppl 2.98\n",
      "| epoch 48 | time 7[s] | loss 1.06 | ppl 2.93\n",
      "| epoch 49 | time 7[s] | loss 1.12 | ppl 2.72\n",
      "| epoch 50 | time 7[s] | loss 1.04 | ppl 2.80\n",
      "| epoch 51 | time 7[s] | loss 0.96 | ppl 2.53\n",
      "| epoch 52 | time 8[s] | loss 1.06 | ppl 2.57\n",
      "| epoch 53 | time 8[s] | loss 0.77 | ppl 2.48\n",
      "| epoch 54 | time 8[s] | loss 0.86 | ppl 2.52\n",
      "| epoch 55 | time 8[s] | loss 0.76 | ppl 2.40\n",
      "| epoch 56 | time 8[s] | loss 0.75 | ppl 2.33\n",
      "| epoch 57 | time 9[s] | loss 0.95 | ppl 2.28\n",
      "| epoch 58 | time 9[s] | loss 0.77 | ppl 2.18\n",
      "| epoch 59 | time 9[s] | loss 0.87 | ppl 2.24\n",
      "| epoch 60 | time 9[s] | loss 0.97 | ppl 2.30\n",
      "| epoch 61 | time 10[s] | loss 0.64 | ppl 2.08\n",
      "| epoch 62 | time 10[s] | loss 0.71 | ppl 2.03\n",
      "| epoch 63 | time 10[s] | loss 0.71 | ppl 2.09\n",
      "| epoch 64 | time 10[s] | loss 0.89 | ppl 2.12\n",
      "| epoch 65 | time 10[s] | loss 0.67 | ppl 2.04\n",
      "| epoch 66 | time 10[s] | loss 0.71 | ppl 1.96\n",
      "| epoch 67 | time 11[s] | loss 0.76 | ppl 1.94\n",
      "| epoch 68 | time 11[s] | loss 0.54 | ppl 1.97\n",
      "| epoch 69 | time 12[s] | loss 0.73 | ppl 2.04\n",
      "| epoch 70 | time 12[s] | loss 0.51 | ppl 1.89\n",
      "| epoch 71 | time 12[s] | loss 0.53 | ppl 1.87\n",
      "| epoch 72 | time 12[s] | loss 0.61 | ppl 1.90\n",
      "| epoch 73 | time 13[s] | loss 0.71 | ppl 1.91\n",
      "| epoch 74 | time 13[s] | loss 0.75 | ppl 1.92\n",
      "| epoch 75 | time 13[s] | loss 0.83 | ppl 1.90\n",
      "| epoch 76 | time 13[s] | loss 0.63 | ppl 1.82\n",
      "| epoch 77 | time 14[s] | loss 0.69 | ppl 1.79\n",
      "| epoch 78 | time 14[s] | loss 0.74 | ppl 1.89\n",
      "| epoch 79 | time 14[s] | loss 0.62 | ppl 1.84\n",
      "| epoch 80 | time 14[s] | loss 0.66 | ppl 1.80\n",
      "| epoch 81 | time 15[s] | loss 0.72 | ppl 1.75\n",
      "| epoch 82 | time 15[s] | loss 0.77 | ppl 1.84\n",
      "| epoch 83 | time 15[s] | loss 0.49 | ppl 1.79\n",
      "| epoch 84 | time 16[s] | loss 0.40 | ppl 1.79\n",
      "| epoch 85 | time 16[s] | loss 0.46 | ppl 1.75\n",
      "| epoch 86 | time 16[s] | loss 0.44 | ppl 1.72\n",
      "| epoch 87 | time 17[s] | loss 0.65 | ppl 1.80\n",
      "| epoch 88 | time 17[s] | loss 0.74 | ppl 1.80\n",
      "| epoch 89 | time 17[s] | loss 0.37 | ppl 1.73\n",
      "| epoch 90 | time 18[s] | loss 0.44 | ppl 1.74\n",
      "| epoch 91 | time 18[s] | loss 0.64 | ppl 1.79\n",
      "| epoch 92 | time 18[s] | loss 0.57 | ppl 1.72\n",
      "| epoch 93 | time 18[s] | loss 0.63 | ppl 1.80\n",
      "| epoch 94 | time 19[s] | loss 0.61 | ppl 1.76\n",
      "| epoch 95 | time 19[s] | loss 0.28 | ppl 1.66\n",
      "| epoch 96 | time 19[s] | loss 0.76 | ppl 1.71\n",
      "| epoch 97 | time 19[s] | loss 0.56 | ppl 1.71\n",
      "| epoch 98 | time 19[s] | loss 0.44 | ppl 1.77\n",
      "| epoch 99 | time 19[s] | loss 0.55 | ppl 1.72\n",
      "| epoch 100 | time 20[s] | loss 0.50 | ppl 1.65\n"
     ]
    }
   ],
   "source": [
    "ppl_list = []\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "start_time = time.time()\n",
    "model.time_idx = 0\n",
    "for epoch in range(max_epoch):\n",
    "    for iters in range(max_iters):\n",
    "        batch_x, batch_t = model.get_batch(xs, ts, batch_size, time_size)\n",
    "\n",
    "        y, loss = model.forward(batch_x, batch_t)\n",
    "        model.backward(batch_t)\n",
    "        model.update(lr)\n",
    "        \n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "        if ((iters % eval_interval) == 0):\n",
    "            ppl = np.exp(total_loss / loss_count)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('| epoch %d | time %d[s] | loss %.2f | ppl %.2f'\n",
    "                % (epoch + 1, elapsed_time, loss, ppl))\n",
    "            ppl_list.append(float(ppl))\n",
    "            total_loss, loss_count = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10200c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(ppl_list))\n",
    "plt.plot(x, ppl_list, label='train')\n",
    "plt.xlabel('iters (x' + str(eval_interval) + ')')\n",
    "plt.ylabel('ppl')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
