{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "901b4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from dataset import ptb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5923d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rnnlm:\n",
    "    \n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        \n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed = (rn(V, D) / 100).astype('f')\n",
    "        rnnWx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
    "        rnnWh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
    "        rnnb = np.zeros(H).astype('f')\n",
    "        affineW = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affineb = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.params = [embed, rnnWx, rnnWh, rnnb, affineW, affineb]\n",
    "        self.grads = []\n",
    "        self.rnn = []\n",
    "        \n",
    "        self.wordvec_size = wordvec_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.time_idx = 0\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        embed, rnnWx, rnnWh, rnnb, affineW, affineb = self.params\n",
    "                \n",
    "        batch_size, time_size = x.shape\n",
    "        wordvec_size = self.wordvec_size\n",
    "        hidden_size = self.hidden_size\n",
    "        \n",
    "        self.rnn = []\n",
    "        \n",
    "        h_prev = np.zeros((batch_size, hidden_size), dtype='f')\n",
    "        hs = np.empty((batch_size, time_size, hidden_size), dtype='f')\n",
    "        for t in range(time_size):\n",
    "            # embed\n",
    "            emb_out = embed[x[:, t]]\n",
    "            \n",
    "            #rnn\n",
    "            h_next = np.matmul(h_prev, rnnWh) + np.matmul(emb_out, rnnWx) + rnnb\n",
    "            h_next = np.tanh(h_next)\n",
    "            \n",
    "            self.rnn.append([emb_out, h_prev, h_next])\n",
    "            h_prev = h_next\n",
    "            hs[:, t, :] = h_prev\n",
    "        \n",
    "        # affine\n",
    "        affine_out = np.matmul(hs, affineW) + affineb\n",
    "        \n",
    "        # softmax\n",
    "        y = self.softmax(affine_out)\n",
    "        \n",
    "        loss = self.getLoss(y, batch_t)\n",
    "        self.xs = x, hs, affine_out, y\n",
    "\n",
    "        return y, loss\n",
    "    \n",
    "    def backward(self, t):\n",
    "        embed, rnnWx, rnnWh, rnnb, affineW, affineb = self.params\n",
    "        x, hs, affine_out, y = self.xs\n",
    "        \n",
    "        wordvec_size = self.wordvec_size\n",
    "        vocab_size = self.vocab_size\n",
    "        batch_size, time_size = x.shape\n",
    "        \n",
    "        # softmax\n",
    "        y = y.reshape(batch_size * time_size, -1)\n",
    "        t = t.reshape(batch_size * time_size)\n",
    "        y[np.arange(batch_size * time_size), t] -= 1\n",
    "        soft_dout = y\n",
    "        \n",
    "        # affine\n",
    "        affine_dout = np.matmul(soft_dout, affineW.T).reshape(batch_size, time_size, -1) # (b, t, h)\n",
    "        affinedW = np.matmul(hs.reshape(batch_size * time_size, -1).T, soft_dout) # (h, v)\n",
    "        affinedb = np.sum(soft_dout, axis=0)\n",
    "        \n",
    "        # rnn\n",
    "        rnndWx = np.zeros_like(rnnWx)\n",
    "        rnndWh = np.zeros_like(rnnWh)\n",
    "        rnndb = np.zeros_like(rnnb)\n",
    "        \n",
    "        rnn_douts = np.empty((batch_size, time_size, wordvec_size), dtype='f')\n",
    "        dh = 0\n",
    "        for t in reversed(range(time_size)):\n",
    "            emb_out, h_prev, h_next = self.rnn[t]\n",
    "            \n",
    "            dh_next = affine_dout[:, t, :] + dh\n",
    "            tanh_dout = dh_next * (1 - h_next ** 2)\n",
    "            \n",
    "            dWx = np.matmul(emb_out.T, tanh_dout)\n",
    "            rnn_dout = np.matmul(tanh_dout, rnnWx.T)\n",
    "            rnn_douts[:, t, :] = rnn_dout\n",
    "            \n",
    "            dWh = np.matmul(h_prev.T, tanh_dout)\n",
    "            dh_prev = np.matmul(tanh_dout, rnnWh.T)\n",
    "            \n",
    "            db = np.sum(tanh_dout, axis=0)\n",
    "            \n",
    "            rnndWx += dWx\n",
    "            rnndWh += dWh\n",
    "            rnndb += db\n",
    "            \n",
    "            dh = dh_prev\n",
    "        \n",
    "        # embed\n",
    "        embed_dout = np.zeros_like(embed)\n",
    "        for t in range(time_size):\n",
    "            np.add.at(embed_dout, x[:, t], rnn_douts[:, t, :])\n",
    "        \n",
    "        self.grads = embed_dout, rnndWx, rnndWh, rnndb, affinedW, affinedb\n",
    "        \n",
    "        \n",
    "    def softmax(self, y):\n",
    "        y = y - np.max(y)\n",
    "        y = np.exp(y)\n",
    "        y = y / y.sum(axis=2, keepdims=True)\n",
    "        return y\n",
    "    \n",
    "    def update(self, lr):\n",
    "        for i in range(len(self.params)):\n",
    "            self.params[i] -= self.grads[i] * lr      \n",
    "            \n",
    "    def getLoss(self, y, t):\n",
    "        N, T, V = y.shape\n",
    "\n",
    "        y = y.reshape(N * T, V)\n",
    "        t = t.reshape(N * T)\n",
    "\n",
    "        ls = np.log(y[np.arange(N * T), t])\n",
    "        return -np.sum(ls) / (N * T)\n",
    "    \n",
    "    def get_batch(self, x, t, batch_size, time_size):\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "\n",
    "        data_size = len(x)\n",
    "        jump = data_size // batch_size\n",
    "        offsets = [i * jump for i in range(batch_size)]  # 배치에서 각 샘플을 읽기 시작하는 위치\n",
    "\n",
    "        for time in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, time] = x[(offset + self.time_idx) % data_size]\n",
    "                batch_t[i, time] = t[(offset + self.time_idx) % data_size]\n",
    "            self.time_idx += 1\n",
    "        return batch_x, batch_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b52e1b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_size = 1000  # 테스트 데이터셋을 작게 설정\n",
    "corpus = corpus[:corpus_size]\n",
    "vocab_size = int(max(corpus) + 1)\n",
    "xs = corpus[:-1]  # 입력\n",
    "ts = corpus[1:]  # 출력（정답 레이블）\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 10\n",
    "wordvec_size = 100\n",
    "hidden_size = 100  # RNN의 은닉 상태 벡터의 원소 수\n",
    "time_size = 5  # RNN을 펼치는 크기\n",
    "lr = 0.01\n",
    "max_epoch = 100\n",
    "max_iters = len(xs) // (batch_size * time_size)\n",
    "eval_interval = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b15a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Rnnlm(vocab_size, wordvec_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2cd95d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 | 시간 0[s] | loss 6.04 | 퍼플렉서티 417.84\n",
      "| 에폭 2 | 시간 0[s] | loss 5.31 | 퍼플렉서티 306.83\n",
      "| 에폭 3 | 시간 0[s] | loss 5.63 | 퍼플렉서티 233.22\n",
      "| 에폭 4 | 시간 0[s] | loss 5.59 | 퍼플렉서티 212.74\n",
      "| 에폭 5 | 시간 0[s] | loss 5.09 | 퍼플렉서티 202.45\n",
      "| 에폭 6 | 시간 0[s] | loss 5.33 | 퍼플렉서티 193.05\n",
      "| 에폭 7 | 시간 0[s] | loss 5.12 | 퍼플렉서티 175.65\n",
      "| 에폭 8 | 시간 0[s] | loss 5.33 | 퍼플렉서티 167.06\n",
      "| 에폭 9 | 시간 0[s] | loss 5.09 | 퍼플렉서티 151.20\n",
      "| 에폭 10 | 시간 0[s] | loss 4.98 | 퍼플렉서티 137.37\n",
      "| 에폭 11 | 시간 0[s] | loss 4.85 | 퍼플렉서티 127.31\n",
      "| 에폭 12 | 시간 0[s] | loss 4.75 | 퍼플렉서티 111.47\n",
      "| 에폭 13 | 시간 0[s] | loss 4.57 | 퍼플렉서티 102.35\n",
      "| 에폭 14 | 시간 0[s] | loss 4.45 | 퍼플렉서티 88.50\n",
      "| 에폭 15 | 시간 0[s] | loss 3.96 | 퍼플렉서티 78.64\n",
      "| 에폭 16 | 시간 0[s] | loss 4.15 | 퍼플렉서티 69.24\n",
      "| 에폭 17 | 시간 0[s] | loss 4.06 | 퍼플렉서티 58.10\n",
      "| 에폭 18 | 시간 0[s] | loss 4.18 | 퍼플렉서티 49.81\n",
      "| 에폭 19 | 시간 0[s] | loss 4.14 | 퍼플렉서티 41.98\n",
      "| 에폭 20 | 시간 0[s] | loss 3.63 | 퍼플렉서티 37.08\n",
      "| 에폭 21 | 시간 0[s] | loss 3.29 | 퍼플렉서티 32.05\n",
      "| 에폭 22 | 시간 0[s] | loss 3.02 | 퍼플렉서티 26.86\n",
      "| 에폭 23 | 시간 0[s] | loss 2.84 | 퍼플렉서티 23.64\n",
      "| 에폭 24 | 시간 0[s] | loss 3.01 | 퍼플렉서티 21.42\n",
      "| 에폭 25 | 시간 0[s] | loss 2.74 | 퍼플렉서티 18.30\n",
      "| 에폭 26 | 시간 0[s] | loss 2.69 | 퍼플렉서티 16.50\n",
      "| 에폭 27 | 시간 0[s] | loss 2.18 | 퍼플렉서티 13.95\n",
      "| 에폭 28 | 시간 0[s] | loss 2.55 | 퍼플렉서티 12.32\n",
      "| 에폭 29 | 시간 0[s] | loss 2.44 | 퍼플렉서티 11.17\n",
      "| 에폭 30 | 시간 0[s] | loss 2.00 | 퍼플렉서티 10.16\n",
      "| 에폭 31 | 시간 0[s] | loss 2.19 | 퍼플렉서티 9.08\n",
      "| 에폭 32 | 시간 0[s] | loss 1.92 | 퍼플렉서티 7.85\n",
      "| 에폭 33 | 시간 0[s] | loss 1.92 | 퍼플렉서티 7.05\n",
      "| 에폭 34 | 시간 0[s] | loss 1.85 | 퍼플렉서티 6.86\n",
      "| 에폭 35 | 시간 0[s] | loss 1.84 | 퍼플렉서티 6.22\n",
      "| 에폭 36 | 시간 0[s] | loss 1.56 | 퍼플렉서티 5.69\n",
      "| 에폭 37 | 시간 0[s] | loss 1.65 | 퍼플렉서티 5.27\n",
      "| 에폭 38 | 시간 0[s] | loss 1.72 | 퍼플렉서티 4.72\n",
      "| 에폭 39 | 시간 0[s] | loss 1.43 | 퍼플렉서티 4.47\n",
      "| 에폭 40 | 시간 1[s] | loss 1.51 | 퍼플렉서티 4.27\n",
      "| 에폭 41 | 시간 1[s] | loss 1.55 | 퍼플렉서티 4.01\n",
      "| 에폭 42 | 시간 1[s] | loss 1.44 | 퍼플렉서티 3.63\n",
      "| 에폭 43 | 시간 1[s] | loss 1.16 | 퍼플렉서티 3.43\n",
      "| 에폭 44 | 시간 1[s] | loss 1.19 | 퍼플렉서티 3.47\n",
      "| 에폭 45 | 시간 1[s] | loss 1.32 | 퍼플렉서티 3.40\n",
      "| 에폭 46 | 시간 1[s] | loss 1.13 | 퍼플렉서티 3.05\n",
      "| 에폭 47 | 시간 1[s] | loss 1.05 | 퍼플렉서티 2.91\n",
      "| 에폭 48 | 시간 1[s] | loss 1.06 | 퍼플렉서티 2.83\n",
      "| 에폭 49 | 시간 1[s] | loss 1.23 | 퍼플렉서티 2.76\n",
      "| 에폭 50 | 시간 1[s] | loss 1.01 | 퍼플렉서티 2.78\n",
      "| 에폭 51 | 시간 1[s] | loss 0.92 | 퍼플렉서티 2.53\n",
      "| 에폭 52 | 시간 1[s] | loss 0.95 | 퍼플렉서티 2.47\n",
      "| 에폭 53 | 시간 1[s] | loss 0.73 | 퍼플렉서티 2.47\n",
      "| 에폭 54 | 시간 1[s] | loss 0.81 | 퍼플렉서티 2.50\n",
      "| 에폭 55 | 시간 1[s] | loss 0.81 | 퍼플렉서티 2.41\n",
      "| 에폭 56 | 시간 1[s] | loss 0.73 | 퍼플렉서티 2.31\n",
      "| 에폭 57 | 시간 1[s] | loss 0.87 | 퍼플렉서티 2.23\n",
      "| 에폭 58 | 시간 1[s] | loss 0.79 | 퍼플렉서티 2.19\n",
      "| 에폭 59 | 시간 1[s] | loss 0.86 | 퍼플렉서티 2.23\n",
      "| 에폭 60 | 시간 1[s] | loss 0.96 | 퍼플렉서티 2.32\n",
      "| 에폭 61 | 시간 1[s] | loss 0.66 | 퍼플렉서티 2.04\n",
      "| 에폭 62 | 시간 1[s] | loss 0.69 | 퍼플렉서티 2.00\n",
      "| 에폭 63 | 시간 1[s] | loss 0.70 | 퍼플렉서티 2.07\n",
      "| 에폭 64 | 시간 1[s] | loss 0.87 | 퍼플렉서티 2.13\n",
      "| 에폭 65 | 시간 1[s] | loss 0.62 | 퍼플렉서티 2.03\n",
      "| 에폭 66 | 시간 1[s] | loss 0.73 | 퍼플렉서티 1.94\n",
      "| 에폭 67 | 시간 1[s] | loss 0.75 | 퍼플렉서티 1.93\n",
      "| 에폭 68 | 시간 1[s] | loss 0.51 | 퍼플렉서티 1.95\n",
      "| 에폭 69 | 시간 1[s] | loss 0.69 | 퍼플렉서티 2.03\n",
      "| 에폭 70 | 시간 1[s] | loss 0.50 | 퍼플렉서티 1.91\n",
      "| 에폭 71 | 시간 1[s] | loss 0.56 | 퍼플렉서티 1.83\n",
      "| 에폭 72 | 시간 2[s] | loss 0.51 | 퍼플렉서티 1.90\n",
      "| 에폭 73 | 시간 2[s] | loss 0.72 | 퍼플렉서티 1.92\n",
      "| 에폭 74 | 시간 2[s] | loss 0.72 | 퍼플렉서티 1.91\n",
      "| 에폭 75 | 시간 2[s] | loss 0.79 | 퍼플렉서티 1.87\n",
      "| 에폭 76 | 시간 2[s] | loss 0.67 | 퍼플렉서티 1.81\n",
      "| 에폭 77 | 시간 2[s] | loss 0.64 | 퍼플렉서티 1.78\n",
      "| 에폭 78 | 시간 2[s] | loss 0.73 | 퍼플렉서티 1.87\n",
      "| 에폭 79 | 시간 2[s] | loss 0.64 | 퍼플렉서티 1.84\n",
      "| 에폭 80 | 시간 2[s] | loss 0.63 | 퍼플렉서티 1.79\n",
      "| 에폭 81 | 시간 2[s] | loss 0.71 | 퍼플렉서티 1.75\n",
      "| 에폭 82 | 시간 2[s] | loss 0.78 | 퍼플렉서티 1.83\n",
      "| 에폭 83 | 시간 2[s] | loss 0.49 | 퍼플렉서티 1.79\n",
      "| 에폭 84 | 시간 2[s] | loss 0.42 | 퍼플렉서티 1.80\n",
      "| 에폭 85 | 시간 2[s] | loss 0.46 | 퍼플렉서티 1.74\n",
      "| 에폭 86 | 시간 2[s] | loss 0.44 | 퍼플렉서티 1.70\n",
      "| 에폭 87 | 시간 2[s] | loss 0.67 | 퍼플렉서티 1.79\n",
      "| 에폭 88 | 시간 2[s] | loss 0.74 | 퍼플렉서티 1.80\n",
      "| 에폭 89 | 시간 2[s] | loss 0.44 | 퍼플렉서티 1.74\n",
      "| 에폭 90 | 시간 2[s] | loss 0.42 | 퍼플렉서티 1.72\n",
      "| 에폭 91 | 시간 2[s] | loss 0.63 | 퍼플렉서티 1.78\n",
      "| 에폭 92 | 시간 2[s] | loss 0.56 | 퍼플렉서티 1.72\n",
      "| 에폭 93 | 시간 2[s] | loss 0.65 | 퍼플렉서티 1.79\n",
      "| 에폭 94 | 시간 2[s] | loss 0.64 | 퍼플렉서티 1.76\n",
      "| 에폭 95 | 시간 2[s] | loss 0.27 | 퍼플렉서티 1.65\n",
      "| 에폭 96 | 시간 2[s] | loss 0.73 | 퍼플렉서티 1.71\n",
      "| 에폭 97 | 시간 2[s] | loss 0.57 | 퍼플렉서티 1.70\n",
      "| 에폭 98 | 시간 2[s] | loss 0.44 | 퍼플렉서티 1.76\n",
      "| 에폭 99 | 시간 2[s] | loss 0.54 | 퍼플렉서티 1.72\n",
      "| 에폭 100 | 시간 2[s] | loss 0.44 | 퍼플렉서티 1.64\n"
     ]
    }
   ],
   "source": [
    "ppl_list = []\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "start_time = time.time()\n",
    "model.time_idx = 0\n",
    "for epoch in range(max_epoch):\n",
    "    for iters in range(max_iters):\n",
    "        batch_x, batch_t = model.get_batch(xs, ts, batch_size, time_size)\n",
    "\n",
    "        y, loss = model.forward(batch_x, batch_t)\n",
    "        model.backward(batch_t)\n",
    "        model.update(lr)\n",
    "        \n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "        if ((iters % eval_interval) == 0):\n",
    "            ppl = np.exp(total_loss / loss_count)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('| 에폭 %d | 시간 %d[s] | loss %.2f | 퍼플렉서티 %.2f'\n",
    "                % (epoch + 1, elapsed_time, loss, ppl))\n",
    "            ppl_list.append(float(ppl))\n",
    "            total_loss, loss_count = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10200c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(ppl_list))\n",
    "plt.plot(x, ppl_list, label='train')\n",
    "plt.xlabel('iters (x' + str(eval_interval) + ')')\n",
    "plt.ylabel('ppl')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
