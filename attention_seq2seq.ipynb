{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b550053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import sequence\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c1fabd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seqlm:\n",
    "    \n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        \n",
    "        rn = np.random.randn\n",
    "        enc_embed = (rn(V, D) / 100).astype('f')\n",
    "        enc_lstmWx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        enc_lstmWh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        enc_lstmb = np.zeros(4 * H).astype('f')\n",
    "        \n",
    "        dec_embed = (rn(V, D) / 100).astype('f')\n",
    "        dec_lstmWx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        dec_lstmWh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        dec_lstmb = np.zeros(4 * H).astype('f')\n",
    "        dec_affineW = (rn(2 * H, V) / np.sqrt(H)).astype('f')\n",
    "        dec_affineb = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.params = [enc_embed, enc_lstmWx, enc_lstmWh, enc_lstmb, \\\n",
    "            dec_embed, dec_lstmWx, dec_lstmWh, dec_lstmb, dec_affineW, dec_affineb]\n",
    "        \n",
    "        self.grads = []\n",
    "        self.enc_lstm = [] \n",
    "        self.dec_lstm = []\n",
    "        self.attention = []\n",
    "        \n",
    "        self.time_idx = 0\n",
    "        self.vocab_size = vocab_size\n",
    "        self.wordvec_size = wordvec_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "\n",
    "        \n",
    "    def forward(self, enc_xs, ts):\n",
    "        enc_embed, enc_lstmWx, enc_lstmWh, enc_lstmb, \\\n",
    "            dec_embed, dec_lstmWx, dec_lstmWh, dec_lstmb, dec_affineW, dec_affineb = self.params\n",
    "        \n",
    "        self.enc_lstm = []\n",
    "        self.dec_lstm = []\n",
    "        \n",
    "        dec_xs = ts[:, :-1]\n",
    "        dec_ts = ts[:, 1:]\n",
    "        \n",
    "        batch_size, enc_time_size = enc_xs.shape\n",
    "        dec_time_size = dec_xs.shape[1]\n",
    "        \n",
    "        hidden_size = self.hidden_size\n",
    "               \n",
    "        ####################### ENCODER #######################\n",
    "        \n",
    "        h_prev = np.zeros((batch_size, hidden_size), dtype='f')\n",
    "        c_prev = np.zeros((batch_size, hidden_size), dtype='f')\n",
    "        enc_hs = np.zeros((batch_size, enc_time_size, hidden_size), dtype='f')\n",
    "        for t in range(enc_time_size):\n",
    "            # embed\n",
    "            emb_out = enc_embed[enc_xs[:, t]]\n",
    "\n",
    "            # lstm\n",
    "            A = np.matmul(emb_out, enc_lstmWx) + np.matmul(h_prev, enc_lstmWh) + enc_lstmb\n",
    "            f = A[:, :hidden_size]\n",
    "            g = A[:, hidden_size: 2*hidden_size]\n",
    "            i = A[:, 2*hidden_size: 3*hidden_size]\n",
    "            o = A[:, 3*hidden_size:]\n",
    "\n",
    "            f = self.sigmoid(f)\n",
    "            g = np.tanh(g)\n",
    "            i = self.sigmoid(i)\n",
    "            o = self.sigmoid(o)\n",
    "\n",
    "            c_next = f * c_prev + g * i\n",
    "            h_next = o * np.tanh(c_next)\n",
    "            \n",
    "            enc_hs[:, t, :] = h_next\n",
    "            \n",
    "            self.enc_lstm.append((emb_out, h_prev, c_prev, f, g, i, o, c_next, h_next))\n",
    "            \n",
    "            c_prev = c_next\n",
    "            h_prev = h_next\n",
    "            \n",
    "        ####################### DECODER #######################\n",
    "        \n",
    "        c_prev = np.zeros((batch_size, hidden_size), dtype='f')\n",
    "        dec_hs = np.zeros((batch_size, dec_time_size, hidden_size), dtype='f')\n",
    "        for t in range(dec_time_size):\n",
    "            # embed\n",
    "            emb_out = dec_embed[dec_xs[:, t]]\n",
    "            \n",
    "            # lstm\n",
    "            A = np.matmul(emb_out, dec_lstmWx) + np.matmul(h_prev, dec_lstmWh) + dec_lstmb\n",
    "            f = A[:, :hidden_size]\n",
    "            g = A[:, hidden_size: 2*hidden_size]\n",
    "            i = A[:, 2*hidden_size: 3*hidden_size]\n",
    "            o = A[:, 3*hidden_size:]\n",
    "\n",
    "            f = self.sigmoid(f)\n",
    "            g = np.tanh(g)\n",
    "            i = self.sigmoid(i)\n",
    "            o = self.sigmoid(o)\n",
    "\n",
    "            c_next = f * c_prev + g * i\n",
    "            h_next = o * np.tanh(c_next)\n",
    "            \n",
    "            dec_hs[:, t, :] = h_next\n",
    "            \n",
    "            self.dec_lstm.append((emb_out, h_prev, c_prev, f, g, i, o, c_next, h_next))\n",
    "            c_prev = c_next\n",
    "            h_prev = h_next\n",
    "            \n",
    "        # attention\n",
    "        self.attention = []\n",
    "        \n",
    "        c = np.zeros_like(dec_hs)\n",
    "        for t in range(dec_time_size):\n",
    "            hr = dec_hs[:, t, :].reshape(batch_size, 1, hidden_size).repeat(enc_time_size, axis=1)\n",
    "            t1 = enc_hs * hr\n",
    "            s = np.sum(t1, axis=2)\n",
    "            a = self.softmax(s)\n",
    "            ar = a.reshape(batch_size, enc_time_size, 1).repeat(hidden_size, axis=2)\n",
    "            t2 = enc_hs * ar\n",
    "            c[:, t, :] = np.sum(t2, axis=1)\n",
    "            self.attention.append([hr, ar, a])\n",
    "            \n",
    "        # affine\n",
    "        concat_attention = np.concatenate((c, dec_hs), axis=2)\n",
    "        dec_affine_out = np.matmul(concat_attention, dec_affineW) + dec_affineb\n",
    "        \n",
    "        # softmax\n",
    "        y = self.softmax(dec_affine_out.reshape(batch_size, dec_time_size, -1))\n",
    "        loss = self.getLoss(y, dec_ts)\n",
    "        \n",
    "        self.xs = enc_xs, enc_hs, dec_xs, dec_hs, concat_attention, dec_affine_out, y\n",
    "        self.ts = dec_ts\n",
    "        return loss\n",
    "        \n",
    "    def backward(self):\n",
    "        enc_embed, enc_lstmWx, enc_lstmWh, enc_lstmb, \\\n",
    "            dec_embed, dec_lstmWx, dec_lstmWh, dec_lstmb, dec_affineW, dec_affineb = self.params\n",
    "        enc_xs, enc_hs, dec_xs, dec_hs, concat_attention, dec_affine_out, y = self.xs\n",
    "        dec_ts = self.ts\n",
    "        \n",
    "        vocab_size = self.vocab_size        \n",
    "        wordvec_size = self.wordvec_size\n",
    "        batch_size, enc_time_size = enc_xs.shape\n",
    "        dec_time_size = dec_xs.shape[1]\n",
    "        \n",
    "        ####################### DECODER #######################\n",
    "        \n",
    "        # softmax\n",
    "        y = y.reshape(batch_size * dec_time_size, -1)\n",
    "        y[np.arange(batch_size * dec_time_size), dec_ts.reshape(batch_size * dec_time_size)] -= 1\n",
    "        soft_dout = y\n",
    "        \n",
    "        # affine\n",
    "        concat_attention = concat_attention.reshape(batch_size * dec_time_size, -1)\n",
    "        affine_dout = np.matmul(soft_dout, dec_affineW.T).reshape(batch_size, dec_time_size, -1)\n",
    "        affinedW = np.matmul(concat_attention.T, soft_dout) \n",
    "        affinedb = np.sum(soft_dout, axis=0)\n",
    "        \n",
    "        dc, affine_dout = affine_dout[:, :, :hidden_size], affine_dout[:, :, hidden_size:]\n",
    "                \n",
    "        # attention\n",
    "        enc_dhs = np.zeros_like(enc_hs)\n",
    "        dec_dhs = np.zeros_like(dec_hs)\n",
    "        for t in range(dec_time_size):\n",
    "            hr, ar, a = self.attention[t]\n",
    "            \n",
    "            # t2\n",
    "            dt2 = dc[:, t, :].reshape(batch_size, 1, hidden_size).repeat(enc_time_size, axis=1)\n",
    "            \n",
    "            # ar\n",
    "            dar = dt2 * enc_hs\n",
    "            enc_dhs0 = dt2 * ar\n",
    "            \n",
    "            # a \n",
    "            da = np.sum(dar, axis=2)\n",
    "            \n",
    "            # s\n",
    "            ds = a * da\n",
    "            sum_ds = np.sum(ds, axis=1, keepdims=True)\n",
    "            ds -= a * sum_ds\n",
    "            \n",
    "            # t1\n",
    "            dt1 = ds.reshape(batch_size, enc_time_size, -1).repeat(hidden_size, axis=2)\n",
    "            \n",
    "            # hr\n",
    "            dhr = dt1 * enc_hs\n",
    "            enc_dhs1 = dt1 * hr\n",
    "            \n",
    "            # h\n",
    "            dh = np.sum(dhr, axis=1)\n",
    "            \n",
    "            enc_dhs += (enc_dhs0 + enc_dhs1)\n",
    "            dec_dhs[:, t, :] = dh\n",
    "            \n",
    "        dec_dhs += affine_dout\n",
    "\n",
    "        # lstm\n",
    "        dec_lstmdWx = np.zeros_like(dec_lstmWx)\n",
    "        dec_lstmdWh = np.zeros_like(dec_lstmWh)\n",
    "        dec_lstmdb = np.zeros_like(dec_lstmb)\n",
    "        \n",
    "        dec_lstm_douts = np.empty((batch_size, dec_time_size, wordvec_size), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "        for t in reversed(range(dec_time_size)):            \n",
    "            emb_out, h_prev, c_prev, f, g, i, o, c_next, _ = self.dec_lstm[t]\n",
    "            dh_next = dec_dhs[:, t, :] + dh\n",
    "            dc_next = dc\n",
    "\n",
    "            tanh_c_next = np.tanh(c_next)\n",
    "            \n",
    "            ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "            \n",
    "            dc_prev = ds * f\n",
    "            \n",
    "            di = ds * g\n",
    "            df = ds * c_prev\n",
    "            do = dh_next * tanh_c_next\n",
    "            dg = ds * i\n",
    "            \n",
    "            di *= i * (1 - i)\n",
    "            df *= f * (1 - f)\n",
    "            do *= o * (1 - o)\n",
    "            dg *= (1 - g ** 2)\n",
    "            \n",
    "            dA = np.hstack((df, dg, di, do))\n",
    "            \n",
    "            dWh = np.matmul(h_prev.T, dA)\n",
    "            dWx = np.matmul(emb_out.T, dA)\n",
    "            db = np.sum(dA, axis=0)\n",
    "            \n",
    "            dec_lstm_douts[:, t, :] = np.matmul(dA, dec_lstmWx.T)\n",
    "            dh_prev = np.matmul(dA, dec_lstmWh.T)\n",
    "        \n",
    "            dec_lstmdWx += dWx\n",
    "            dec_lstmdWh += dWh\n",
    "            dec_lstmdb += db\n",
    "            dh = dh_prev\n",
    "            dc = dc_prev\n",
    "            \n",
    "        # embed\n",
    "        dec_embed_dout = np.zeros_like(dec_embed)\n",
    "        for t in range(dec_time_size):\n",
    "            np.add.at(dec_embed_dout, dec_xs[:, t], dec_lstm_douts[:, t, :])\n",
    "            \n",
    "        ####################### ENCODER #######################\n",
    "        \n",
    "        # lstm\n",
    "        enc_lstmdWx = np.zeros_like(enc_lstmWx)\n",
    "        enc_lstmdWh = np.zeros_like(enc_lstmWh)\n",
    "        enc_lstmdb = np.zeros_like(enc_lstmb)\n",
    "        \n",
    "        enc_lstm_douts = np.empty((batch_size, enc_time_size, wordvec_size), dtype='f')\n",
    "        dc = 0\n",
    "        for t in reversed(range(enc_time_size)):            \n",
    "            emb_out, h_prev, c_prev, f, g, i, o, c_next, _ = self.enc_lstm[t]\n",
    "            dh_next = enc_dhs[:, t, :] + dh\n",
    "            dc_next = dc\n",
    "\n",
    "            tanh_c_next = np.tanh(c_next)\n",
    "            \n",
    "            ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "            \n",
    "            dc_prev = ds * f\n",
    "            \n",
    "            di = ds * g\n",
    "            df = ds * c_prev\n",
    "            do = dh_next * tanh_c_next\n",
    "            dg = ds * i\n",
    "            \n",
    "            di *= i * (1 - i)\n",
    "            df *= f * (1 - f)\n",
    "            do *= o * (1 - o)\n",
    "            dg *= (1 - g ** 2)\n",
    "            \n",
    "            dA = np.hstack((df, dg, di, do))\n",
    "            \n",
    "            dWh = np.matmul(h_prev.T, dA)\n",
    "            dWx = np.matmul(emb_out.T, dA)\n",
    "            db = np.sum(dA, axis=0)\n",
    "            \n",
    "            enc_lstm_douts[:, t, :] = np.matmul(dA, enc_lstmWx.T)\n",
    "            dh_prev = np.matmul(dA, enc_lstmWh.T)\n",
    "        \n",
    "            enc_lstmdWx += dWx\n",
    "            enc_lstmdWh += dWh\n",
    "            enc_lstmdb += db\n",
    "            dh = dh_prev\n",
    "            dc = dc_prev        \n",
    "        \n",
    "        # embed\n",
    "        enc_embed_dout = np.zeros_like(enc_embed)\n",
    "        for t in range(enc_time_size):\n",
    "            np.add.at(enc_embed_dout, enc_xs[:, t], enc_lstm_douts[:, t, :])\n",
    "            \n",
    "        self.grads = enc_embed_dout, enc_lstmdWx, enc_lstmdWh, enc_lstmdb, \\\n",
    "            dec_embed_dout, dec_lstmdWx, dec_lstmdWh, dec_lstmdb, affinedW, affinedb\n",
    "        \n",
    "    def softmax(self, y):\n",
    "        y = y - np.max(y)\n",
    "        y = np.exp(y)\n",
    "        y = y / y.sum(axis=-1, keepdims=True)\n",
    "        return y\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def getLoss(self, y, t):\n",
    "        N, T, V = y.shape\n",
    "\n",
    "        y = y.reshape(N * T, V)\n",
    "        t = t.reshape(N * T)\n",
    "\n",
    "        ls = np.log(y[np.arange(N * T), t])\n",
    "        return -np.sum(ls) / (N * T)\n",
    "                \n",
    "    def updateAdam(self, lr):\n",
    "        params = self.params\n",
    "        grads = self.grads\n",
    "        \n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "                \n",
    "        self.iter += 1\n",
    "        lr_t = lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "        \n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n",
    "        self.params = params\n",
    "        \n",
    "    def clip_grads(self, max_norm):\n",
    "        grads = self.grads\n",
    "        total_norm = 0\n",
    "        \n",
    "        for grad in grads:\n",
    "            total_norm += np.sum(grad**2)\n",
    "            \n",
    "        total_norm = np.sqrt(total_norm)\n",
    "        \n",
    "        rate = max_norm / (total_norm + 1e-6)\n",
    "        \n",
    "        if rate  < 1:\n",
    "            for grad in grads:\n",
    "                grad *= rate\n",
    "        self.grads = grads\n",
    "            \n",
    "    def generate(self, xs, start_id, sample_size):\n",
    "        enc_embed, enc_lstmWx, enc_lstmWh, enc_lstmb, \\\n",
    "            dec_embed, dec_lstmWx, dec_lstmWh, dec_lstmb, dec_affineW, dec_affineb = self.params\n",
    "        \n",
    "        \n",
    "        enc_time_size = xs.shape[1]\n",
    "        hidden_size = self.hidden_size\n",
    "        \n",
    "        ####################### ENCODER #######################\n",
    "        \n",
    "        h_prev = np.zeros((hidden_size), dtype='f')\n",
    "        c_prev = np.zeros((hidden_size), dtype='f')\n",
    "        enc_hs = np.zeros((enc_time_size, hidden_size), dtype='f')\n",
    "        for t in range(enc_time_size):\n",
    "            # embed\n",
    "            emb_out = enc_embed[xs[:, t]]\n",
    "\n",
    "            # lstm\n",
    "            A = np.matmul(emb_out, enc_lstmWx) + np.matmul(h_prev, enc_lstmWh) + enc_lstmb\n",
    "            f = A[:, :hidden_size]\n",
    "            g = A[:, hidden_size: 2*hidden_size]\n",
    "            i = A[:, 2*hidden_size: 3*hidden_size]\n",
    "            o = A[:, 3*hidden_size:]\n",
    "\n",
    "            f = self.sigmoid(f)\n",
    "            g = np.tanh(g)\n",
    "            i = self.sigmoid(i)\n",
    "            o = self.sigmoid(o)\n",
    "\n",
    "            c_next = f * c_prev + g * i\n",
    "            h_next = o * np.tanh(c_next)\n",
    "            \n",
    "            enc_hs[t, :] = h_next\n",
    "            \n",
    "            c_prev = c_next\n",
    "            h_prev = h_next\n",
    "                        \n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "\n",
    "        ####################### DECODER #######################\n",
    "\n",
    "        c_prev = np.zeros((hidden_size), dtype='f')\n",
    "        for _ in range(sample_size):\n",
    "            \n",
    "            # embed\n",
    "            emb_out = dec_embed[sample_id]\n",
    "\n",
    "            # lstm\n",
    "            A = np.matmul(emb_out, dec_lstmWx) + np.matmul(h_prev, dec_lstmWh) + dec_lstmb\n",
    "            f = A[:, :hidden_size]\n",
    "            g = A[:, hidden_size: 2*hidden_size]\n",
    "            i = A[:, 2*hidden_size: 3*hidden_size]\n",
    "            o = A[:, 3*hidden_size:]\n",
    "\n",
    "            f = self.sigmoid(f)\n",
    "            g = np.tanh(g)\n",
    "            i = self.sigmoid(i)\n",
    "            o = self.sigmoid(o)\n",
    "\n",
    "            c_next = f * c_prev + g * i\n",
    "            h_next = o * np.tanh(c_next)\n",
    "\n",
    "            c_prev = c_next\n",
    "            h_prev = h_next\n",
    "            \n",
    "            # attention\n",
    "            hr = h_next.reshape(1, 1, hidden_size).repeat(enc_time_size, axis=1)\n",
    "            t1 = enc_hs * hr\n",
    "            s = np.sum(t1, axis=2)\n",
    "            a = self.softmax(s)\n",
    "            ar = a.reshape(1, enc_time_size, 1).repeat(hidden_size, axis=2)\n",
    "            t2 = enc_hs * ar\n",
    "            c = np.sum(t2, axis=1)\n",
    "            \n",
    "            # affine\n",
    "            concat_attention = np.concatenate((c, h_next), axis=1)\n",
    "            dec_affine_out = np.matmul(concat_attention, dec_affineW) + dec_affineb\n",
    "            \n",
    "            # softmax\n",
    "            y = self.softmax(dec_affine_out)\n",
    "            sample_id = np.argmax(y.flatten())\n",
    "            sampled.append(int(sample_id))\n",
    "        \n",
    "        return sampled\n",
    "            \n",
    "    def eval_seq2seq(self, question, correct, id_to_char,\n",
    "                     verbos=False, is_reverse=False):\n",
    "        correct = correct.flatten()\n",
    "        # 머릿글자\n",
    "        start_id = correct[0]\n",
    "        correct = correct[1:]\n",
    "        guess = self.generate(question, start_id, len(correct))\n",
    "\n",
    "        # 문자열로 변환\n",
    "        question = ''.join([id_to_char[int(c)] for c in question.flatten()])\n",
    "        correct = ''.join([id_to_char[int(c)] for c in correct])\n",
    "        guess = ''.join([id_to_char[int(c)] for c in guess])\n",
    "\n",
    "        if verbos:\n",
    "            if is_reverse:\n",
    "                question = question[::-1]\n",
    "\n",
    "            print('Q', question)\n",
    "            print('T', correct)\n",
    "\n",
    "            if correct == guess:\n",
    "                mark = 'O'\n",
    "                print(mark + ' ' + guess)\n",
    "            else:\n",
    "                mark = 'X'\n",
    "                print(mark + ' ' + guess)\n",
    "            print('---')\n",
    "\n",
    "        return 1 if guess == correct else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c6b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f77ac798",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "data_size = len(x_train)\n",
    "learning_rate = 0.001\n",
    "data_size = len(x_train)\n",
    "max_iters = data_size // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87cfdf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionSeq2seqlm(vocab_size, wordvec_size, hidden_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0ffdf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iters 1 / 351 | time 1[s] | loss 4.08\n",
      "| epoch 1 |  iters 21 / 351 | time 36[s] | loss 2.99\n",
      "| epoch 1 |  iters 41 / 351 | time 68[s] | loss 1.80\n",
      "| epoch 1 |  iters 61 / 351 | time 98[s] | loss 1.51\n",
      "| epoch 1 |  iters 81 / 351 | time 130[s] | loss 1.22\n",
      "| epoch 1 |  iters 101 / 351 | time 161[s] | loss 1.11\n",
      "| epoch 1 |  iters 121 / 351 | time 185[s] | loss 1.05\n",
      "| epoch 1 |  iters 141 / 351 | time 209[s] | loss 1.03\n",
      "| epoch 1 |  iters 161 / 351 | time 233[s] | loss 1.01\n",
      "| epoch 1 |  iters 181 / 351 | time 266[s] | loss 0.99\n",
      "| epoch 1 |  iters 201 / 351 | time 294[s] | loss 0.97\n",
      "| epoch 1 |  iters 221 / 351 | time 320[s] | loss 0.96\n",
      "| epoch 1 |  iters 241 / 351 | time 345[s] | loss 0.94\n",
      "| epoch 1 |  iters 261 / 351 | time 370[s] | loss 0.94\n",
      "| epoch 1 |  iters 281 / 351 | time 394[s] | loss 0.91\n",
      "| epoch 1 |  iters 301 / 351 | time 418[s] | loss 0.90\n",
      "| epoch 1 |  iters 321 / 351 | time 442[s] | loss 0.88\n",
      "| epoch 1 |  iters 341 / 351 | time 466[s] | loss 0.86\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1999-01-14\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1999-08-21\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 2005-05-05\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1977-08-29\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1999-09-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1999-01-16\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 2004-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2004-08-20\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1999-01-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1999-08-26\n",
      "---\n",
      "accuracy 0.180%\n",
      "| epoch 2 |  iters 1 / 351 | time 558[s] | loss 0.84\n",
      "| epoch 2 |  iters 21 / 351 | time 585[s] | loss 0.80\n",
      "| epoch 2 |  iters 41 / 351 | time 610[s] | loss 0.78\n",
      "| epoch 2 |  iters 61 / 351 | time 635[s] | loss 0.75\n",
      "| epoch 2 |  iters 81 / 351 | time 660[s] | loss 0.72\n",
      "| epoch 2 |  iters 101 / 351 | time 685[s] | loss 0.70\n",
      "| epoch 2 |  iters 121 / 351 | time 712[s] | loss 0.66\n",
      "| epoch 2 |  iters 141 / 351 | time 742[s] | loss 0.65\n",
      "| epoch 2 |  iters 161 / 351 | time 776[s] | loss 0.63\n",
      "| epoch 2 |  iters 181 / 351 | time 811[s] | loss 0.61\n",
      "| epoch 2 |  iters 201 / 351 | time 845[s] | loss 0.59\n",
      "| epoch 2 |  iters 221 / 351 | time 869[s] | loss 0.56\n",
      "| epoch 2 |  iters 241 / 351 | time 893[s] | loss 0.55\n",
      "| epoch 2 |  iters 261 / 351 | time 916[s] | loss 0.53\n",
      "| epoch 2 |  iters 281 / 351 | time 940[s] | loss 0.50\n",
      "| epoch 2 |  iters 301 / 351 | time 963[s] | loss 0.50\n",
      "| epoch 2 |  iters 321 / 351 | time 986[s] | loss 0.47\n",
      "| epoch 2 |  iters 341 / 351 | time 1008[s] | loss 0.45\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1998-10-05\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2001-12-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 2005-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 2012-12-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1983-07-28\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1999-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2008-08-00\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 2012-10-08\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2012-10-04\n",
      "---\n",
      "accuracy 6.000%\n",
      "| epoch 3 |  iters 1 / 351 | time 1096[s] | loss 0.43\n",
      "| epoch 3 |  iters 21 / 351 | time 1125[s] | loss 0.41\n",
      "| epoch 3 |  iters 41 / 351 | time 1154[s] | loss 0.39\n",
      "| epoch 3 |  iters 61 / 351 | time 1180[s] | loss 0.38\n",
      "| epoch 3 |  iters 81 / 351 | time 1205[s] | loss 0.36\n",
      "| epoch 3 |  iters 101 / 351 | time 1230[s] | loss 0.33\n",
      "| epoch 3 |  iters 121 / 351 | time 1256[s] | loss 0.32\n",
      "| epoch 3 |  iters 141 / 351 | time 1283[s] | loss 0.31\n",
      "| epoch 3 |  iters 161 / 351 | time 1311[s] | loss 0.30\n",
      "| epoch 3 |  iters 181 / 351 | time 1336[s] | loss 0.29\n",
      "| epoch 3 |  iters 201 / 351 | time 1362[s] | loss 0.29\n",
      "| epoch 3 |  iters 221 / 351 | time 1388[s] | loss 0.28\n",
      "| epoch 3 |  iters 241 / 351 | time 1414[s] | loss 0.28\n",
      "| epoch 3 |  iters 261 / 351 | time 1438[s] | loss 0.27\n",
      "| epoch 3 |  iters 281 / 351 | time 1462[s] | loss 0.26\n",
      "| epoch 3 |  iters 301 / 351 | time 1487[s] | loss 0.27\n",
      "| epoch 3 |  iters 321 / 351 | time 1510[s] | loss 0.26\n",
      "| epoch 3 |  iters 341 / 351 | time 1530[s] | loss 0.26\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1998-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2003-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 2012-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1973-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2008-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 2012-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2012-11-06\n",
      "---\n",
      "accuracy 10.280%\n",
      "| epoch 4 |  iters 1 / 351 | time 1596[s] | loss 0.26\n",
      "| epoch 4 |  iters 21 / 351 | time 1616[s] | loss 0.26\n",
      "| epoch 4 |  iters 41 / 351 | time 1637[s] | loss 0.25\n",
      "| epoch 4 |  iters 61 / 351 | time 1660[s] | loss 0.25\n",
      "| epoch 4 |  iters 81 / 351 | time 1682[s] | loss 0.25\n",
      "| epoch 4 |  iters 101 / 351 | time 1704[s] | loss 0.24\n",
      "| epoch 4 |  iters 121 / 351 | time 1725[s] | loss 0.24\n",
      "| epoch 4 |  iters 141 / 351 | time 1746[s] | loss 0.24\n",
      "| epoch 4 |  iters 161 / 351 | time 1766[s] | loss 0.24\n",
      "| epoch 4 |  iters 181 / 351 | time 1788[s] | loss 0.23\n",
      "| epoch 4 |  iters 201 / 351 | time 1811[s] | loss 0.23\n",
      "| epoch 4 |  iters 221 / 351 | time 1832[s] | loss 0.21\n",
      "| epoch 4 |  iters 241 / 351 | time 1852[s] | loss 0.20\n",
      "| epoch 4 |  iters 261 / 351 | time 1874[s] | loss 0.19\n",
      "| epoch 4 |  iters 281 / 351 | time 1895[s] | loss 0.19\n",
      "| epoch 4 |  iters 301 / 351 | time 1916[s] | loss 0.19\n",
      "| epoch 4 |  iters 321 / 351 | time 1938[s] | loss 0.17\n",
      "| epoch 4 |  iters 341 / 351 | time 1960[s] | loss 0.16\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2004-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 2000-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1990-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 2004-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2004-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "accuracy 32.700%\n",
      "| epoch 5 |  iters 1 / 351 | time 2028[s] | loss 0.16\n",
      "| epoch 5 |  iters 21 / 351 | time 2051[s] | loss 0.15\n",
      "| epoch 5 |  iters 41 / 351 | time 2073[s] | loss 0.11\n",
      "| epoch 5 |  iters 61 / 351 | time 2094[s] | loss 0.09\n",
      "| epoch 5 |  iters 81 / 351 | time 2116[s] | loss 0.08\n",
      "| epoch 5 |  iters 101 / 351 | time 2138[s] | loss 0.05\n",
      "| epoch 5 |  iters 121 / 351 | time 2159[s] | loss 0.05\n",
      "| epoch 5 |  iters 141 / 351 | time 2181[s] | loss 0.05\n",
      "| epoch 5 |  iters 161 / 351 | time 2203[s] | loss 0.04\n",
      "| epoch 5 |  iters 181 / 351 | time 2224[s] | loss 0.03\n",
      "| epoch 5 |  iters 201 / 351 | time 2243[s] | loss 0.03\n",
      "| epoch 5 |  iters 221 / 351 | time 2261[s] | loss 0.02\n",
      "| epoch 5 |  iters 241 / 351 | time 2280[s] | loss 0.02\n",
      "| epoch 5 |  iters 261 / 351 | time 2297[s] | loss 0.02\n",
      "| epoch 5 |  iters 281 / 351 | time 2314[s] | loss 0.01\n",
      "| epoch 5 |  iters 301 / 351 | time 2329[s] | loss 0.01\n",
      "| epoch 5 |  iters 321 / 351 | time 2347[s] | loss 0.01\n",
      "| epoch 5 |  iters 341 / 351 | time 2363[s] | loss 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "accuracy 98.980%\n",
      "| epoch 6 |  iters 1 / 351 | time 2412[s] | loss 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 6 |  iters 21 / 351 | time 2428[s] | loss 0.00\n",
      "| epoch 6 |  iters 41 / 351 | time 2445[s] | loss 0.01\n",
      "| epoch 6 |  iters 61 / 351 | time 2462[s] | loss 0.00\n",
      "| epoch 6 |  iters 81 / 351 | time 2479[s] | loss 0.00\n",
      "| epoch 6 |  iters 101 / 351 | time 2495[s] | loss 0.01\n",
      "| epoch 6 |  iters 121 / 351 | time 2511[s] | loss 0.01\n",
      "| epoch 6 |  iters 141 / 351 | time 2527[s] | loss 0.00\n",
      "| epoch 6 |  iters 161 / 351 | time 2543[s] | loss 0.01\n",
      "| epoch 6 |  iters 181 / 351 | time 2558[s] | loss 0.00\n",
      "| epoch 6 |  iters 201 / 351 | time 2574[s] | loss 0.00\n",
      "| epoch 6 |  iters 221 / 351 | time 2590[s] | loss 0.00\n",
      "| epoch 6 |  iters 241 / 351 | time 2608[s] | loss 0.00\n",
      "| epoch 6 |  iters 261 / 351 | time 2625[s] | loss 0.00\n",
      "| epoch 6 |  iters 281 / 351 | time 2641[s] | loss 0.00\n",
      "| epoch 6 |  iters 301 / 351 | time 2657[s] | loss 0.01\n",
      "| epoch 6 |  iters 321 / 351 | time 2672[s] | loss 0.00\n",
      "| epoch 6 |  iters 341 / 351 | time 2689[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "accuracy 99.560%\n",
      "| epoch 7 |  iters 1 / 351 | time 2736[s] | loss 0.00\n",
      "| epoch 7 |  iters 21 / 351 | time 2752[s] | loss 0.00\n",
      "| epoch 7 |  iters 41 / 351 | time 2768[s] | loss 0.00\n",
      "| epoch 7 |  iters 61 / 351 | time 2783[s] | loss 0.00\n",
      "| epoch 7 |  iters 81 / 351 | time 2800[s] | loss 0.00\n",
      "| epoch 7 |  iters 101 / 351 | time 2816[s] | loss 0.00\n",
      "| epoch 7 |  iters 121 / 351 | time 2832[s] | loss 0.00\n",
      "| epoch 7 |  iters 141 / 351 | time 2848[s] | loss 0.00\n",
      "| epoch 7 |  iters 161 / 351 | time 2864[s] | loss 0.00\n",
      "| epoch 7 |  iters 181 / 351 | time 2881[s] | loss 0.00\n",
      "| epoch 7 |  iters 201 / 351 | time 2897[s] | loss 0.00\n",
      "| epoch 7 |  iters 221 / 351 | time 2914[s] | loss 0.00\n",
      "| epoch 7 |  iters 241 / 351 | time 2930[s] | loss 0.00\n",
      "| epoch 7 |  iters 261 / 351 | time 2946[s] | loss 0.00\n",
      "| epoch 7 |  iters 281 / 351 | time 2962[s] | loss 0.00\n",
      "| epoch 7 |  iters 301 / 351 | time 2979[s] | loss 0.00\n",
      "| epoch 7 |  iters 321 / 351 | time 2995[s] | loss 0.00\n",
      "| epoch 7 |  iters 341 / 351 | time 3011[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "accuracy 99.560%\n",
      "| epoch 8 |  iters 1 / 351 | time 3057[s] | loss 0.00\n",
      "| epoch 8 |  iters 21 / 351 | time 3075[s] | loss 0.00\n",
      "| epoch 8 |  iters 41 / 351 | time 3092[s] | loss 0.00\n",
      "| epoch 8 |  iters 61 / 351 | time 3109[s] | loss 0.00\n",
      "| epoch 8 |  iters 81 / 351 | time 3125[s] | loss 0.00\n",
      "| epoch 8 |  iters 101 / 351 | time 3141[s] | loss 0.00\n",
      "| epoch 8 |  iters 121 / 351 | time 3157[s] | loss 0.00\n",
      "| epoch 8 |  iters 141 / 351 | time 3173[s] | loss 0.00\n",
      "| epoch 8 |  iters 161 / 351 | time 3190[s] | loss 0.00\n",
      "| epoch 8 |  iters 181 / 351 | time 3206[s] | loss 0.00\n",
      "| epoch 8 |  iters 201 / 351 | time 3223[s] | loss 0.00\n",
      "| epoch 8 |  iters 221 / 351 | time 3239[s] | loss 0.00\n",
      "| epoch 8 |  iters 241 / 351 | time 3256[s] | loss 0.00\n",
      "| epoch 8 |  iters 261 / 351 | time 3272[s] | loss 0.00\n",
      "| epoch 8 |  iters 281 / 351 | time 3288[s] | loss 0.00\n",
      "| epoch 8 |  iters 301 / 351 | time 3305[s] | loss 0.00\n",
      "| epoch 8 |  iters 321 / 351 | time 3321[s] | loss 0.00\n",
      "| epoch 8 |  iters 341 / 351 | time 3337[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "accuracy 99.860%\n",
      "| epoch 9 |  iters 1 / 351 | time 3382[s] | loss 0.00\n",
      "| epoch 9 |  iters 21 / 351 | time 3399[s] | loss 0.00\n",
      "| epoch 9 |  iters 41 / 351 | time 3415[s] | loss 0.00\n",
      "| epoch 9 |  iters 61 / 351 | time 3431[s] | loss 0.00\n",
      "| epoch 9 |  iters 81 / 351 | time 3447[s] | loss 0.00\n",
      "| epoch 9 |  iters 101 / 351 | time 3464[s] | loss 0.00\n",
      "| epoch 9 |  iters 121 / 351 | time 3480[s] | loss 0.00\n",
      "| epoch 9 |  iters 141 / 351 | time 3497[s] | loss 0.00\n",
      "| epoch 9 |  iters 161 / 351 | time 3514[s] | loss 0.00\n",
      "| epoch 9 |  iters 181 / 351 | time 3531[s] | loss 0.00\n",
      "| epoch 9 |  iters 201 / 351 | time 3548[s] | loss 0.00\n",
      "| epoch 9 |  iters 221 / 351 | time 3565[s] | loss 0.00\n",
      "| epoch 9 |  iters 241 / 351 | time 3581[s] | loss 0.00\n",
      "| epoch 9 |  iters 261 / 351 | time 3597[s] | loss 0.00\n",
      "| epoch 9 |  iters 281 / 351 | time 3614[s] | loss 0.00\n",
      "| epoch 9 |  iters 301 / 351 | time 3630[s] | loss 0.00\n",
      "| epoch 9 |  iters 321 / 351 | time 3646[s] | loss 0.00\n",
      "| epoch 9 |  iters 341 / 351 | time 3663[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "accuracy 99.780%\n",
      "| epoch 10 |  iters 1 / 351 | time 3709[s] | loss 0.00\n",
      "| epoch 10 |  iters 21 / 351 | time 3725[s] | loss 0.00\n",
      "| epoch 10 |  iters 41 / 351 | time 3741[s] | loss 0.00\n",
      "| epoch 10 |  iters 61 / 351 | time 3758[s] | loss 0.00\n",
      "| epoch 10 |  iters 81 / 351 | time 3774[s] | loss 0.00\n",
      "| epoch 10 |  iters 101 / 351 | time 3791[s] | loss 0.00\n",
      "| epoch 10 |  iters 121 / 351 | time 3807[s] | loss 0.00\n",
      "| epoch 10 |  iters 141 / 351 | time 3824[s] | loss 0.00\n",
      "| epoch 10 |  iters 161 / 351 | time 3840[s] | loss 0.00\n",
      "| epoch 10 |  iters 181 / 351 | time 3857[s] | loss 0.00\n",
      "| epoch 10 |  iters 201 / 351 | time 3873[s] | loss 0.00\n",
      "| epoch 10 |  iters 221 / 351 | time 3889[s] | loss 0.00\n",
      "| epoch 10 |  iters 241 / 351 | time 3906[s] | loss 0.00\n",
      "| epoch 10 |  iters 261 / 351 | time 3922[s] | loss 0.00\n",
      "| epoch 10 |  iters 281 / 351 | time 3938[s] | loss 0.00\n",
      "| epoch 10 |  iters 301 / 351 | time 3954[s] | loss 0.00\n",
      "| epoch 10 |  iters 321 / 351 | time 3971[s] | loss 0.00\n",
      "| epoch 10 |  iters 341 / 351 | time 3986[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "accuracy 99.780%\n"
     ]
    }
   ],
   "source": [
    "acc_list = []\n",
    "loss_list = []\n",
    "start_time = time.time()\n",
    "total_loss, loss_count = 0, 0\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    idx = np.random.permutation(np.arange(data_size))\n",
    "    x_train = x_train[idx]\n",
    "    t_train = t_train[idx]\n",
    "    \n",
    "    for iters in range(max_iters):\n",
    "        batch_x = x_train[(iters * batch_size) : (iters + 1) * batch_size]\n",
    "        batch_t = t_train[(iters * batch_size) : (iters + 1) * batch_size]\n",
    "        \n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        model.clip_grads(max_grad)\n",
    "        model.updateAdam(learning_rate)\n",
    "        \n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "        \n",
    "        if iters % 20 == 0:\n",
    "            avg_loss = total_loss / loss_count\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('| epoch %d |  iters %d / %d | time %d[s] | loss %.2f'\n",
    "                % (epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "            loss_list.append(avg_loss)\n",
    "            total_loss, loss_count = 0, 0\n",
    "            \n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += model.eval_seq2seq(question, correct, id_to_char, verbose)\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('accuracy %.3f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52a975af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = np.arange(len(acc_list))\n",
    "plt.plot(acc, acc_list, label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
