{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b550053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import sequence\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c1fabd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seqlm:\n",
    "    \n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        \n",
    "        rn = np.random.randn\n",
    "        enc_embed = (rn(V, D) / 100).astype('f')\n",
    "        enc_lstmWx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        enc_lstmWh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        enc_lstmb = np.zeros(4 * H).astype('f')\n",
    "        \n",
    "        dec_embed = (rn(V, D) / 100).astype('f')\n",
    "        dec_lstmWx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        dec_lstmWh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        dec_lstmb = np.zeros(4 * H).astype('f')\n",
    "        dec_affineW = (rn(2 * H, V) / np.sqrt(H)).astype('f')\n",
    "        dec_affineb = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.params = [enc_embed, enc_lstmWx, enc_lstmWh, enc_lstmb, \\\n",
    "            dec_embed, dec_lstmWx, dec_lstmWh, dec_lstmb, dec_affineW, dec_affineb]\n",
    "        \n",
    "        self.grads = []\n",
    "        self.enc_lstm = [] \n",
    "        self.dec_lstm = []\n",
    "        self.attention = []\n",
    "        \n",
    "        self.time_idx = 0\n",
    "        self.vocab_size = vocab_size\n",
    "        self.wordvec_size = wordvec_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "\n",
    "        \n",
    "    def forward(self, enc_xs, ts):\n",
    "        enc_embed, enc_lstmWx, enc_lstmWh, enc_lstmb, \\\n",
    "            dec_embed, dec_lstmWx, dec_lstmWh, dec_lstmb, dec_affineW, dec_affineb = self.params\n",
    "        \n",
    "        self.enc_lstm = []\n",
    "        self.dec_lstm = []\n",
    "        \n",
    "        dec_xs = ts[:, :-1]\n",
    "        dec_ts = ts[:, 1:]\n",
    "        \n",
    "        batch_size, enc_time_size = enc_xs.shape\n",
    "        dec_time_size = dec_xs.shape[1]\n",
    "        \n",
    "        hidden_size = self.hidden_size\n",
    "               \n",
    "        ####################### ENCODER #######################\n",
    "        \n",
    "        h_prev = np.zeros((batch_size, hidden_size), dtype='f')\n",
    "        c_prev = np.zeros((batch_size, hidden_size), dtype='f')\n",
    "        enc_hs = np.zeros((batch_size, enc_time_size, hidden_size), dtype='f')\n",
    "        for t in range(enc_time_size):\n",
    "            # embed\n",
    "            emb_out = enc_embed[enc_xs[:, t]]\n",
    "\n",
    "            # lstm\n",
    "            A = np.matmul(emb_out, enc_lstmWx) + np.matmul(h_prev, enc_lstmWh) + enc_lstmb\n",
    "            f = A[:, :hidden_size]\n",
    "            g = A[:, hidden_size: 2*hidden_size]\n",
    "            i = A[:, 2*hidden_size: 3*hidden_size]\n",
    "            o = A[:, 3*hidden_size:]\n",
    "\n",
    "            f = self.sigmoid(f)\n",
    "            g = np.tanh(g)\n",
    "            i = self.sigmoid(i)\n",
    "            o = self.sigmoid(o)\n",
    "\n",
    "            c_next = f * c_prev + g * i\n",
    "            h_next = o * np.tanh(c_next)\n",
    "            \n",
    "            enc_hs[:, t, :] = h_next\n",
    "            \n",
    "            self.enc_lstm.append((emb_out, h_prev, c_prev, f, g, i, o, c_next, h_next))\n",
    "            \n",
    "            c_prev = c_next\n",
    "            h_prev = h_next\n",
    "            \n",
    "        ####################### DECODER #######################\n",
    "        \n",
    "        c_prev = np.zeros((batch_size, hidden_size), dtype='f')\n",
    "        dec_hs = np.zeros((batch_size, dec_time_size, hidden_size), dtype='f')\n",
    "        for t in range(dec_time_size):\n",
    "            # embed\n",
    "            emb_out = dec_embed[dec_xs[:, t]]\n",
    "            \n",
    "            # lstm\n",
    "            A = np.matmul(emb_out, dec_lstmWx) + np.matmul(h_prev, dec_lstmWh) + dec_lstmb\n",
    "            f = A[:, :hidden_size]\n",
    "            g = A[:, hidden_size: 2*hidden_size]\n",
    "            i = A[:, 2*hidden_size: 3*hidden_size]\n",
    "            o = A[:, 3*hidden_size:]\n",
    "\n",
    "            f = self.sigmoid(f)\n",
    "            g = np.tanh(g)\n",
    "            i = self.sigmoid(i)\n",
    "            o = self.sigmoid(o)\n",
    "\n",
    "            c_next = f * c_prev + g * i\n",
    "            h_next = o * np.tanh(c_next)\n",
    "            \n",
    "            dec_hs[:, t, :] = h_next\n",
    "            \n",
    "            self.dec_lstm.append((emb_out, h_prev, c_prev, f, g, i, o, c_next, h_next))\n",
    "            c_prev = c_next\n",
    "            h_prev = h_next\n",
    "            \n",
    "        # attention\n",
    "        self.attention = []\n",
    "        \n",
    "        c = np.zeros_like(dec_hs)\n",
    "        for t in range(dec_time_size):\n",
    "            hr = dec_hs[:, t, :].reshape(batch_size, 1, hidden_size).repeat(enc_time_size, axis=1)\n",
    "            t1 = enc_hs * hr\n",
    "            s = np.sum(t1, axis=2)\n",
    "            a = self.softmax(s)\n",
    "            ar = a.reshape(batch_size, enc_time_size, 1).repeat(hidden_size, axis=2)\n",
    "            t2 = enc_hs * ar\n",
    "            c[:, t, :] = np.sum(t2, axis=1)\n",
    "            self.attention.append([hr, ar, a])\n",
    "            \n",
    "        # affine\n",
    "        concat_attention = np.concatenate((c, dec_hs), axis=2)\n",
    "        dec_affine_out = np.matmul(concat_attention, dec_affineW) + dec_affineb\n",
    "        \n",
    "        # softmax\n",
    "        y = self.softmax(dec_affine_out.reshape(batch_size, dec_time_size, -1))\n",
    "        loss = self.getLoss(y, dec_ts)\n",
    "        \n",
    "        self.xs = enc_xs, enc_hs, dec_xs, dec_hs, concat_attention, dec_affine_out, y\n",
    "        self.ts = dec_ts\n",
    "        return loss\n",
    "        \n",
    "    def backward(self):\n",
    "        enc_embed, enc_lstmWx, enc_lstmWh, enc_lstmb, \\\n",
    "            dec_embed, dec_lstmWx, dec_lstmWh, dec_lstmb, dec_affineW, dec_affineb = self.params\n",
    "        enc_xs, enc_hs, dec_xs, dec_hs, concat_attention, dec_affine_out, y = self.xs\n",
    "        dec_ts = self.ts\n",
    "        \n",
    "        vocab_size = self.vocab_size        \n",
    "        wordvec_size = self.wordvec_size\n",
    "        batch_size, enc_time_size = enc_xs.shape\n",
    "        dec_time_size = dec_xs.shape[1]\n",
    "        \n",
    "        ####################### DECODER #######################\n",
    "        \n",
    "        # softmax\n",
    "        y = y.reshape(batch_size * dec_time_size, -1)\n",
    "        y[np.arange(batch_size * dec_time_size), dec_ts.reshape(batch_size * dec_time_size)] -= 1\n",
    "        soft_dout = y\n",
    "        \n",
    "        # affine\n",
    "        concat_attention = concat_attention.reshape(batch_size * dec_time_size, -1)\n",
    "        affine_dout = np.matmul(soft_dout, dec_affineW.T).reshape(batch_size, dec_time_size, -1)\n",
    "        affinedW = np.matmul(concat_attention.T, soft_dout) \n",
    "        affinedb = np.sum(soft_dout, axis=0)\n",
    "        \n",
    "        dc, affine_dout = affine_dout[:, :, :hidden_size], affine_dout[:, :, hidden_size:]\n",
    "                \n",
    "        # attention\n",
    "        enc_dhs = np.zeros_like(enc_hs)\n",
    "        dec_dhs = np.zeros_like(dec_hs)\n",
    "        for t in range(dec_time_size):\n",
    "            hr, ar, a = self.attention[t]\n",
    "            \n",
    "            # t2\n",
    "            dt2 = dc[:, t, :].reshape(batch_size, 1, hidden_size).repeat(enc_time_size, axis=1)\n",
    "            \n",
    "            # ar\n",
    "            dar = dt2 * enc_hs\n",
    "            enc_dhs0 = dt2 * ar\n",
    "            \n",
    "            # a \n",
    "            da = np.sum(dar, axis=2)\n",
    "            \n",
    "            # s\n",
    "            ds = a * da\n",
    "            sum_ds = np.sum(ds, axis=1, keepdims=True)\n",
    "            ds -= a * sum_ds\n",
    "            \n",
    "            # t1\n",
    "            dt1 = ds.reshape(batch_size, enc_time_size, -1).repeat(hidden_size, axis=2)\n",
    "            \n",
    "            # hr\n",
    "            dhr = dt1 * enc_hs\n",
    "            enc_dhs1 = dt1 * hr\n",
    "            \n",
    "            # h\n",
    "            dh = np.sum(dhr, axis=1)\n",
    "            \n",
    "            enc_dhs += (enc_dhs0 + enc_dhs1)\n",
    "            dec_dhs[:, t, :] = dh\n",
    "            \n",
    "        dec_dhs += affine_dout\n",
    "\n",
    "        # lstm\n",
    "        dec_lstmdWx = np.zeros_like(dec_lstmWx)\n",
    "        dec_lstmdWh = np.zeros_like(dec_lstmWh)\n",
    "        dec_lstmdb = np.zeros_like(dec_lstmb)\n",
    "        \n",
    "        dec_lstm_douts = np.empty((batch_size, dec_time_size, wordvec_size), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "        for t in reversed(range(dec_time_size)):            \n",
    "            emb_out, h_prev, c_prev, f, g, i, o, c_next, _ = self.dec_lstm[t]\n",
    "            dh_next = dec_dhs[:, t, :] + dh\n",
    "            dc_next = dc\n",
    "\n",
    "            tanh_c_next = np.tanh(c_next)\n",
    "            \n",
    "            ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "            \n",
    "            dc_prev = ds * f\n",
    "            \n",
    "            di = ds * g\n",
    "            df = ds * c_prev\n",
    "            do = dh_next * tanh_c_next\n",
    "            dg = ds * i\n",
    "            \n",
    "            di *= i * (1 - i)\n",
    "            df *= f * (1 - f)\n",
    "            do *= o * (1 - o)\n",
    "            dg *= (1 - g ** 2)\n",
    "            \n",
    "            dA = np.hstack((df, dg, di, do))\n",
    "            \n",
    "            dWh = np.matmul(h_prev.T, dA)\n",
    "            dWx = np.matmul(emb_out.T, dA)\n",
    "            db = np.sum(dA, axis=0)\n",
    "            \n",
    "            dec_lstm_douts[:, t, :] = np.matmul(dA, dec_lstmWx.T)\n",
    "            dh_prev = np.matmul(dA, dec_lstmWh.T)\n",
    "        \n",
    "            dec_lstmdWx += dWx\n",
    "            dec_lstmdWh += dWh\n",
    "            dec_lstmdb += db\n",
    "            dh = dh_prev\n",
    "            dc = dc_prev\n",
    "            \n",
    "        # embed\n",
    "        dec_embed_dout = np.zeros_like(dec_embed)\n",
    "        for t in range(dec_time_size):\n",
    "            np.add.at(dec_embed_dout, dec_xs[:, t], dec_lstm_douts[:, t, :])\n",
    "            \n",
    "        ####################### ENCODER #######################\n",
    "        \n",
    "        # lstm\n",
    "        enc_lstmdWx = np.zeros_like(enc_lstmWx)\n",
    "        enc_lstmdWh = np.zeros_like(enc_lstmWh)\n",
    "        enc_lstmdb = np.zeros_like(enc_lstmb)\n",
    "        \n",
    "        enc_lstm_douts = np.empty((batch_size, enc_time_size, wordvec_size), dtype='f')\n",
    "        dc = 0\n",
    "        for t in reversed(range(enc_time_size)):            \n",
    "            emb_out, h_prev, c_prev, f, g, i, o, c_next, _ = self.enc_lstm[t]\n",
    "            dh_next = enc_dhs[:, t, :] + dh\n",
    "            dc_next = dc\n",
    "\n",
    "            tanh_c_next = np.tanh(c_next)\n",
    "            \n",
    "            ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "            \n",
    "            dc_prev = ds * f\n",
    "            \n",
    "            di = ds * g\n",
    "            df = ds * c_prev\n",
    "            do = dh_next * tanh_c_next\n",
    "            dg = ds * i\n",
    "            \n",
    "            di *= i * (1 - i)\n",
    "            df *= f * (1 - f)\n",
    "            do *= o * (1 - o)\n",
    "            dg *= (1 - g ** 2)\n",
    "            \n",
    "            dA = np.hstack((df, dg, di, do))\n",
    "            \n",
    "            dWh = np.matmul(h_prev.T, dA)\n",
    "            dWx = np.matmul(emb_out.T, dA)\n",
    "            db = np.sum(dA, axis=0)\n",
    "            \n",
    "            enc_lstm_douts[:, t, :] = np.matmul(dA, enc_lstmWx.T)\n",
    "            dh_prev = np.matmul(dA, enc_lstmWh.T)\n",
    "        \n",
    "            enc_lstmdWx += dWx\n",
    "            enc_lstmdWh += dWh\n",
    "            enc_lstmdb += db\n",
    "            dh = dh_prev\n",
    "            dc = dc_prev        \n",
    "        \n",
    "        # embed\n",
    "        enc_embed_dout = np.zeros_like(enc_embed)\n",
    "        for t in range(enc_time_size):\n",
    "            np.add.at(enc_embed_dout, enc_xs[:, t], enc_lstm_douts[:, t, :])\n",
    "            \n",
    "        self.grads = enc_embed_dout, enc_lstmdWx, enc_lstmdWh, enc_lstmdb, \\\n",
    "            dec_embed_dout, dec_lstmdWx, dec_lstmdWh, dec_lstmdb, affinedW, affinedb\n",
    "        \n",
    "    def softmax(self, y):\n",
    "        y = y - np.max(y)\n",
    "        y = np.exp(y)\n",
    "        y = y / y.sum(axis=-1, keepdims=True)\n",
    "        return y\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def getLoss(self, y, t):\n",
    "        N, T, V = y.shape\n",
    "\n",
    "        y = y.reshape(N * T, V)\n",
    "        t = t.reshape(N * T)\n",
    "\n",
    "        ls = np.log(y[np.arange(N * T), t])\n",
    "        return -np.sum(ls) / (N * T)\n",
    "                \n",
    "    def updateAdam(self, lr):\n",
    "        params = self.params\n",
    "        grads = self.grads\n",
    "        \n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "                \n",
    "        self.iter += 1\n",
    "        lr_t = lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "        \n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n",
    "        self.params = params\n",
    "        \n",
    "    def clip_grads(self, max_norm):\n",
    "        grads = self.grads\n",
    "        total_norm = 0\n",
    "        \n",
    "        for grad in grads:\n",
    "            total_norm += np.sum(grad**2)\n",
    "            \n",
    "        total_norm = np.sqrt(total_norm)\n",
    "        \n",
    "        rate = max_norm / (total_norm + 1e-6)\n",
    "        \n",
    "        if rate  < 1:\n",
    "            for grad in grads:\n",
    "                grad *= rate\n",
    "        self.grads = grads\n",
    "            \n",
    "    def generate(self, xs, start_id, sample_size):\n",
    "        enc_embed, enc_lstmWx, enc_lstmWh, enc_lstmb, \\\n",
    "            dec_embed, dec_lstmWx, dec_lstmWh, dec_lstmb, dec_affineW, dec_affineb = self.params\n",
    "        \n",
    "        \n",
    "        enc_time_size = xs.shape[1]\n",
    "        hidden_size = self.hidden_size\n",
    "        \n",
    "        ####################### ENCODER #######################\n",
    "        \n",
    "        h_prev = np.zeros((hidden_size), dtype='f')\n",
    "        c_prev = np.zeros((hidden_size), dtype='f')\n",
    "        enc_hs = np.zeros((enc_time_size, hidden_size), dtype='f')\n",
    "        for t in range(enc_time_size):\n",
    "            # embed\n",
    "            emb_out = enc_embed[xs[:, t]]\n",
    "\n",
    "            # lstm\n",
    "            A = np.matmul(emb_out, enc_lstmWx) + np.matmul(h_prev, enc_lstmWh) + enc_lstmb\n",
    "            f = A[:, :hidden_size]\n",
    "            g = A[:, hidden_size: 2*hidden_size]\n",
    "            i = A[:, 2*hidden_size: 3*hidden_size]\n",
    "            o = A[:, 3*hidden_size:]\n",
    "\n",
    "            f = self.sigmoid(f)\n",
    "            g = np.tanh(g)\n",
    "            i = self.sigmoid(i)\n",
    "            o = self.sigmoid(o)\n",
    "\n",
    "            c_next = f * c_prev + g * i\n",
    "            h_next = o * np.tanh(c_next)\n",
    "            \n",
    "            enc_hs[t, :] = h_next\n",
    "            \n",
    "            c_prev = c_next\n",
    "            h_prev = h_next\n",
    "                        \n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "\n",
    "        ####################### DECODER #######################\n",
    "\n",
    "        c_prev = np.zeros((hidden_size), dtype='f')\n",
    "        for _ in range(sample_size):\n",
    "            \n",
    "            # embed\n",
    "            emb_out = dec_embed[sample_id]\n",
    "\n",
    "            # lstm\n",
    "            A = np.matmul(emb_out, dec_lstmWx) + np.matmul(h_prev, dec_lstmWh) + dec_lstmb\n",
    "            f = A[:, :hidden_size]\n",
    "            g = A[:, hidden_size: 2*hidden_size]\n",
    "            i = A[:, 2*hidden_size: 3*hidden_size]\n",
    "            o = A[:, 3*hidden_size:]\n",
    "\n",
    "            f = self.sigmoid(f)\n",
    "            g = np.tanh(g)\n",
    "            i = self.sigmoid(i)\n",
    "            o = self.sigmoid(o)\n",
    "\n",
    "            c_next = f * c_prev + g * i\n",
    "            h_next = o * np.tanh(c_next)\n",
    "\n",
    "            c_prev = c_next\n",
    "            h_prev = h_next\n",
    "            \n",
    "            # attention\n",
    "            hr = h_next.reshape(1, 1, hidden_size).repeat(enc_time_size, axis=1)\n",
    "            t1 = enc_hs * hr\n",
    "            s = np.sum(t1, axis=2)\n",
    "            a = self.softmax(s)\n",
    "            ar = a.reshape(1, enc_time_size, 1).repeat(hidden_size, axis=2)\n",
    "            t2 = enc_hs * ar\n",
    "            c = np.sum(t2, axis=1)\n",
    "            \n",
    "            # affine\n",
    "            concat_attention = np.concatenate((c, h_next), axis=1)\n",
    "            dec_affine_out = np.matmul(concat_attention, dec_affineW) + dec_affineb\n",
    "            \n",
    "            # softmax\n",
    "            y = self.softmax(dec_affine_out)\n",
    "            sample_id = np.argmax(y.flatten())\n",
    "            sampled.append(int(sample_id))\n",
    "        \n",
    "        return sampled\n",
    "            \n",
    "    def eval_seq2seq(self, question, correct, id_to_char,\n",
    "                     verbos=False, is_reverse=False):\n",
    "        correct = correct.flatten()\n",
    "        # 머릿글자\n",
    "        start_id = correct[0]\n",
    "        correct = correct[1:]\n",
    "        guess = self.generate(question, start_id, len(correct))\n",
    "\n",
    "        # 문자열로 변환\n",
    "        question = ''.join([id_to_char[int(c)] for c in question.flatten()])\n",
    "        correct = ''.join([id_to_char[int(c)] for c in correct])\n",
    "        guess = ''.join([id_to_char[int(c)] for c in guess])\n",
    "\n",
    "        if verbos:\n",
    "            if is_reverse:\n",
    "                question = question[::-1]\n",
    "\n",
    "            print('Q', question)\n",
    "            print('T', correct)\n",
    "\n",
    "            if correct == guess:\n",
    "                mark = 'O'\n",
    "                print(mark + ' ' + guess)\n",
    "            else:\n",
    "                mark = 'X'\n",
    "                print(mark + ' ' + guess)\n",
    "            print('---')\n",
    "\n",
    "        return 1 if guess == correct else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c6b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f77ac798",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "data_size = len(x_train)\n",
    "learning_rate = 0.001\n",
    "data_size = len(x_train)\n",
    "max_iters = data_size // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87cfdf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionSeq2seqlm(vocab_size, wordvec_size, hidden_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ffdf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iters 1 / 351 | time 1[s] | loss 4.08\n",
      "| epoch 1 |  iters 21 / 351 | time 36[s] | loss 2.99\n",
      "| epoch 1 |  iters 41 / 351 | time 68[s] | loss 1.80\n",
      "| epoch 1 |  iters 61 / 351 | time 98[s] | loss 1.51\n",
      "| epoch 1 |  iters 81 / 351 | time 130[s] | loss 1.22\n",
      "| epoch 1 |  iters 101 / 351 | time 161[s] | loss 1.11\n",
      "| epoch 1 |  iters 121 / 351 | time 185[s] | loss 1.05\n",
      "| epoch 1 |  iters 141 / 351 | time 209[s] | loss 1.03\n",
      "| epoch 1 |  iters 161 / 351 | time 233[s] | loss 1.01\n",
      "| epoch 1 |  iters 181 / 351 | time 266[s] | loss 0.99\n",
      "| epoch 1 |  iters 201 / 351 | time 294[s] | loss 0.97\n",
      "| epoch 1 |  iters 221 / 351 | time 320[s] | loss 0.96\n",
      "| epoch 1 |  iters 241 / 351 | time 345[s] | loss 0.94\n",
      "| epoch 1 |  iters 261 / 351 | time 370[s] | loss 0.94\n",
      "| epoch 1 |  iters 281 / 351 | time 394[s] | loss 0.91\n",
      "| epoch 1 |  iters 301 / 351 | time 418[s] | loss 0.90\n",
      "| epoch 1 |  iters 321 / 351 | time 442[s] | loss 0.88\n",
      "| epoch 1 |  iters 341 / 351 | time 466[s] | loss 0.86\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1999-01-14\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1999-08-21\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 2005-05-05\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1977-08-29\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1999-09-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1999-01-16\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 2004-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2004-08-20\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1999-01-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1999-08-26\n",
      "---\n",
      "accuracy 0.180%\n",
      "| epoch 2 |  iters 1 / 351 | time 558[s] | loss 0.84\n",
      "| epoch 2 |  iters 21 / 351 | time 585[s] | loss 0.80\n",
      "| epoch 2 |  iters 41 / 351 | time 610[s] | loss 0.78\n",
      "| epoch 2 |  iters 61 / 351 | time 635[s] | loss 0.75\n",
      "| epoch 2 |  iters 81 / 351 | time 660[s] | loss 0.72\n",
      "| epoch 2 |  iters 101 / 351 | time 685[s] | loss 0.70\n",
      "| epoch 2 |  iters 121 / 351 | time 712[s] | loss 0.66\n",
      "| epoch 2 |  iters 141 / 351 | time 742[s] | loss 0.65\n",
      "| epoch 2 |  iters 161 / 351 | time 776[s] | loss 0.63\n",
      "| epoch 2 |  iters 181 / 351 | time 811[s] | loss 0.61\n",
      "| epoch 2 |  iters 201 / 351 | time 845[s] | loss 0.59\n",
      "| epoch 2 |  iters 221 / 351 | time 869[s] | loss 0.56\n",
      "| epoch 2 |  iters 241 / 351 | time 893[s] | loss 0.55\n",
      "| epoch 2 |  iters 261 / 351 | time 916[s] | loss 0.53\n",
      "| epoch 2 |  iters 281 / 351 | time 940[s] | loss 0.50\n",
      "| epoch 2 |  iters 301 / 351 | time 963[s] | loss 0.50\n",
      "| epoch 2 |  iters 321 / 351 | time 986[s] | loss 0.47\n",
      "| epoch 2 |  iters 341 / 351 | time 1008[s] | loss 0.45\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1998-10-05\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2001-12-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 2005-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 2012-12-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1983-07-28\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1999-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2008-08-00\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 2012-10-08\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2012-10-04\n",
      "---\n",
      "accuracy 6.000%\n",
      "| epoch 3 |  iters 1 / 351 | time 1096[s] | loss 0.43\n",
      "| epoch 3 |  iters 21 / 351 | time 1125[s] | loss 0.41\n",
      "| epoch 3 |  iters 41 / 351 | time 1154[s] | loss 0.39\n",
      "| epoch 3 |  iters 61 / 351 | time 1180[s] | loss 0.38\n",
      "| epoch 3 |  iters 81 / 351 | time 1205[s] | loss 0.36\n",
      "| epoch 3 |  iters 101 / 351 | time 1230[s] | loss 0.33\n",
      "| epoch 3 |  iters 121 / 351 | time 1256[s] | loss 0.32\n",
      "| epoch 3 |  iters 141 / 351 | time 1283[s] | loss 0.31\n",
      "| epoch 3 |  iters 161 / 351 | time 1311[s] | loss 0.30\n",
      "| epoch 3 |  iters 181 / 351 | time 1336[s] | loss 0.29\n",
      "| epoch 3 |  iters 201 / 351 | time 1362[s] | loss 0.29\n",
      "| epoch 3 |  iters 221 / 351 | time 1388[s] | loss 0.28\n",
      "| epoch 3 |  iters 241 / 351 | time 1414[s] | loss 0.28\n",
      "| epoch 3 |  iters 261 / 351 | time 1438[s] | loss 0.27\n",
      "| epoch 3 |  iters 281 / 351 | time 1462[s] | loss 0.26\n",
      "| epoch 3 |  iters 301 / 351 | time 1487[s] | loss 0.27\n",
      "| epoch 3 |  iters 321 / 351 | time 1510[s] | loss 0.26\n",
      "| epoch 3 |  iters 341 / 351 | time 1530[s] | loss 0.26\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1998-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2003-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 2012-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1973-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2008-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 2012-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2012-11-06\n",
      "---\n",
      "accuracy 10.280%\n",
      "| epoch 4 |  iters 1 / 351 | time 1596[s] | loss 0.26\n",
      "| epoch 4 |  iters 21 / 351 | time 1616[s] | loss 0.26\n",
      "| epoch 4 |  iters 41 / 351 | time 1637[s] | loss 0.25\n",
      "| epoch 4 |  iters 61 / 351 | time 1660[s] | loss 0.25\n",
      "| epoch 4 |  iters 81 / 351 | time 1682[s] | loss 0.25\n",
      "| epoch 4 |  iters 101 / 351 | time 1704[s] | loss 0.24\n",
      "| epoch 4 |  iters 121 / 351 | time 1725[s] | loss 0.24\n",
      "| epoch 4 |  iters 141 / 351 | time 1746[s] | loss 0.24\n",
      "| epoch 4 |  iters 161 / 351 | time 1766[s] | loss 0.24\n",
      "| epoch 4 |  iters 181 / 351 | time 1788[s] | loss 0.23\n",
      "| epoch 4 |  iters 201 / 351 | time 1811[s] | loss 0.23\n",
      "| epoch 4 |  iters 221 / 351 | time 1832[s] | loss 0.21\n",
      "| epoch 4 |  iters 241 / 351 | time 1852[s] | loss 0.20\n",
      "| epoch 4 |  iters 261 / 351 | time 1874[s] | loss 0.19\n",
      "| epoch 4 |  iters 281 / 351 | time 1895[s] | loss 0.19\n",
      "| epoch 4 |  iters 301 / 351 | time 1916[s] | loss 0.19\n",
      "| epoch 4 |  iters 321 / 351 | time 1938[s] | loss 0.17\n",
      "| epoch 4 |  iters 341 / 351 | time 1960[s] | loss 0.16\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2004-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 2000-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1990-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 2004-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2004-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "accuracy 32.700%\n",
      "| epoch 5 |  iters 1 / 351 | time 2028[s] | loss 0.16\n",
      "| epoch 5 |  iters 21 / 351 | time 2051[s] | loss 0.15\n",
      "| epoch 5 |  iters 41 / 351 | time 2073[s] | loss 0.11\n",
      "| epoch 5 |  iters 61 / 351 | time 2094[s] | loss 0.09\n",
      "| epoch 5 |  iters 81 / 351 | time 2116[s] | loss 0.08\n",
      "| epoch 5 |  iters 101 / 351 | time 2138[s] | loss 0.05\n",
      "| epoch 5 |  iters 121 / 351 | time 2159[s] | loss 0.05\n",
      "| epoch 5 |  iters 141 / 351 | time 2181[s] | loss 0.05\n",
      "| epoch 5 |  iters 161 / 351 | time 2203[s] | loss 0.04\n",
      "| epoch 5 |  iters 181 / 351 | time 2224[s] | loss 0.03\n",
      "| epoch 5 |  iters 201 / 351 | time 2243[s] | loss 0.03\n",
      "| epoch 5 |  iters 221 / 351 | time 2261[s] | loss 0.02\n",
      "| epoch 5 |  iters 241 / 351 | time 2280[s] | loss 0.02\n",
      "| epoch 5 |  iters 261 / 351 | time 2297[s] | loss 0.02\n",
      "| epoch 5 |  iters 281 / 351 | time 2314[s] | loss 0.01\n",
      "| epoch 5 |  iters 301 / 351 | time 2329[s] | loss 0.01\n",
      "| epoch 5 |  iters 321 / 351 | time 2347[s] | loss 0.01\n",
      "| epoch 5 |  iters 341 / 351 | time 2363[s] | loss 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "accuracy 98.980%\n",
      "| epoch 6 |  iters 1 / 351 | time 2412[s] | loss 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 6 |  iters 21 / 351 | time 2428[s] | loss 0.00\n",
      "| epoch 6 |  iters 41 / 351 | time 2445[s] | loss 0.01\n",
      "| epoch 6 |  iters 61 / 351 | time 2462[s] | loss 0.00\n",
      "| epoch 6 |  iters 81 / 351 | time 2479[s] | loss 0.00\n",
      "| epoch 6 |  iters 101 / 351 | time 2495[s] | loss 0.01\n",
      "| epoch 6 |  iters 121 / 351 | time 2511[s] | loss 0.01\n",
      "| epoch 6 |  iters 141 / 351 | time 2527[s] | loss 0.00\n",
      "| epoch 6 |  iters 161 / 351 | time 2543[s] | loss 0.01\n",
      "| epoch 6 |  iters 181 / 351 | time 2558[s] | loss 0.00\n",
      "| epoch 6 |  iters 201 / 351 | time 2574[s] | loss 0.00\n",
      "| epoch 6 |  iters 221 / 351 | time 2590[s] | loss 0.00\n",
      "| epoch 6 |  iters 241 / 351 | time 2608[s] | loss 0.00\n",
      "| epoch 6 |  iters 261 / 351 | time 2625[s] | loss 0.00\n",
      "| epoch 6 |  iters 281 / 351 | time 2641[s] | loss 0.00\n",
      "| epoch 6 |  iters 301 / 351 | time 2657[s] | loss 0.01\n",
      "| epoch 6 |  iters 321 / 351 | time 2672[s] | loss 0.00\n",
      "| epoch 6 |  iters 341 / 351 | time 2689[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "accuracy 99.560%\n",
      "| epoch 7 |  iters 1 / 351 | time 2736[s] | loss 0.00\n",
      "| epoch 7 |  iters 21 / 351 | time 2752[s] | loss 0.00\n",
      "| epoch 7 |  iters 41 / 351 | time 2768[s] | loss 0.00\n",
      "| epoch 7 |  iters 61 / 351 | time 2783[s] | loss 0.00\n",
      "| epoch 7 |  iters 81 / 351 | time 2800[s] | loss 0.00\n",
      "| epoch 7 |  iters 101 / 351 | time 2816[s] | loss 0.00\n",
      "| epoch 7 |  iters 121 / 351 | time 2832[s] | loss 0.00\n",
      "| epoch 7 |  iters 141 / 351 | time 2848[s] | loss 0.00\n",
      "| epoch 7 |  iters 161 / 351 | time 2864[s] | loss 0.00\n",
      "| epoch 7 |  iters 181 / 351 | time 2881[s] | loss 0.00\n",
      "| epoch 7 |  iters 201 / 351 | time 2897[s] | loss 0.00\n",
      "| epoch 7 |  iters 221 / 351 | time 2914[s] | loss 0.00\n",
      "| epoch 7 |  iters 241 / 351 | time 2930[s] | loss 0.00\n",
      "| epoch 7 |  iters 261 / 351 | time 2946[s] | loss 0.00\n",
      "| epoch 7 |  iters 281 / 351 | time 2962[s] | loss 0.00\n",
      "| epoch 7 |  iters 301 / 351 | time 2979[s] | loss 0.00\n",
      "| epoch 7 |  iters 321 / 351 | time 2995[s] | loss 0.00\n",
      "| epoch 7 |  iters 341 / 351 | time 3011[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "accuracy 99.560%\n",
      "| epoch 8 |  iters 1 / 351 | time 3057[s] | loss 0.00\n",
      "| epoch 8 |  iters 21 / 351 | time 3075[s] | loss 0.00\n",
      "| epoch 8 |  iters 41 / 351 | time 3092[s] | loss 0.00\n",
      "| epoch 8 |  iters 61 / 351 | time 3109[s] | loss 0.00\n",
      "| epoch 8 |  iters 81 / 351 | time 3125[s] | loss 0.00\n",
      "| epoch 8 |  iters 101 / 351 | time 3141[s] | loss 0.00\n",
      "| epoch 8 |  iters 121 / 351 | time 3157[s] | loss 0.00\n",
      "| epoch 8 |  iters 141 / 351 | time 3173[s] | loss 0.00\n",
      "| epoch 8 |  iters 161 / 351 | time 3190[s] | loss 0.00\n",
      "| epoch 8 |  iters 181 / 351 | time 3206[s] | loss 0.00\n",
      "| epoch 8 |  iters 201 / 351 | time 3223[s] | loss 0.00\n",
      "| epoch 8 |  iters 221 / 351 | time 3239[s] | loss 0.00\n",
      "| epoch 8 |  iters 241 / 351 | time 3256[s] | loss 0.00\n",
      "| epoch 8 |  iters 261 / 351 | time 3272[s] | loss 0.00\n",
      "| epoch 8 |  iters 281 / 351 | time 3288[s] | loss 0.00\n",
      "| epoch 8 |  iters 301 / 351 | time 3305[s] | loss 0.00\n",
      "| epoch 8 |  iters 321 / 351 | time 3321[s] | loss 0.00\n",
      "| epoch 8 |  iters 341 / 351 | time 3337[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "accuracy 99.860%\n",
      "| epoch 9 |  iters 1 / 351 | time 3382[s] | loss 0.00\n",
      "| epoch 9 |  iters 21 / 351 | time 3399[s] | loss 0.00\n",
      "| epoch 9 |  iters 41 / 351 | time 3415[s] | loss 0.00\n",
      "| epoch 9 |  iters 61 / 351 | time 3431[s] | loss 0.00\n",
      "| epoch 9 |  iters 81 / 351 | time 3447[s] | loss 0.00\n",
      "| epoch 9 |  iters 101 / 351 | time 3464[s] | loss 0.00\n",
      "| epoch 9 |  iters 121 / 351 | time 3480[s] | loss 0.00\n",
      "| epoch 9 |  iters 141 / 351 | time 3497[s] | loss 0.00\n",
      "| epoch 9 |  iters 161 / 351 | time 3514[s] | loss 0.00\n",
      "| epoch 9 |  iters 181 / 351 | time 3531[s] | loss 0.00\n",
      "| epoch 9 |  iters 201 / 351 | time 3548[s] | loss 0.00\n",
      "| epoch 9 |  iters 221 / 351 | time 3565[s] | loss 0.00\n",
      "| epoch 9 |  iters 241 / 351 | time 3581[s] | loss 0.00\n",
      "| epoch 9 |  iters 261 / 351 | time 3597[s] | loss 0.00\n",
      "| epoch 9 |  iters 281 / 351 | time 3614[s] | loss 0.00\n",
      "| epoch 9 |  iters 301 / 351 | time 3630[s] | loss 0.00\n",
      "| epoch 9 |  iters 321 / 351 | time 3646[s] | loss 0.00\n",
      "| epoch 9 |  iters 341 / 351 | time 3663[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "accuracy 99.780%\n",
      "| epoch 10 |  iters 1 / 351 | time 3709[s] | loss 0.00\n",
      "| epoch 10 |  iters 21 / 351 | time 3725[s] | loss 0.00\n",
      "| epoch 10 |  iters 41 / 351 | time 3741[s] | loss 0.00\n",
      "| epoch 10 |  iters 61 / 351 | time 3758[s] | loss 0.00\n",
      "| epoch 10 |  iters 81 / 351 | time 3774[s] | loss 0.00\n",
      "| epoch 10 |  iters 101 / 351 | time 3791[s] | loss 0.00\n",
      "| epoch 10 |  iters 121 / 351 | time 3807[s] | loss 0.00\n",
      "| epoch 10 |  iters 141 / 351 | time 3824[s] | loss 0.00\n",
      "| epoch 10 |  iters 161 / 351 | time 3840[s] | loss 0.00\n",
      "| epoch 10 |  iters 181 / 351 | time 3857[s] | loss 0.00\n",
      "| epoch 10 |  iters 201 / 351 | time 3873[s] | loss 0.00\n",
      "| epoch 10 |  iters 221 / 351 | time 3889[s] | loss 0.00\n",
      "| epoch 10 |  iters 241 / 351 | time 3906[s] | loss 0.00\n",
      "| epoch 10 |  iters 261 / 351 | time 3922[s] | loss 0.00\n",
      "| epoch 10 |  iters 281 / 351 | time 3938[s] | loss 0.00\n",
      "| epoch 10 |  iters 301 / 351 | time 3954[s] | loss 0.00\n",
      "| epoch 10 |  iters 321 / 351 | time 3971[s] | loss 0.00\n",
      "| epoch 10 |  iters 341 / 351 | time 3986[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "acc_list = []\n",
    "loss_list = []\n",
    "start_time = time.time()\n",
    "total_loss, loss_count = 0, 0\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    idx = np.random.permutation(np.arange(data_size))\n",
    "    x_train = x_train[idx]\n",
    "    t_train = t_train[idx]\n",
    "    \n",
    "    for iters in range(max_iters):\n",
    "        batch_x = x_train[(iters * batch_size) : (iters + 1) * batch_size]\n",
    "        batch_t = t_train[(iters * batch_size) : (iters + 1) * batch_size]\n",
    "        \n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        model.clip_grads(max_grad)\n",
    "        model.updateAdam(learning_rate)\n",
    "        \n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "        \n",
    "        if iters % 20 == 0:\n",
    "            avg_loss = total_loss / loss_count\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('| epoch %d |  iters %d / %d | time %d[s] | loss %.2f'\n",
    "                % (epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "            loss_list.append(avg_loss)\n",
    "            total_loss, loss_count = 0, 0\n",
    "            \n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += model.eval_seq2seq(question, correct, id_to_char, verbose)\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('accuracy %.3f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a975af",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.arange(len(acc_list))\n",
    "plt.plot(acc, acc_list, label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
