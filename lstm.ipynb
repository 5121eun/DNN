{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "901b4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from dataset import ptb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5923d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstmlm:\n",
    "    \n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        \n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed = (rn(V, D) / 100).astype('f')\n",
    "        lstmWx = rn(D, H*4) / np.sqrt(D)\n",
    "        lstmWh = rn(H, H*4) / np.sqrt(H)\n",
    "        lstmb = np.zeros((H*4), dtype='f')\n",
    "        affineW = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affineb = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.params = [embed, lstmWx, lstmWh, lstmb, affineW, affineb]\n",
    "        self.grads = []\n",
    "        self.lstm = []\n",
    "        \n",
    "        self.wordvec_size = wordvec_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.time_idx = 0\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        embed, lstmWx, lstmWh, lstmb, affineW, affineb = self.params\n",
    "        \n",
    "        batch_size, time_size = x.shape\n",
    "        wordvec_size = self.wordvec_size\n",
    "        hidden_size = self.hidden_size\n",
    "        \n",
    "        h_prev = np.zeros((batch_size, hidden_size), dtype='f')\n",
    "        c_prev = np.zeros((batch_size, hidden_size), dtype='f')\n",
    "        hs = np.empty((batch_size, time_size, hidden_size), dtype='f')\n",
    "        for t in range(time_size):\n",
    "            # embed\n",
    "            emb_out = embed[x[:, t]]\n",
    "            \n",
    "            # lstm\n",
    "            A = np.matmul(emb_out, lstmWx) + np.matmul(h_prev, lstmWh) + lstmb\n",
    "            f = A[:, :hidden_size]\n",
    "            g = A[:, hidden_size: 2*hidden_size]\n",
    "            i = A[:, 2*hidden_size: 3*hidden_size]\n",
    "            o = A[:, 3*hidden_size:]\n",
    "\n",
    "            f = self.sigmoid(f)\n",
    "            g = np.tanh(g)\n",
    "            i = self.sigmoid(i)\n",
    "            o = self.sigmoid(o)\n",
    "\n",
    "            c_next = f * c_prev + g * i\n",
    "            h_next = o * np.tanh(c_prev)\n",
    "            \n",
    "            self.lstm.append((emb_out, h_prev, c_prev, f, g, i, o, c_next, h_next))\n",
    "            c_prev = c_next\n",
    "            h_prev = h_next\n",
    "            \n",
    "            hs[:, t, :] = h_prev\n",
    "        \n",
    "        # affine\n",
    "        affine_out = np.matmul(hs, affineW) + affineb\n",
    "        \n",
    "        # softmax\n",
    "        y = self.softmax(affine_out)\n",
    "        \n",
    "        loss = self.getLoss(y, batch_t)\n",
    "        self.xs = x, hs, affine_out, y\n",
    "\n",
    "        return y, loss\n",
    "    \n",
    "    def backward(self, t):\n",
    "        embed, lstmWx, lstmWh, lstmb, affineW, affineb = self.params\n",
    "        x, hs, affine_out, y = self.xs\n",
    "        \n",
    "        wordvec_size = self.wordvec_size\n",
    "        vocab_size = self.vocab_size\n",
    "        batch_size, time_size = x.shape\n",
    "        \n",
    "        # softmax\n",
    "        y = y.reshape(batch_size * time_size, -1)\n",
    "        t = t.reshape(batch_size * time_size)\n",
    "        y[np.arange(batch_size * time_size), t] -= 1\n",
    "        soft_dout = y\n",
    "        \n",
    "        # affine\n",
    "        affine_dout = np.matmul(soft_dout, affineW.T).reshape(batch_size, time_size, -1) # (b, t, h)\n",
    "        affinedW = np.matmul(hs.reshape(batch_size * time_size, -1).T, soft_dout) # (h, v)\n",
    "        affinedb = np.sum(soft_dout, axis=0)\n",
    "        \n",
    "        # lstm\n",
    "        lstmdWx = np.zeros_like(lstmWx)\n",
    "        lstmdWh = np.zeros_like(lstmWh)\n",
    "        lstmdb = np.zeros_like(lstmb)\n",
    "        \n",
    "        lstm_douts = np.empty((batch_size, time_size, wordvec_size), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "        for t in reversed(range(time_size)):            \n",
    "            emb_out, h_prev, c_prev, f, g, i, o, c_next, _ = self.lstm[t]\n",
    "            dh_next = affine_dout[:, t, :] + dh\n",
    "            dc_next = dc\n",
    "\n",
    "            tanh_c_next = np.tanh(c_next)\n",
    "            \n",
    "            ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "            \n",
    "            dc_prev = ds * f\n",
    "            \n",
    "            di = ds * g\n",
    "            df = ds * c_prev\n",
    "            do = dh_next * tanh_c_next\n",
    "            dg = ds * i\n",
    "            \n",
    "            di *= i * (1 - i)\n",
    "            df *= f * (1 - f)\n",
    "            do *= o * (1 - o)\n",
    "            dg *= (1 - g ** 2)\n",
    "            \n",
    "            dA = np.hstack((df, dg, di, do))\n",
    "            \n",
    "            dWh = np.matmul(h_prev.T, dA)\n",
    "            dWx = np.matmul(emb_out.T, dA)\n",
    "            db = np.sum(dA, axis=0)\n",
    "            \n",
    "            lstm_douts[:, t, :] = np.matmul(dA, lstmWx.T)\n",
    "            dh_prev = np.matmul(dA, lstmWh.T)\n",
    "        \n",
    "            lstmdWx += dWx\n",
    "            lstmdWh += dWh\n",
    "            lstmdb += db\n",
    "            dh = dh_prev\n",
    "            dc = dc_prev\n",
    "        \n",
    "        # embed\n",
    "        embed_dout = np.zeros_like(embed)\n",
    "        for t in range(time_size):\n",
    "            np.add.at(embed_dout, x[:, t], lstm_douts[:, t, :])\n",
    "        \n",
    "        self.grads = embed_dout, lstmdWx, lstmdWh, lstmdb, affinedW, affinedb\n",
    "        \n",
    "        \n",
    "    def softmax(self, y):\n",
    "        y = y - np.max(y)\n",
    "        y = np.exp(y)\n",
    "        y = y / y.sum(axis=2, keepdims=True)\n",
    "        return y\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def update(self, lr):\n",
    "        for i in range(len(self.params)):\n",
    "            self.params[i] -= self.grads[i] * lr      \n",
    "            \n",
    "    def getLoss(self, y, t):\n",
    "        N, T, V = y.shape\n",
    "\n",
    "        y = y.reshape(N * T, V)\n",
    "        t = t.reshape(N * T)\n",
    "\n",
    "        ls = np.log(y[np.arange(N * T), t])\n",
    "        return -np.sum(ls) / (N * T)\n",
    "    \n",
    "    def get_batch(self, x, t, batch_size, time_size):\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "\n",
    "        data_size = len(x)\n",
    "        jump = data_size // batch_size\n",
    "        offsets = [i * jump for i in range(batch_size)]  # 배치에서 각 샘플을 읽기 시작하는 위치\n",
    "\n",
    "        for time in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, time] = x[(offset + self.time_idx) % data_size]\n",
    "                batch_t[i, time] = t[(offset + self.time_idx) % data_size]\n",
    "            self.time_idx += 1\n",
    "        return batch_x, batch_t\n",
    "    \n",
    "    def clip_grads(self, max_norm):\n",
    "        grads = self.grads\n",
    "        total_norm = 0\n",
    "        \n",
    "        for grad in grads:\n",
    "            total_norm += np.sum(grad**2)\n",
    "            \n",
    "        total_norm = np.sqrt(total_norm)\n",
    "        \n",
    "        rate = max_norm / (total_norm + 1e-6)\n",
    "        \n",
    "        if rate  < 1:\n",
    "            for grad in grads:\n",
    "                grad *= rate\n",
    "        self.grads = grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b52e1b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]  # 입력\n",
    "ts = corpus[1:]  # 출력（정답 레이블）\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 20\n",
    "wordvec_size = 100\n",
    "hidden_size = 100  # LSTM의 은닉 상태 벡터의 원소 수\n",
    "time_size = 35  # LSTM을 펼치는 크기\n",
    "lr = 15.0\n",
    "max_epoch = 1\n",
    "max_iters = len(xs) // (batch_size * time_size)\n",
    "eval_interval = 20\n",
    "max_grad = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b15a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lstmlm(vocab_size, wordvec_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2cd95d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 | 반복 1 | 시간 0[s] | loss 9.21 | 퍼플렉서티 10001.21\n",
      "| 에폭 1 | 반복 21 | 시간 2[s] | loss 7.10 | 퍼플렉서티 2114.84\n",
      "| 에폭 1 | 반복 41 | 시간 4[s] | loss 7.10 | 퍼플렉서티 1370.81\n",
      "| 에폭 1 | 반복 61 | 시간 7[s] | loss 7.40 | 퍼플렉서티 1117.10\n",
      "| 에폭 1 | 반복 81 | 시간 9[s] | loss 6.68 | 퍼플렉서티 992.92\n",
      "| 에폭 1 | 반복 101 | 시간 12[s] | loss 6.65 | 퍼플렉서티 865.58\n",
      "| 에폭 1 | 반복 121 | 시간 16[s] | loss 6.65 | 퍼플렉서티 874.38\n",
      "| 에폭 1 | 반복 141 | 시간 19[s] | loss 6.56 | 퍼플렉서티 858.80\n",
      "| 에폭 1 | 반복 161 | 시간 22[s] | loss 6.65 | 퍼플렉서티 841.59\n",
      "| 에폭 1 | 반복 181 | 시간 25[s] | loss 6.67 | 퍼플렉서티 851.05\n",
      "| 에폭 1 | 반복 201 | 시간 28[s] | loss 6.57 | 퍼플렉서티 795.33\n",
      "| 에폭 1 | 반복 221 | 시간 31[s] | loss 6.71 | 퍼플렉서티 800.55\n",
      "| 에폭 1 | 반복 241 | 시간 34[s] | loss 6.55 | 퍼플렉서티 749.70\n",
      "| 에폭 1 | 반복 261 | 시간 37[s] | loss 6.61 | 퍼플렉서티 783.09\n",
      "| 에폭 1 | 반복 281 | 시간 41[s] | loss 6.25 | 퍼플렉서티 778.42\n",
      "| 에폭 1 | 반복 301 | 시간 44[s] | loss 6.45 | 퍼플렉서티 709.36\n",
      "| 에폭 1 | 반복 321 | 시간 48[s] | loss 6.30 | 퍼플렉서티 623.31\n",
      "| 에폭 1 | 반복 341 | 시간 52[s] | loss 6.69 | 퍼플렉서티 700.22\n",
      "| 에폭 1 | 반복 361 | 시간 56[s] | loss 6.57 | 퍼플렉서티 750.16\n",
      "| 에폭 1 | 반복 381 | 시간 59[s] | loss 6.15 | 퍼플렉서티 666.56\n",
      "| 에폭 1 | 반복 401 | 시간 63[s] | loss 6.59 | 퍼플렉서티 678.48\n",
      "| 에폭 1 | 반복 421 | 시간 67[s] | loss 6.59 | 퍼플렉서티 683.77\n",
      "| 에폭 1 | 반복 441 | 시간 70[s] | loss 6.26 | 퍼플렉서티 660.56\n",
      "| 에폭 1 | 반복 461 | 시간 74[s] | loss 6.65 | 퍼플렉서티 661.20\n",
      "| 에폭 1 | 반복 481 | 시간 77[s] | loss 6.44 | 퍼플렉서티 648.02\n",
      "| 에폭 1 | 반복 501 | 시간 81[s] | loss 6.43 | 퍼플렉서티 644.67\n",
      "| 에폭 1 | 반복 521 | 시간 85[s] | loss 6.34 | 퍼플렉서티 638.94\n",
      "| 에폭 1 | 반복 541 | 시간 88[s] | loss 6.68 | 퍼플렉서티 677.98\n",
      "| 에폭 1 | 반복 561 | 시간 92[s] | loss 6.49 | 퍼플렉서티 632.87\n",
      "| 에폭 1 | 반복 581 | 시간 95[s] | loss 6.66 | 퍼플렉서티 586.24\n",
      "| 에폭 1 | 반복 601 | 시간 99[s] | loss 6.52 | 퍼플렉서티 719.21\n",
      "| 에폭 1 | 반복 621 | 시간 103[s] | loss 6.41 | 퍼플렉서티 675.59\n",
      "| 에폭 1 | 반복 641 | 시간 106[s] | loss 6.28 | 퍼플렉서티 620.66\n",
      "| 에폭 1 | 반복 661 | 시간 110[s] | loss 6.61 | 퍼플렉서티 616.60\n",
      "| 에폭 1 | 반복 681 | 시간 113[s] | loss 6.53 | 퍼플렉서티 538.71\n",
      "| 에폭 1 | 반복 701 | 시간 117[s] | loss 6.38 | 퍼플렉서티 582.14\n",
      "| 에폭 1 | 반복 721 | 시간 120[s] | loss 6.21 | 퍼플렉서티 602.22\n",
      "| 에폭 1 | 반복 741 | 시간 123[s] | loss 6.16 | 퍼플렉서티 519.96\n",
      "| 에폭 1 | 반복 761 | 시간 127[s] | loss 6.19 | 퍼플렉서티 546.60\n",
      "| 에폭 1 | 반복 781 | 시간 130[s] | loss 5.96 | 퍼플렉서티 528.22\n",
      "| 에폭 1 | 반복 801 | 시간 134[s] | loss 6.33 | 퍼플렉서티 555.05\n",
      "| 에폭 1 | 반복 821 | 시간 137[s] | loss 6.41 | 퍼플렉서티 534.74\n",
      "| 에폭 1 | 반복 841 | 시간 141[s] | loss 6.35 | 퍼플렉서티 557.54\n",
      "| 에폭 1 | 반복 861 | 시간 145[s] | loss 6.18 | 퍼플렉서티 558.55\n",
      "| 에폭 1 | 반복 881 | 시간 148[s] | loss 5.91 | 퍼플렉서티 496.39\n",
      "| 에폭 1 | 반복 901 | 시간 152[s] | loss 6.36 | 퍼플렉서티 569.20\n",
      "| 에폭 1 | 반복 921 | 시간 155[s] | loss 6.32 | 퍼플렉서티 544.53\n",
      "| 에폭 1 | 반복 941 | 시간 159[s] | loss 6.29 | 퍼플렉서티 531.94\n",
      "| 에폭 1 | 반복 961 | 시간 163[s] | loss 6.31 | 퍼플렉서티 578.33\n",
      "| 에폭 1 | 반복 981 | 시간 166[s] | loss 6.32 | 퍼플렉서티 538.56\n",
      "| 에폭 1 | 반복 1001 | 시간 170[s] | loss 6.17 | 퍼플렉서티 482.24\n",
      "| 에폭 1 | 반복 1021 | 시간 174[s] | loss 6.15 | 퍼플렉서티 539.81\n",
      "| 에폭 1 | 반복 1041 | 시간 177[s] | loss 6.08 | 퍼플렉서티 513.46\n",
      "| 에폭 1 | 반복 1061 | 시간 181[s] | loss 6.16 | 퍼플렉서티 512.08\n",
      "| 에폭 1 | 반복 1081 | 시간 184[s] | loss 6.15 | 퍼플렉서티 457.45\n",
      "| 에폭 1 | 반복 1101 | 시간 188[s] | loss 6.29 | 퍼플렉서티 492.19\n",
      "| 에폭 1 | 반복 1121 | 시간 191[s] | loss 6.18 | 퍼플렉서티 563.49\n",
      "| 에폭 1 | 반복 1141 | 시간 195[s] | loss 6.50 | 퍼플렉서티 528.90\n",
      "| 에폭 1 | 반복 1161 | 시간 199[s] | loss 6.14 | 퍼플렉서티 508.29\n",
      "| 에폭 1 | 반복 1181 | 시간 202[s] | loss 6.11 | 퍼플렉서티 501.08\n",
      "| 에폭 1 | 반복 1201 | 시간 206[s] | loss 6.23 | 퍼플렉서티 437.75\n",
      "| 에폭 1 | 반복 1221 | 시간 210[s] | loss 6.42 | 퍼플렉서티 437.69\n",
      "| 에폭 1 | 반복 1241 | 시간 214[s] | loss 6.25 | 퍼플렉서티 491.64\n",
      "| 에폭 1 | 반복 1261 | 시간 217[s] | loss 6.11 | 퍼플렉서티 449.22\n",
      "| 에폭 1 | 반복 1281 | 시간 221[s] | loss 6.40 | 퍼플렉서티 470.77\n",
      "| 에폭 1 | 반복 1301 | 시간 224[s] | loss 6.30 | 퍼플렉서티 583.25\n",
      "| 에폭 1 | 반복 1321 | 시간 228[s] | loss 6.33 | 퍼플렉서티 531.11\n"
     ]
    }
   ],
   "source": [
    "ppl_list = []\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "start_time = time.time()\n",
    "model.time_idx = 0\n",
    "for epoch in range(max_epoch):\n",
    "    for iters in range(max_iters):\n",
    "        batch_x, batch_t = model.get_batch(xs, ts, batch_size, time_size)\n",
    "\n",
    "        y, loss = model.forward(batch_x, batch_t)\n",
    "        model.backward(batch_t)\n",
    "        model.clip_grads(max_grad)\n",
    "        model.update(lr)\n",
    "        \n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "        if ((iters % eval_interval) == 0):\n",
    "            ppl = np.exp(total_loss / loss_count)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('| 에폭 %d | 반복 %d | 시간 %d[s] | loss %.2f | 퍼플렉서티 %.2f'\n",
    "                % (epoch + 1, iters + 1, elapsed_time, loss, ppl))\n",
    "            ppl_list.append(float(ppl))\n",
    "            total_loss, loss_count = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10200c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEGCAYAAABPdROvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAjGUlEQVR4nO3de3Sc9X3n8fd3ZjSj+8W25ItksB1sjO0ABgdMCAnhEmiaBk6TbJxuFjfL1j2UdpNstz3Q3bM56ZazyWlOmtA27FKSAE1CSmkTHFJCqEsgzQUjYwzYxtjG4ItsS7Zs3SyNLvPdP55H8sgam5mRRiOhz+ucOTPzm+eZ+Y4Q+vj3+z2/5zF3R0REJF+RYhcgIiLTm4JERETGRUEiIiLjoiAREZFxUZCIiMi4xIpdwGSbM2eOL1q0qNhliIhMK1u2bDnm7vWZXptxQbJo0SKam5uLXYaIyLRiZm+d7TUNbYmIyLgoSEREZFwUJCIiMi4KEhERGRcFiYiIjEvBgsTMvmVmrWb2alrbLDN72sx2h/d1aa/dbWZ7zGyXmd2U1n65mb0SvnavmVnYnjCzfwjbnzezRYX6LiIicnaF7JE8CNx8RttdwCZ3XwpsCp9jZiuAdcDKcJ9vmFk03Oc+YAOwNLwNv+ftwAl3vwD4K+DLBfsmIiJyVgULEnd/Dmg/o/kW4KHw8UPArWnt33f3pLvvA/YAV5jZfKDa3X/lwfnuHz5jn+H3egy4fri3UggvvNnOV57axeBQqlAfISIyLU32HMlcdz8MEN43hO2NwIG07Q6GbY3h4zPbR+3j7oNABzA704ea2QYzazaz5ra2trwK37r/BH/zzB76BhUkIiLppspke6aehJ+j/Vz7jG10v9/d17j7mvr6jCv831Y8GvyokgNDee0vIvJONdlBcjQcriK8bw3bDwIL07ZrAlrC9qYM7aP2MbMYUMPYobQJkygJpmz6NbQlIjLKZAfJRmB9+Hg98Hha+7rwSKzFBJPqm8Phry4zWxvOf9x2xj7D7/Vx4N+8gNcNHu6R9GtoS0RklIKdtNHMHgGuBeaY2UHgC8CXgEfN7HZgP/AJAHffbmaPAjuAQeBOdx8eQ7qD4AiwMuDJ8AbwTeDvzWwPQU9kXaG+C0A8piAREcmkYEHi7p86y0vXn2X7e4B7MrQ3A6sytPcRBtFkGA6SpIJERGSUqTLZPuUlFCQiIhkpSLKkoS0RkcwUJFka7pHoqC0RkdEUJFmKR8PDf9UjEREZRUGSpUTJ8ByJFiSKiKRTkGRJ60hERDJTkGRJk+0iIpkpSLIU12S7iEhGCpIsjawjGVCQiIikU5BkST0SEZHMFCRZGjmNvOZIRERGUZBkycyIRyOabBcROYOCJAeJWETrSEREzqAgyUE8ph6JiMiZFCQ5UJCIiIylIMlBPBbRUVsiImdQkOQgEYtoHYmIyBkUJDlQj0REZCwFSQ50+K+IyFgKkhxosl1EZCwFSQ4SsajWkYiInEFBkoN4LKJTpIiInEFBkgNNtouIjKUgyUFCk+0iImMoSHKQKNHQlojImRQkOdDhvyIiYylIcqDDf0VExlKQ5ECT7SIiYylIchCPRhlKOYMKExGREQqSHCRKdN12EZEzKUhyMHzdds2TiIicpiDJQTymIBEROVNRgsTMPm9m283sVTN7xMxKzWyWmT1tZrvD+7q07e82sz1mtsvMbkprv9zMXglfu9fMrJB1DweJ1pKIiJw26UFiZo3AfwXWuPsqIAqsA+4CNrn7UmBT+BwzWxG+vhK4GfiGmUXDt7sP2AAsDW83F7L2hIJERGSMYg1txYAyM4sB5UALcAvwUPj6Q8Ct4eNbgO+7e9Ld9wF7gCvMbD5Q7e6/cncHHk7bpyASGtoSERlj0oPE3Q8BXwH2A4eBDnf/KTDX3Q+H2xwGGsJdGoEDaW9xMGxrDB+f2V4wI3MkOmpLRGREMYa26gh6GYuBBUCFmX36XLtkaPNztGf6zA1m1mxmzW1tbbmWPCIeDUbU1CMRETmtGENbNwD73L3N3QeAfwbeCxwNh6sI71vD7Q8CC9P2byIYCjsYPj6zfQx3v9/d17j7mvr6+rwLH15HootbiYicVowg2Q+sNbPy8Cir64GdwEZgfbjNeuDx8PFGYJ2ZJcxsMcGk+uZw+KvLzNaG73Nb2j4FoXUkIiJjxSb7A939eTN7DHgRGAS2AvcDlcCjZnY7Qdh8Itx+u5k9CuwIt7/T3Ye7BHcADwJlwJPhrWC0jkREZKxJDxIAd/8C8IUzmpMEvZNM298D3JOhvRlYNeEFnoUm20VExtLK9hyMrCMZUJCIiAxTkORgZGW7eiQiIiMUJDlI6PBfEZExFCQ50GS7iMhYCpIcnD5po9aRiIgMU5DkIBoxYhFTj0REJI2CJEfxWERBIiKSRkGSo3gsonUkIiJpFCQ5SsQiWkciIpJGQZIj9UhEREZTkOQoHtUciYhIOgVJjuKxqC61KyKSRkGSo0QsonUkIiJpFCQ50uG/IiKjKUhylNBku4jIKAqSHGmyXURkNAVJjhIlChIRkXQKkhzFoxEdtSUikkZBkiNNtouIjKYgyZFWtouIjKYgyVE8GlWPREQkjYIkR4kSLUgUEUmnIMlRPBphYMhJpbzYpYiITAkKkhyNXLdd8yQiIoCCJGcJBYmIyCgKkhwNB4kubiUiElCQ5EhDWyIioylIcjQSJDoEWEQEUJDkLB6NAgoSEZFhCpIcjcyRaC2JiAigIMmZhrZEREZTkORIQSIiMpqCJEfDQZLUUVsiIkCRgsTMas3sMTN7zcx2mtlVZjbLzJ42s93hfV3a9neb2R4z22VmN6W1X25mr4Sv3WtmVujatY5ERGS0YvVIvg78xN2XA5cAO4G7gE3uvhTYFD7HzFYA64CVwM3AN8wsGr7PfcAGYGl4u7nQhWtlu4jIaJMeJGZWDbwf+CaAu/e7+0ngFuChcLOHgFvDx7cA33f3pLvvA/YAV5jZfKDa3X/l7g48nLZPwejwXxGR0YrRI1kCtAHfNrOtZvaAmVUAc939MEB43xBu3wgcSNv/YNjWGD4+s30MM9tgZs1m1tzW1jau4jXZLiIyWjGCJAZcBtzn7quBHsJhrLPINO/h52gf2+h+v7uvcfc19fX1udY7itaRiIiMVowgOQgcdPfnw+ePEQTL0XC4ivC+NW37hWn7NwEtYXtThvaCUo9ERGS0SQ8Sdz8CHDCzC8Om64EdwEZgfdi2Hng8fLwRWGdmCTNbTDCpvjkc/uoys7Xh0Vq3pe1TMAoSEZHRYkX63D8CvmtmceAN4DMEofaomd0O7Ac+AeDu283sUYKwGQTudPfhcaU7gAeBMuDJ8FZQsYhhpqO2RESGFSVI3P0lYE2Gl64/y/b3APdkaG8GVk1ocW/DzEjEIiTVIxERAbSyPS/xaERDWyIiIQVJHuKxqHokIiIhBUkeEjH1SEREhilI8pCIRTTZLiISUpDkIR6LkBzQgkQREVCQ5CWuHomIyIhzHv5rZv/tXK+7+1cntpzpQUdtiYic9nbrSKompYppJlES0fVIRERC5wwSd//iZBUyncSjETp7B4tdhojIlJDVHImZLTGzH5lZm5m1mtnjZrak0MVNVXEd/isiMiLbyfbvAY8C84EFwD8CjxSqqKkuHotqsl1EJJRtkJi7/727D4a373CWa3/MBJpsFxE5LduTNj5jZncT9EIc+CTwYzObBeDu7QWqb0pKlER0YSsRkVC2QfLJ8P73wvvhqxP+Z4JgmVHzJfGozv4rIjIs2yBZAfwB8D6C4Pg5waVy+wpV2FSmc22JiJyW7RzJQ8BFwL3AX4ePHy5UUVPd8Mp29xk7TSQiMiLbHsmF7n5J2vNnzGxbIQqaDhKxCO4wMOTEY/b2O4iIvINl2yPZamZrh5+Y2ZXALwpT0tQ3ct12HQIsIpJ1j+RK4DYz2x8+Pw/YaWavAO7uFxekuikqHg2DZDAFiSIXIyJSZNkGyc0FrWKaiceiAJpwFxEhyyBx97cKXch0kgiHtrSWRERE1yPJy8gciXokIiIKknzER3okChIREQVJHnTUlojIaQqSPIzMkejiViIiCpJ8JNQjEREZoSDJQzyqw39FRIYpSPKgo7ZERE5TkOTh9NCW1pGIiChI8hDXZLuIyAgFSR50+K+IyGkKkjxojkRE5LSiBYmZRc1sq5k9ET6fZWZPm9nu8L4ubdu7zWyPme0ys5vS2i83s1fC1+41s0m5OEhCK9tFREYUs0fyWWBn2vO7gE3uvhTYFD7HzFYA64CVBGch/oaZRcN97gM2AEvD26ScpXj4NPIKEhGRIgWJmTUBvwk8kNZ8C8ElfQnvb01r/767J919H7AHuMLM5gPV7v4rD655+3DaPgVlZsSjum67iAgUr0fyNeBPgfS/xHPd/TBAeN8QtjcCB9K2Oxi2NYaPz2wfw8w2mFmzmTW3tbVNyBeIxxQkIiJQhCAxs48Are6+JdtdMrT5OdrHNrrf7+5r3H1NfX19lh97bolYROtIRETI/gqJE+lq4KNm9mGgFKg2s+8AR81svrsfDoetWsPtDwIL0/ZvAlrC9qYM7ZMiHotoHYmICEXokbj73e7e5O6LCCbR/83dPw1sBNaHm60HHg8fbwTWmVnCzBYTTKpvDoe/usxsbXi01m1p+xRcPBbROhIREYrTIzmbLwGPmtntwH7gEwDuvt3MHgV2AIPAne4+PKZ0B/AgUAY8Gd4mhSbbRUQCRQ0Sd/8Z8LPw8XHg+rNsdw9wT4b2ZmBV4So8u0SJgkREBLSyPW/xaETrSEREUJDkTYf/iogEFCR5iseiJDXZLiKiIMmXJttFRAIKkjwlSiIkB7UgUUREQZKnhHokIiKAgiRvmmwXEQkoSPKkle0iIgEFSZ4SOteWiAigIMmbeiQiIgEFSZ7i0ShDKWcolfHM9SIiM4aCJE/x8LrtmnAXkZlOQZKnRGz4uu1aSyIiM5uCJE/qkYiIBBQkeYqP9EgUJCIysylI8jQ8tKUjt0RkplOQ5CmhoS0REUBBkjcNbYmIBBQkeYpHo4B6JCIiCpI86agtEZGAgiRPpyfbtY5ERGY2BUmeRuZIdOJGEZnhFCR5iuvwXxERQEGSt3hUR22JiICCJG+JEk22i4iAgiRvifDwX/VIRGSmU5DkSYf/iogEFCR5UpCIiAQUJHmKRoxYxLSORERmPAXJOMRjEa0jEZEZT0EyDvFYROtIRGTGU5CMQzwa0RyJiMx4kx4kZrbQzJ4xs51mtt3MPhu2zzKzp81sd3hfl7bP3Wa2x8x2mdlNae2Xm9kr4Wv3mplN5ndJlChIRESK0SMZBP7Y3S8C1gJ3mtkK4C5gk7svBTaFzwlfWwesBG4GvmFm0fC97gM2AEvD282T+UVmVSTY335qMj9SRGTKmfQgcffD7v5i+LgL2Ak0ArcAD4WbPQTcGj6+Bfi+uyfdfR+wB7jCzOYD1e7+K3d34OG0fSbFB5bV8+L+E5zo6Z/MjxURmVKKOkdiZouA1cDzwFx3PwxB2AAN4WaNwIG03Q6GbY3h4zPbJ831yxtIOTyzq3UyP1ZEZEopWpCYWSXwT8Dn3L3zXJtmaPNztGf6rA1m1mxmzW1tbbkXexbvbqyhvirBpp0KEhGZuYoSJGZWQhAi33X3fw6bj4bDVYT3w3+dDwIL03ZvAlrC9qYM7WO4+/3uvsbd19TX10/Y94hEjOuXN/Dc622adBeRGasYR20Z8E1gp7t/Ne2ljcD68PF64PG09nVmljCzxQST6pvD4a8uM1sbvudtaftMmuuWN9CVHOSFN9sn+6NFRKaEYvRIrgb+E3Cdmb0U3j4MfAm40cx2AzeGz3H37cCjwA7gJ8Cd7j58XpI7gAcIJuD3Ak9O6jcB3rd0DvFYhH/deXSyP1pEZEqw4ICnmWPNmjXe3Nw8oe/5mW9vZm9bD8/+ybVM8lIWEZFJYWZb3H1Npte0sn0CXH/RXPa3n2JvW3exSxERmXQKkglw3fLgSOV/1dFbIjIDKUgmwILaMlbMr2aT5klEZAZSkEyQGy5qYMtbWuUuIjOPgmSCXH/RXFIOP3tdw1siMrMoSCbI8Cp3zZOIyEyjIJkgkYhx3YUNPLdLq9xFZGZRkEygG1bMpSs5yCOb9xe7FBGRSaMgmUDXLW/guuUNfPFH23UEl4jMGAqSCRSNGH/9qdWsXFDDH35vK9sOnCx2SSIiBacgmWAViRjf/N01zK6Mc/tDL7D/uK6gKCLvbAqSAmioKuXBz1zBwJDzu9/erLUlIvKOpiApkAsaKnlg/RoOnuzldx54ni1v6TTzIvLOpCApoPcsmsX/+/TlHO9O8rH7fsUffHcLbx3vKXZZIiITSkFSYB9c3sDP/uRaPnfDUp55rY0bvvos//uJHZw8peEuEXlnUJBMgvJ4jM/dsIxn/+Rafnt1E9/6xT6u/crPeOiXbzIwpMWLIjK9KUgmUUN1KV/++MX8+I+u4aJ51Xxh43Z+4+s/59nX24pdmohI3nSFxCJxd57ecZR7/mUnbx0/xerzaqlMxOjtH6J3ILjFIkZtWZya8hJqy0qoLS+hriJOXXlwm1URp7QkQm//EKcGhugL9z1/djnvbqwlHtO/E0RkYpzrComxyS5GAmbGh1bO4wMX1vPwL99i47YWepKDlMWj1JaXkCiJMjTknOzt50D7KV7tHeDEqX76BrIbCisribJmUR1rl8zm0oW1mMHAkDMwmGIwlaK2PM4lTbWUxaMF/qYi8k6nHsk009s/xIlT/cGtZ4DegSHK41HK4lHK41ESsSi7jnTx6zeO8+s3jvPaka6zvlcsYqxYUM3l59dxcVMNHacG2HeshzeO9bDvWA9tXUliESMWjVASjRCPGkvnVnHjirncuGIuc6tLc67f3TnQ3ktbd5JLF9YSjega9yLTwbl6JAqSd7j2nn5eO9xJJGJhGESIRY3DHb00v3mCLW+dYNvBkyM9ncpEjMVzKlhSX8G86lKGUs7AUIqBlNM3MMSWt07wVrha/5KFtXxoxVxuXd1IY21Zxs9PpZytB06wed8JXtx/gq37T3CsOzhibX5NKb99WSMfu6yJJfWVk/MDKbD9x0/xrV/so6ashFWNNaxqrGZedSlmCkyZ3hQkaWZakGRjYCjF3rZuZpXHqa9KnPOPnruzu7Wbn24/wtM7jrLtYAdmcM3Sev7DmiZuXDGXeDTCywc7+NG2Fp54+TBHOvsAWDyngtXn1XLZeXVUlcb44dZDPPt6GymHy8+vY9ncKrqTg/QkB+nuG6Q7OUj/UCoIssEU/UNOSdRoqC5lXnWCudWlzK0uJRGL4A5O8LscMaMyEaOqtISq0hjVZSWUlkSIRYxoJELUjGjUKIkaiWiUkpgRj0boSQ6x62gXu450sutoF7uPdlNXHufdTTW8uzG41VXEM/5cevuHuO/ZvfzfZ/eCw0AqxfD/WrMrgve4YvEsrlw8m4ubaiiJav5KphcFSRoFycQ60H6Kf9xykMeaD9DS0UdteQnVpSXsbz9FSdT4wLIGfuuS+VyztJ5ZGf4It3b28YOth/jB1kMc7+mnKhGjIhGjMhGjIhEM1ZVEg95USSxCciBFa1cfRzv7ONLRR2ffYEG+V1UixtK5lbT39PNm2vnSFs4q4+KmWi5pquGSplpWNdbw73uO8ec/2sGhk7189JIF/NmHL6K6LMbOw51sb+nk1UMdbN1/kt2t3UAwf3X5+XU01ZURDXuKsYhRHo9y7fIGVi+sfdseTCrlnOwd4Fh3EndYNrdSvR4pKAVJGgVJYQylnH/fc4x/bD5Ad3KQD6+az00r51FTXlLQz+0bGBpZi2NmGDCYcrqTg3T1DdDVF9z3DaQYSjlDKWcw5Qylgh5O/2CK/sGg1xOPRbhwbhXL5lWxoOb0cFRH7wDbD3Xw8qEOXj54km0HOjh0sjf8TEb+kH/xo6u46l2zz1rr8e4km/e183x4a+9JMjgU1DM4lKJ3YIiUB2H10UsW8NFLGjl/djmvHeni1UMdbG/pYEdLJ4c7+mjv6Wcwdfr/3SX1FXzssqaMw4zuTmffIBXxKLEMPaHBoRRHu5IcPtlLIhZlTlWc2RWJkaP+3J3O3kGOhgHekxwMe4DBd49G4OKmWhacZXhz99EufrrjKKmUM7+2jPk1pcyrKWVedSnl8eiYADzS0ce2gyfZduAku450UV+VYPm8KpbPr2b5vCpqyzP3CifDm8d6+PUbx6mvSvCu+kqa6soy/kyLaffRLv7mmT0c6ejjsvPreM+iOi4/b9a4/19UkKRRkMhEONad5OWDJ3npQAcNVQk++Z6F4x6u6uwb4Kfbj7JxWwu/2HOMoZSPBBVATVkJKxdU01RXxpzKBPVVCeZUJuhODvKDrYfYvK8dM1i7eDYLZ5XRcrKPQyd7aTnZSzK8amdNWQmzK+PMrohjZhw60cuRzj6GUmP/DtSWl1ARj3G8J5nV0YJLGyp5/7J63r+snlnlcZ7afoQnXz3M3raznxYoGhkehgx6oe09/bR2JUdeWzKngrbuJCdPDYzs01hbxpWLZ7H2XbO5aslsFs4qZyjlbG/p4Jd7j/PLvcfZ0dJBXXmcxroyFtSW0VhbRl15HDOIGBhGJGIsm1vJivnVZw0Dd2fn4S6e2n6Ep7YfGXPwSknUWDS7gvNnl1NflaC+MsGc8L9LZSJGIhahtCRKoiRCTVkJ82syh+1E2NPaxdc37eGJl1soL4nyroZKdrR0jvyD48K5VXz+xqXcvGp+Xu+vIEmjIJHp4Fh3kidfOUxbV5IVC4JJ+8basnMOXx1oP8UPth7ihy8doqtvkMbwD2hjXRkNVUHgtPf0c7ynn+PdSVLOyDYLasuYX1tK/2CKY91JjnX1c6w7SXdykDmV8ZH5qLnVpVQkokTMsPAPct/AEJv3tfPc7jae39c+cqnpiMGVi2fz4XfP46aV86guK+FoZx8tJ/s40tnL0c7kSK+xu2+Qzr5BqkpjXNxUw8VNtaxcUE1pSRR3p7Uryc7Dnew60sW2gyf59RvttIdn1W6sLaOrb2BkmHNpQyWXLqyls2+AlpN9tJzs5fg5zsBdmYhx+fl1XLlkFhfUV3LgRC9727rZ29rN3rYejnUnMQvOnXfTynl8YFk9Hb0DwTZt3bzR1sOB9lMc605yvKefc/1JPX92Odcuq+cDF9Zz1ZI5GQ+/P96d5IU3T7B5Xzub3zxOW1eSC+dVs3JBNSvmV7NiQTURM1o7+2jrTtLamWTrgZM88XILZSVR1r93Eb93zRJmVcTp7R/ipQMnaX6znRfeOsHt71vMB5bVZ/MrOIaCJI2CRKRwevuH+PW+47R393PthfXMrkwU5HPcndePdvPrN46zeV87lYkY771gNle9azYNVWMPS+/tH6KjdwDHcYeUB8Oar7Z08vwbx3l+Xzt7wjksCHpuFzRU8q76ClafV8cNF82lvurtv8vgUIr2U/20dSXp7R8iOZgiOThEciDFkc4+fr77GL/ce4y+gWAodWHd6H8c9A+m2N8ezMklYhFWn1fLgpoyXjvSxe7WLgaGMv+9rohH+fRV57PhmiUF+5krSNIoSEQkk2PdSfa3n+L8WeXMCof+CmG4B/fs620c6egb9ZoZrFhQzZWLZ7GqsYZE7HSPJTk4xJ7WbnYe7sKAhuoEDVWl1FclqC0rIVLgNVkKkjQKEhGR3J0rSKbW4QYiIjLtKEhERGRcFCQiIjIuChIRERmXaR8kZnazme0ysz1mdlex6xERmWmmdZCYWRT4W+A3gBXAp8xsRXGrEhGZWaZ1kABXAHvc/Q137we+D9xS5JpERGaU6R4kjcCBtOcHw7ZRzGyDmTWbWXNbm66PLiIykab7pXYzLeUcs8LS3e8H7gcwszYzeyvPz5sDHMtz32KarnXD9K1ddU8u1V1455/thekeJAeBhWnPm4CWc+3g7vmdsQwws+azreycyqZr3TB9a1fdk0t1F9d0H9p6AVhqZovNLA6sAzYWuSYRkRllWvdI3H3QzP4QeAqIAt9y9+1FLktEZEaZ1kEC4O7/AvzLJH3c/ZP0ORNtutYN07d21T25VHcRzbiz/4qIyMSa7nMkIiJSZAoSEREZFwVJlqbLOb3M7Ftm1mpmr6a1zTKzp81sd3hfV8waMzGzhWb2jJntNLPtZvbZsH1K125mpWa22cy2hXV/MWyf0nUPM7OomW01syfC51O+bjN708xeMbOXzKw5bJsOddea2WNm9lr4e37VdKg7GwqSLEyzc3o9CNx8RttdwCZ3XwpsCp9PNYPAH7v7RcBa4M7wZzzVa08C17n7JcClwM1mtpapX/ewzwI7055Pl7o/6O6Xpq3BmA51fx34ibsvBy4h+LlPh7rfnrvr9jY34CrgqbTndwN3F7uuc9S7CHg17fkuYH74eD6wq9g1ZvEdHgdunE61A+XAi8CV06FuggW8m4DrgCemy+8K8CYw54y2KV03UA3sIzzAabrUne1NPZLsZHVOrylsrrsfBgjvG4pczzmZ2SJgNfA806D2cHjoJaAVeNrdp0XdwNeAPwVSaW3ToW4HfmpmW8xsQ9g21eteArQB3w6HEh8wswqmft1ZUZBkJ6tzesn4mVkl8E/A59y9s9j1ZMPdh9z9UoJ/4V9hZquKXNLbMrOPAK3uvqXYteThane/jGCo+U4ze3+xC8pCDLgMuM/dVwM9TNdhrAwUJNnJ+ZxeU8xRM5sPEN63FrmejMyshCBEvuvu/xw2T4vaAdz9JPAzgjmqqV731cBHzexNgssvXGdm32Hq1427t4T3rcAPCC4nMdXrPggcDHurAI8RBMtUrzsrCpLsTPdzem0E1oeP1xPMP0wpZmbAN4Gd7v7VtJemdO1mVm9mteHjMuAG4DWmeN3ufre7N7n7IoLf539z908zxes2swozqxp+DHwIeJUpXre7HwEOmNmFYdP1wA6meN3Z0sr2LJnZhwnGlIfP6XVPcSvKzMweAa4lOD31UeALwA+BR4HzgP3AJ9y9vUglZmRm7wN+DrzC6TH7PyOYJ5mytZvZxcBDBL8XEeBRd/9zM5vNFK47nZldC/x3d//IVK/bzJYQ9EIgGC76nrvfM9XrBjCzS4EHgDjwBvAZwt8ZpnDd2VCQiIjIuGhoS0RExkVBIiIi46IgERGRcVGQiIjIuChIRERkXBQkIlkys1+G94vM7HcK+DlfO9dqbTMrN7Mfh2eR3W5mX0p7LWFm/xCepfr58HQzw+tdflKommVmU5CIZMnd3xs+XATkFCThGaSz2W4WsNbdn3ubTb/iwVlkVwNXm9lvhO23Ayfc/QLgr4Avh7W3AYfN7Opc6hbJhoJEJEtm1h0+/BJwTXg9jM+HJ238SzN7wcxeNrPfD7e/1oJrrHwPeCVclf3j8Nolr5rZJzN8zMeBn4T711hwDZwLw+ePmNnvufspd38GwN37Cc443BTufwvBAkkITsNxfXjWAAgWpv7HifyZiECwMlREcnMX4UpwgPAMtB3u/h4zSwC/MLOfhtteAaxy931m9jGgxd1/M9yvJsN7X00QALh7h5n9IfCgmX0dqHP3v0vfODw9y28RXOsC0s5U7e6DZtYBzAaOAc3AX0zIT0AkjXokIuP3IeC28FTyzxP84V4avrbZ3feFj18BbjCzL5vZNe7ekeG95hOcbhwAd3863O9vgf+SvqGZxYBHgHvd/Y3h5gzvOXz6ilZgQY7fTeRtKUhExs+AP/Lgin2Xuvtidx/ukfQMb+TurwOXEwTD/zGz/5XhvXqB0pE3NosAF4Xts87Y9n5gt7t/La1t5EzVYdDUAMPnbioN30dkQilIRHLXBVSlPX8KuCM8DT5mtiw8M+0oZrYAOOXu3wG+QnAa8TPtBC5Ie/75sO1TwLfSPuMvCELic2fsn3422Y8TnNV3uEeyjOBMuSITSnMkIrl7GRg0s23AgwTzE4uAF8OJ7Tbg1gz7vRv4SzNLAQPAHRm2+THw+8ADZraMYDjrCnfvMrPngP9pZn8H/A+C09W/GM6l/427P0BwKv6/N7M9BD2RdWnv/cHw/UUmlM7+KzLFmNm/Ax8JL5Q1ke/7HHCLu5+YyPcVUZCITDFmdiXQ6+4vT+B71hNcovaHE/WeIsMUJCIiMi6abBcRkXFRkIiIyLgoSEREZFwUJCIiMi4KEhERGZf/D0gqN+kAEWgsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = np.arange(len(ppl_list))\n",
    "plt.plot(p, ppl_list, label='train')\n",
    "plt.xlabel('iters (x' + str(eval_interval) + ')')\n",
    "plt.ylabel('ppl')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
