{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901b4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from dataset import ptb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5923d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstmlm:\n",
    "    \n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        \n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed = (rn(V, D) / 100).astype('f')\n",
    "        lstmWx = rn(D, H*4) / np.sqrt(D)\n",
    "        lstmWh = rn(H, H*4) / np.sqrt(H)\n",
    "        lstmb = np.zeros((H*4), dtype='f')\n",
    "        affineW = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affineb = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.params = [embed, lstmWx, lstmWh, lstmb, affineW, affineb]\n",
    "        self.grads = []\n",
    "        self.lstm = []\n",
    "        \n",
    "        self.wordvec_size = wordvec_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.time_idx = 0\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        embed, lstmWx, lstmWh, lstmb, affineW, affineb = self.params\n",
    "        \n",
    "        batch_size, time_size = x.shape\n",
    "        wordvec_size = self.wordvec_size\n",
    "        hidden_size = self.hidden_size\n",
    "        \n",
    "        h_prev = np.zeros((batch_size, hidden_size), dtype='f')\n",
    "        c_prev = np.zeros((batch_size, hidden_size), dtype='f')\n",
    "        hs = np.empty((batch_size, time_size, hidden_size), dtype='f')\n",
    "        for t in range(time_size):\n",
    "            # embed\n",
    "            emb_out = embed[x[:, t]]\n",
    "            \n",
    "            # lstm\n",
    "            A = np.matmul(emb_out, lstmWx) + np.matmul(h_prev, lstmWh) + lstmb\n",
    "            f = A[:, :hidden_size]\n",
    "            g = A[:, hidden_size: 2*hidden_size]\n",
    "            i = A[:, 2*hidden_size: 3*hidden_size]\n",
    "            o = A[:, 3*hidden_size:]\n",
    "\n",
    "            f = self.sigmoid(f)\n",
    "            g = np.tanh(g)\n",
    "            i = self.sigmoid(i)\n",
    "            o = self.sigmoid(o)\n",
    "\n",
    "            c_next = f * c_prev + g * i\n",
    "            h_next = o * np.tanh(c_next)\n",
    "            \n",
    "            self.lstm.append((emb_out, h_prev, c_prev, f, g, i, o, c_next, h_next))\n",
    "            c_prev = c_next\n",
    "            h_prev = h_next\n",
    "            \n",
    "            hs[:, t, :] = h_prev\n",
    "        \n",
    "        # affine\n",
    "        affine_out = np.matmul(hs, affineW) + affineb\n",
    "        \n",
    "        # softmax\n",
    "        y = self.softmax(affine_out)\n",
    "        \n",
    "        loss = self.getLoss(y, batch_t)\n",
    "        self.xs = x, hs, affine_out, y\n",
    "\n",
    "        return y, loss\n",
    "    \n",
    "    def backward(self, t):\n",
    "        embed, lstmWx, lstmWh, lstmb, affineW, affineb = self.params\n",
    "        x, hs, affine_out, y = self.xs\n",
    "        \n",
    "        wordvec_size = self.wordvec_size\n",
    "        vocab_size = self.vocab_size\n",
    "        batch_size, time_size = x.shape\n",
    "        \n",
    "        # softmax\n",
    "        y = y.reshape(batch_size * time_size, -1)\n",
    "        t = t.reshape(batch_size * time_size)\n",
    "        y[np.arange(batch_size * time_size), t] -= 1\n",
    "        soft_dout = y\n",
    "        \n",
    "        # affine\n",
    "        affine_dout = np.matmul(soft_dout, affineW.T).reshape(batch_size, time_size, -1) # (b, t, h)\n",
    "        affinedW = np.matmul(hs.reshape(batch_size * time_size, -1).T, soft_dout) # (h, v)\n",
    "        affinedb = np.sum(soft_dout, axis=0)\n",
    "        \n",
    "        # lstm\n",
    "        lstmdWx = np.zeros_like(lstmWx)\n",
    "        lstmdWh = np.zeros_like(lstmWh)\n",
    "        lstmdb = np.zeros_like(lstmb)\n",
    "        \n",
    "        lstm_douts = np.empty((batch_size, time_size, wordvec_size), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "        for t in reversed(range(time_size)):            \n",
    "            emb_out, h_prev, c_prev, f, g, i, o, c_next, _ = self.lstm[t]\n",
    "            dh_next = affine_dout[:, t, :] + dh\n",
    "            dc_next = dc\n",
    "\n",
    "            tanh_c_next = np.tanh(c_next)\n",
    "            \n",
    "            ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "            \n",
    "            dc_prev = ds * f\n",
    "            \n",
    "            di = ds * g\n",
    "            df = ds * c_prev\n",
    "            do = dh_next * tanh_c_next\n",
    "            dg = ds * i\n",
    "            \n",
    "            di *= i * (1 - i)\n",
    "            df *= f * (1 - f)\n",
    "            do *= o * (1 - o)\n",
    "            dg *= (1 - g ** 2)\n",
    "            \n",
    "            dA = np.hstack((df, dg, di, do))\n",
    "            \n",
    "            dWh = np.matmul(h_prev.T, dA)\n",
    "            dWx = np.matmul(emb_out.T, dA)\n",
    "            db = np.sum(dA, axis=0)\n",
    "            \n",
    "            lstm_douts[:, t, :] = np.matmul(dA, lstmWx.T)\n",
    "            dh_prev = np.matmul(dA, lstmWh.T)\n",
    "        \n",
    "            lstmdWx += dWx\n",
    "            lstmdWh += dWh\n",
    "            lstmdb += db\n",
    "            dh = dh_prev\n",
    "            dc = dc_prev\n",
    "        \n",
    "        # embed\n",
    "        embed_dout = np.zeros_like(embed)\n",
    "        for t in range(time_size):\n",
    "            np.add.at(embed_dout, x[:, t], lstm_douts[:, t, :])\n",
    "        \n",
    "        self.grads = embed_dout, lstmdWx, lstmdWh, lstmdb, affinedW, affinedb\n",
    "        \n",
    "        \n",
    "    def softmax(self, y):\n",
    "        y = y - np.max(y)\n",
    "        y = np.exp(y)\n",
    "        y = y / y.sum(axis=2, keepdims=True)\n",
    "        return y\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def update(self, lr):\n",
    "        for i in range(len(self.params)):\n",
    "            self.params[i] -= self.grads[i] * lr      \n",
    "            \n",
    "    def getLoss(self, y, t):\n",
    "        N, T, V = y.shape\n",
    "\n",
    "        y = y.reshape(N * T, V)\n",
    "        t = t.reshape(N * T)\n",
    "\n",
    "        ls = np.log(y[np.arange(N * T), t])\n",
    "        return -np.sum(ls) / (N * T)\n",
    "    \n",
    "    def get_batch(self, x, t, batch_size, time_size):\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "\n",
    "        data_size = len(x)\n",
    "        jump = data_size // batch_size\n",
    "        offsets = [i * jump for i in range(batch_size)]  # 배치에서 각 샘플을 읽기 시작하는 위치\n",
    "\n",
    "        for time in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, time] = x[(offset + self.time_idx) % data_size]\n",
    "                batch_t[i, time] = t[(offset + self.time_idx) % data_size]\n",
    "            self.time_idx += 1\n",
    "        return batch_x, batch_t\n",
    "    \n",
    "    def clip_grads(self, max_norm):\n",
    "        grads = self.grads\n",
    "        total_norm = 0\n",
    "        \n",
    "        for grad in grads:\n",
    "            total_norm += np.sum(grad**2)\n",
    "            \n",
    "        total_norm = np.sqrt(total_norm)\n",
    "        \n",
    "        rate = max_norm / (total_norm + 1e-6)\n",
    "        \n",
    "        if rate  < 1:\n",
    "            for grad in grads:\n",
    "                grad *= rate\n",
    "        self.grads = grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b52e1b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]  # 입력\n",
    "ts = corpus[1:]  # 출력（정답 레이블）\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 20\n",
    "wordvec_size = 100\n",
    "hidden_size = 100  # LSTM의 은닉 상태 벡터의 원소 수\n",
    "time_size = 35  # LSTM을 펼치는 크기\n",
    "lr = 20.0\n",
    "max_epoch = 1\n",
    "max_iters = len(xs) // (batch_size * time_size)\n",
    "eval_interval = 20\n",
    "max_grad = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b15a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lstmlm(vocab_size, wordvec_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2cd95d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 | 반복 1 | 시간 0[s] | loss 9.21 | 퍼플렉서티 10000.04\n",
      "| 에폭 1 | 반복 21 | 시간 2[s] | loss 7.04 | 퍼플렉서티 2150.77\n",
      "| 에폭 1 | 반복 41 | 시간 5[s] | loss 7.93 | 퍼플렉서티 1533.54\n",
      "| 에폭 1 | 반복 61 | 시간 7[s] | loss 7.10 | 퍼플렉서티 1298.84\n",
      "| 에폭 1 | 반복 81 | 시간 10[s] | loss 6.84 | 퍼플렉서티 1023.28\n",
      "| 에폭 1 | 반복 101 | 시간 13[s] | loss 6.75 | 퍼플렉서티 945.53\n",
      "| 에폭 1 | 반복 121 | 시간 16[s] | loss 6.88 | 퍼플렉서티 940.24\n",
      "| 에폭 1 | 반복 141 | 시간 19[s] | loss 6.71 | 퍼플렉서티 882.15\n",
      "| 에폭 1 | 반복 161 | 시간 21[s] | loss 6.42 | 퍼플렉서티 850.53\n",
      "| 에폭 1 | 반복 181 | 시간 24[s] | loss 6.68 | 퍼플렉서티 884.62\n",
      "| 에폭 1 | 반복 201 | 시간 27[s] | loss 6.48 | 퍼플렉서티 764.86\n",
      "| 에폭 1 | 반복 221 | 시간 30[s] | loss 6.64 | 퍼플렉서티 775.27\n",
      "| 에폭 1 | 반복 241 | 시간 33[s] | loss 6.41 | 퍼플렉서티 744.21\n",
      "| 에폭 1 | 반복 261 | 시간 36[s] | loss 6.53 | 퍼플렉서티 751.21\n",
      "| 에폭 1 | 반복 281 | 시간 40[s] | loss 6.12 | 퍼플렉서티 681.79\n",
      "| 에폭 1 | 반복 301 | 시간 44[s] | loss 6.24 | 퍼플렉서티 639.53\n",
      "| 에폭 1 | 반복 321 | 시간 47[s] | loss 6.51 | 퍼플렉서티 575.12\n",
      "| 에폭 1 | 반복 341 | 시간 51[s] | loss 6.49 | 퍼플렉서티 618.40\n",
      "| 에폭 1 | 반복 361 | 시간 54[s] | loss 6.32 | 퍼플렉서티 643.50\n",
      "| 에폭 1 | 반복 381 | 시간 60[s] | loss 6.06 | 퍼플렉서티 578.87\n",
      "| 에폭 1 | 반복 401 | 시간 65[s] | loss 6.52 | 퍼플렉서티 580.89\n",
      "| 에폭 1 | 반복 421 | 시간 70[s] | loss 6.57 | 퍼플렉서티 598.87\n",
      "| 에폭 1 | 반복 441 | 시간 75[s] | loss 6.14 | 퍼플렉서티 546.10\n",
      "| 에폭 1 | 반복 461 | 시간 80[s] | loss 6.37 | 퍼플렉서티 537.35\n",
      "| 에폭 1 | 반복 481 | 시간 85[s] | loss 6.23 | 퍼플렉서티 517.76\n",
      "| 에폭 1 | 반복 501 | 시간 89[s] | loss 6.16 | 퍼플렉서티 525.49\n",
      "| 에폭 1 | 반복 521 | 시간 95[s] | loss 6.06 | 퍼플렉서티 496.60\n",
      "| 에폭 1 | 반복 541 | 시간 100[s] | loss 6.40 | 퍼플렉서티 531.44\n",
      "| 에폭 1 | 반복 561 | 시간 105[s] | loss 6.19 | 퍼플렉서티 498.41\n",
      "| 에폭 1 | 반복 581 | 시간 110[s] | loss 6.29 | 퍼플렉서티 443.13\n",
      "| 에폭 1 | 반복 601 | 시간 115[s] | loss 6.23 | 퍼플렉서티 573.13\n",
      "| 에폭 1 | 반복 621 | 시간 120[s] | loss 6.09 | 퍼플렉서티 525.80\n",
      "| 에폭 1 | 반복 641 | 시간 125[s] | loss 6.03 | 퍼플렉서티 482.28\n",
      "| 에폭 1 | 반복 661 | 시간 130[s] | loss 6.09 | 퍼플렉서티 468.48\n",
      "| 에폭 1 | 반복 681 | 시간 135[s] | loss 6.22 | 퍼플렉서티 407.07\n",
      "| 에폭 1 | 반복 701 | 시간 140[s] | loss 6.19 | 퍼플렉서티 427.70\n",
      "| 에폭 1 | 반복 721 | 시간 145[s] | loss 5.93 | 퍼플렉서티 436.25\n",
      "| 에폭 1 | 반복 741 | 시간 150[s] | loss 5.87 | 퍼플렉서티 382.36\n",
      "| 에폭 1 | 반복 761 | 시간 155[s] | loss 5.87 | 퍼플렉서티 409.26\n",
      "| 에폭 1 | 반복 781 | 시간 160[s] | loss 5.74 | 퍼플렉서티 383.02\n",
      "| 에폭 1 | 반복 801 | 시간 166[s] | loss 6.00 | 퍼플렉서티 418.83\n",
      "| 에폭 1 | 반복 821 | 시간 171[s] | loss 6.11 | 퍼플렉서티 390.06\n",
      "| 에폭 1 | 반복 841 | 시간 176[s] | loss 6.03 | 퍼플렉서티 397.77\n",
      "| 에폭 1 | 반복 861 | 시간 181[s] | loss 5.80 | 퍼플렉서티 391.30\n",
      "| 에폭 1 | 반복 881 | 시간 186[s] | loss 5.57 | 퍼플렉서티 357.14\n",
      "| 에폭 1 | 반복 901 | 시간 191[s] | loss 5.99 | 퍼플렉서티 418.23\n",
      "| 에폭 1 | 반복 921 | 시간 196[s] | loss 6.13 | 퍼플렉서티 395.27\n",
      "| 에폭 1 | 반복 941 | 시간 202[s] | loss 6.02 | 퍼플렉서티 392.50\n",
      "| 에폭 1 | 반복 961 | 시간 208[s] | loss 5.97 | 퍼플렉서티 412.88\n",
      "| 에폭 1 | 반복 981 | 시간 213[s] | loss 5.98 | 퍼플렉서티 387.59\n",
      "| 에폭 1 | 반복 1001 | 시간 219[s] | loss 5.89 | 퍼플렉서티 339.17\n",
      "| 에폭 1 | 반복 1021 | 시간 224[s] | loss 5.97 | 퍼플렉서티 393.90\n",
      "| 에폭 1 | 반복 1041 | 시간 230[s] | loss 5.67 | 퍼플렉서티 362.28\n",
      "| 에폭 1 | 반복 1061 | 시간 237[s] | loss 5.70 | 퍼플렉서티 344.45\n",
      "| 에폭 1 | 반복 1081 | 시간 244[s] | loss 5.84 | 퍼플렉서티 317.09\n",
      "| 에폭 1 | 반복 1101 | 시간 249[s] | loss 6.03 | 퍼플렉서티 349.02\n",
      "| 에폭 1 | 반복 1121 | 시간 255[s] | loss 5.88 | 퍼플렉서티 397.43\n",
      "| 에폭 1 | 반복 1141 | 시간 260[s] | loss 6.12 | 퍼플렉서티 365.67\n",
      "| 에폭 1 | 반복 1161 | 시간 266[s] | loss 5.68 | 퍼플렉서티 343.97\n",
      "| 에폭 1 | 반복 1181 | 시간 271[s] | loss 5.71 | 퍼플렉서티 328.10\n",
      "| 에폭 1 | 반복 1201 | 시간 276[s] | loss 5.81 | 퍼플렉서티 283.24\n",
      "| 에폭 1 | 반복 1221 | 시간 280[s] | loss 5.98 | 퍼플렉서티 291.44\n",
      "| 에폭 1 | 반복 1241 | 시간 284[s] | loss 5.93 | 퍼플렉서티 333.67\n",
      "| 에폭 1 | 반복 1261 | 시간 288[s] | loss 5.70 | 퍼플렉서티 302.03\n",
      "| 에폭 1 | 반복 1281 | 시간 293[s] | loss 5.87 | 퍼플렉서티 313.61\n",
      "| 에폭 1 | 반복 1301 | 시간 299[s] | loss 5.93 | 퍼플렉서티 389.63\n",
      "| 에폭 1 | 반복 1321 | 시간 303[s] | loss 5.94 | 퍼플렉서티 356.64\n"
     ]
    }
   ],
   "source": [
    "ppl_list = []\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "start_time = time.time()\n",
    "model.time_idx = 0\n",
    "for epoch in range(max_epoch):\n",
    "    for iters in range(max_iters):\n",
    "        batch_x, batch_t = model.get_batch(xs, ts, batch_size, time_size)\n",
    "\n",
    "        y, loss = model.forward(batch_x, batch_t)\n",
    "        model.backward(batch_t)\n",
    "        model.clip_grads(max_grad)\n",
    "        model.update(lr)\n",
    "        \n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "        if ((iters % eval_interval) == 0):\n",
    "            ppl = np.exp(total_loss / loss_count)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('| 에폭 %d | 반복 %d | 시간 %d[s] | loss %.2f | 퍼플렉서티 %.2f'\n",
    "                % (epoch + 1, iters + 1, elapsed_time, loss, ppl))\n",
    "            ppl_list.append(float(ppl))\n",
    "            total_loss, loss_count = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e10200c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = np.arange(len(ppl_list))\n",
    "plt.plot(p, ppl_list, label='train')\n",
    "plt.xlabel('iters (x' + str(eval_interval) + ')')\n",
    "plt.ylabel('ppl')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
