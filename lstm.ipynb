{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "901b4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from dataset import ptb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5923d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstmlm:\n",
    "    \n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        \n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed = (rn(V, D) / 100).astype('f')\n",
    "        lstmWx = rn(D, H*4) / np.sqrt(D)\n",
    "        lstmWh = rn(H, H*4) / np.sqrt(H)\n",
    "        lstmb = np.zeros((H*4), dtype='f')\n",
    "        affineW = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affineb = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.params = [embed, lstmWx, lstmWh, lstmb, affineW, affineb]\n",
    "        self.grads = []\n",
    "        self.lstm = []\n",
    "        \n",
    "        self.wordvec_size = wordvec_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.time_idx = 0\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        embed, lstmWx, lstmWh, lstmb, affineW, affineb = self.params\n",
    "                \n",
    "        batch_size, time_size = x.shape\n",
    "        wordvec_size = self.wordvec_size\n",
    "        hidden_size = self.hidden_size\n",
    "        \n",
    "        self.lstm = []\n",
    "        \n",
    "        h_prev = np.zeros((batch_size, hidden_size), dtype='f')\n",
    "        c_prev = np.zeros((batch_size, hidden_size), dtype='f')\n",
    "        hs = np.empty((batch_size, time_size, hidden_size), dtype='f')\n",
    "        for t in range(time_size):\n",
    "            # embed\n",
    "            emb_out = embed[x[:, t]]\n",
    "            \n",
    "            # lstm\n",
    "            A = np.matmul(emb_out, lstmWx) + np.matmul(h_prev, lstmWh) + lstmb\n",
    "            f = A[:, :hidden_size]\n",
    "            g = A[:, hidden_size: 2*hidden_size]\n",
    "            i = A[:, 2*hidden_size: 3*hidden_size]\n",
    "            o = A[:, 3*hidden_size:]\n",
    "\n",
    "            f = self.sigmoid(f)\n",
    "            g = np.tanh(g)\n",
    "            i = self.sigmoid(i)\n",
    "            o = self.sigmoid(o)\n",
    "\n",
    "            c_next = f * c_prev + g * i\n",
    "            h_next = o * np.tanh(c_next)\n",
    "            \n",
    "            hs[:, t, :] = h_next\n",
    "            \n",
    "            self.lstm.append((emb_out, h_prev, c_prev, f, g, i, o, c_next))\n",
    "            c_prev = c_next\n",
    "            h_prev = h_next\n",
    "            \n",
    "        \n",
    "        # affine\n",
    "        affine_out = np.matmul(hs, affineW) + affineb\n",
    "        \n",
    "        # softmax\n",
    "        y = self.softmax(affine_out)\n",
    "        \n",
    "        loss = self.getLoss(y, batch_t)\n",
    "        self.xs = x, hs, affine_out, y\n",
    "\n",
    "        return y, loss\n",
    "    \n",
    "    def backward(self, t):\n",
    "        embed, lstmWx, lstmWh, lstmb, affineW, affineb = self.params\n",
    "        x, hs, affine_out, y = self.xs\n",
    "        \n",
    "        wordvec_size = self.wordvec_size\n",
    "        vocab_size = self.vocab_size\n",
    "        batch_size, time_size = x.shape\n",
    "        \n",
    "        # softmax\n",
    "        y = y.reshape(batch_size * time_size, -1)\n",
    "        t = t.reshape(batch_size * time_size)\n",
    "        y[np.arange(batch_size * time_size), t] -= 1\n",
    "        soft_dout = y\n",
    "        \n",
    "        # affine\n",
    "        affine_dout = np.matmul(soft_dout, affineW.T).reshape(batch_size, time_size, -1) # (b, t, h)\n",
    "        affinedW = np.matmul(hs.reshape(batch_size * time_size, -1).T, soft_dout) # (h, v)\n",
    "        affinedb = np.sum(soft_dout, axis=0)\n",
    "        \n",
    "        # lstm\n",
    "        lstmdWx = np.zeros_like(lstmWx)\n",
    "        lstmdWh = np.zeros_like(lstmWh)\n",
    "        lstmdb = np.zeros_like(lstmb)\n",
    "        \n",
    "        lstm_douts = np.empty((batch_size, time_size, wordvec_size), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "        for t in reversed(range(time_size)):            \n",
    "            emb_out, h_prev, c_prev, f, g, i, o, c_next = self.lstm[t]\n",
    "            dh_next = affine_dout[:, t, :] + dh\n",
    "            dc_next = dc\n",
    "\n",
    "            tanh_c_next = np.tanh(c_next)\n",
    "            \n",
    "            ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "            \n",
    "            dc_prev = ds * f\n",
    "            \n",
    "            di = ds * g\n",
    "            df = ds * c_prev\n",
    "            do = dh_next * tanh_c_next\n",
    "            dg = ds * i\n",
    "            \n",
    "            di *= i * (1 - i)\n",
    "            df *= f * (1 - f)\n",
    "            do *= o * (1 - o)\n",
    "            dg *= (1 - g ** 2)\n",
    "            \n",
    "            dA = np.hstack((df, dg, di, do))\n",
    "            \n",
    "            dWh = np.matmul(h_prev.T, dA)\n",
    "            dWx = np.matmul(emb_out.T, dA)\n",
    "            db = np.sum(dA, axis=0)\n",
    "            \n",
    "            lstm_douts[:, t, :] = np.matmul(dA, lstmWx.T)\n",
    "            dh_prev = np.matmul(dA, lstmWh.T)\n",
    "        \n",
    "            lstmdWx += dWx\n",
    "            lstmdWh += dWh\n",
    "            lstmdb += db\n",
    "            dh = dh_prev\n",
    "            dc = dc_prev\n",
    "        \n",
    "        # embed\n",
    "        embed_dout = np.zeros_like(embed)\n",
    "        for t in range(time_size):\n",
    "            np.add.at(embed_dout, x[:, t], lstm_douts[:, t, :])\n",
    "        \n",
    "        self.grads = embed_dout, lstmdWx, lstmdWh, lstmdb, affinedW, affinedb\n",
    "        \n",
    "        \n",
    "    def softmax(self, y):\n",
    "        y = y - np.max(y)\n",
    "        y = np.exp(y)\n",
    "        y = y / y.sum(axis=2, keepdims=True)\n",
    "        return y\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def update(self, lr):\n",
    "        for i in range(len(self.params)):\n",
    "            self.params[i] -= self.grads[i] * lr      \n",
    "            \n",
    "    def getLoss(self, y, t):\n",
    "        N, T, V = y.shape\n",
    "\n",
    "        y = y.reshape(N * T, V)\n",
    "        t = t.reshape(N * T)\n",
    "\n",
    "        ls = np.log(y[np.arange(N * T), t])\n",
    "        return -np.sum(ls) / (N * T)\n",
    "    \n",
    "    def get_batch(self, x, t, batch_size, time_size):\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "\n",
    "        data_size = len(x)\n",
    "        jump = data_size // batch_size\n",
    "        offsets = [i * jump for i in range(batch_size)]  # 배치에서 각 샘플을 읽기 시작하는 위치\n",
    "\n",
    "        for time in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, time] = x[(offset + self.time_idx) % data_size]\n",
    "                batch_t[i, time] = t[(offset + self.time_idx) % data_size]\n",
    "            self.time_idx += 1\n",
    "        return batch_x, batch_t\n",
    "    \n",
    "    def clip_grads(self, max_norm):\n",
    "        grads = self.grads\n",
    "        total_norm = 0\n",
    "        \n",
    "        for grad in grads:\n",
    "            total_norm += np.sum(grad**2)\n",
    "            \n",
    "        total_norm = np.sqrt(total_norm)\n",
    "        \n",
    "        rate = max_norm / (total_norm + 1e-6)\n",
    "        \n",
    "        if rate  < 1:\n",
    "            for grad in grads:\n",
    "                grad *= rate\n",
    "        self.grads = grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b52e1b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]  # 입력\n",
    "ts = corpus[1:]  # 출력（정답 레이블）\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 20\n",
    "wordvec_size = 100\n",
    "hidden_size = 100  # LSTM의 은닉 상태 벡터의 원소 수\n",
    "time_size = 35  # LSTM을 펼치는 크기\n",
    "lr = 20.0\n",
    "max_epoch = 1\n",
    "max_iters = len(xs) // (batch_size * time_size)\n",
    "eval_interval = 20\n",
    "max_grad = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b15a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lstmlm(vocab_size, wordvec_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2cd95d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 | 반복 1 | 시간 0[s] | loss 9.21 | 퍼플렉서티 9999.42\n",
      "| 에폭 1 | 반복 21 | 시간 2[s] | loss 7.09 | 퍼플렉서티 2460.71\n",
      "| 에폭 1 | 반복 41 | 시간 4[s] | loss 7.12 | 퍼플렉서티 1277.62\n",
      "| 에폭 1 | 반복 61 | 시간 7[s] | loss 6.86 | 퍼플렉서티 910.46\n",
      "| 에폭 1 | 반복 81 | 시간 9[s] | loss 6.34 | 퍼플렉서티 711.71\n",
      "| 에폭 1 | 반복 101 | 시간 11[s] | loss 6.18 | 퍼플렉서티 585.24\n",
      "| 에폭 1 | 반복 121 | 시간 14[s] | loss 6.28 | 퍼플렉서티 577.29\n",
      "| 에폭 1 | 반복 141 | 시간 16[s] | loss 6.15 | 퍼플렉서티 530.14\n",
      "| 에폭 1 | 반복 161 | 시간 19[s] | loss 6.11 | 퍼플렉서티 510.38\n",
      "| 에폭 1 | 반복 181 | 시간 22[s] | loss 6.28 | 퍼플렉서티 518.21\n",
      "| 에폭 1 | 반복 201 | 시간 24[s] | loss 5.98 | 퍼플렉서티 437.25\n",
      "| 에폭 1 | 반복 221 | 시간 27[s] | loss 6.05 | 퍼플렉서티 425.42\n",
      "| 에폭 1 | 반복 241 | 시간 29[s] | loss 5.93 | 퍼플렉서티 389.60\n",
      "| 에폭 1 | 반복 261 | 시간 32[s] | loss 5.95 | 퍼플렉서티 400.14\n",
      "| 에폭 1 | 반복 281 | 시간 35[s] | loss 5.68 | 퍼플렉서티 399.52\n",
      "| 에폭 1 | 반복 301 | 시간 37[s] | loss 5.69 | 퍼플렉서티 347.36\n",
      "| 에폭 1 | 반복 321 | 시간 40[s] | loss 5.29 | 퍼플렉서티 301.64\n",
      "| 에폭 1 | 반복 341 | 시간 43[s] | loss 6.13 | 퍼플렉서티 358.54\n",
      "| 에폭 1 | 반복 361 | 시간 46[s] | loss 5.70 | 퍼플렉서티 366.88\n",
      "| 에폭 1 | 반복 381 | 시간 49[s] | loss 5.28 | 퍼플렉서티 293.98\n",
      "| 에폭 1 | 반복 401 | 시간 52[s] | loss 5.92 | 퍼플렉서티 316.22\n",
      "| 에폭 1 | 반복 421 | 시간 54[s] | loss 6.03 | 퍼플렉서티 307.28\n",
      "| 에폭 1 | 반복 441 | 시간 57[s] | loss 5.48 | 퍼플렉서티 298.06\n",
      "| 에폭 1 | 반복 461 | 시간 60[s] | loss 5.96 | 퍼플렉서티 299.75\n",
      "| 에폭 1 | 반복 481 | 시간 63[s] | loss 5.80 | 퍼플렉서티 278.57\n",
      "| 에폭 1 | 반복 501 | 시간 67[s] | loss 5.73 | 퍼플렉서티 292.52\n",
      "| 에폭 1 | 반복 521 | 시간 70[s] | loss 5.50 | 퍼플렉서티 276.79\n",
      "| 에폭 1 | 반복 541 | 시간 74[s] | loss 5.87 | 퍼플렉서티 296.26\n",
      "| 에폭 1 | 반복 561 | 시간 77[s] | loss 5.55 | 퍼플렉서티 260.30\n",
      "| 에폭 1 | 반복 581 | 시간 81[s] | loss 5.69 | 퍼플렉서티 235.67\n",
      "| 에폭 1 | 반복 601 | 시간 85[s] | loss 5.76 | 퍼플렉서티 308.14\n",
      "| 에폭 1 | 반복 621 | 시간 89[s] | loss 5.44 | 퍼플렉서티 284.94\n",
      "| 에폭 1 | 반복 641 | 시간 92[s] | loss 5.42 | 퍼플렉서티 262.02\n",
      "| 에폭 1 | 반복 661 | 시간 96[s] | loss 5.38 | 퍼플렉서티 251.44\n",
      "| 에폭 1 | 반복 681 | 시간 99[s] | loss 5.62 | 퍼플렉서티 208.45\n",
      "| 에폭 1 | 반복 701 | 시간 103[s] | loss 5.49 | 퍼플렉서티 232.35\n",
      "| 에폭 1 | 반복 721 | 시간 107[s] | loss 5.26 | 퍼플렉서티 241.90\n",
      "| 에폭 1 | 반복 741 | 시간 111[s] | loss 5.26 | 퍼플렉서티 206.04\n",
      "| 에폭 1 | 반복 761 | 시간 115[s] | loss 5.33 | 퍼플렉서티 212.94\n",
      "| 에폭 1 | 반복 781 | 시간 118[s] | loss 5.03 | 퍼플렉서티 205.17\n",
      "| 에폭 1 | 반복 801 | 시간 123[s] | loss 5.51 | 퍼플렉서티 229.51\n",
      "| 에폭 1 | 반복 821 | 시간 127[s] | loss 5.49 | 퍼플렉서티 212.02\n",
      "| 에폭 1 | 반복 841 | 시간 130[s] | loss 5.44 | 퍼플렉서티 213.94\n",
      "| 에폭 1 | 반복 861 | 시간 134[s] | loss 5.12 | 퍼플렉서티 212.43\n",
      "| 에폭 1 | 반복 881 | 시간 137[s] | loss 5.07 | 퍼플렉서티 192.81\n",
      "| 에폭 1 | 반복 901 | 시간 141[s] | loss 5.42 | 퍼플렉서티 243.18\n",
      "| 에폭 1 | 반복 921 | 시간 144[s] | loss 5.45 | 퍼플렉서티 218.13\n",
      "| 에폭 1 | 반복 941 | 시간 148[s] | loss 5.33 | 퍼플렉서티 217.13\n",
      "| 에폭 1 | 반복 961 | 시간 151[s] | loss 5.35 | 퍼플렉서티 232.73\n",
      "| 에폭 1 | 반복 981 | 시간 156[s] | loss 5.37 | 퍼플렉서티 219.23\n",
      "| 에폭 1 | 반복 1001 | 시간 159[s] | loss 5.31 | 퍼플렉서티 184.06\n",
      "| 에폭 1 | 반복 1021 | 시간 163[s] | loss 5.25 | 퍼플렉서티 217.27\n",
      "| 에폭 1 | 반복 1041 | 시간 166[s] | loss 4.94 | 퍼플렉서티 201.97\n",
      "| 에폭 1 | 반복 1061 | 시간 169[s] | loss 5.03 | 퍼플렉서티 186.23\n",
      "| 에폭 1 | 반복 1081 | 시간 172[s] | loss 5.20 | 퍼플렉서티 158.43\n",
      "| 에폭 1 | 반복 1101 | 시간 176[s] | loss 5.38 | 퍼플렉서티 179.73\n",
      "| 에폭 1 | 반복 1121 | 시간 179[s] | loss 5.29 | 퍼플렉서티 219.35\n",
      "| 에폭 1 | 반복 1141 | 시간 183[s] | loss 5.42 | 퍼플렉서티 200.41\n",
      "| 에폭 1 | 반복 1161 | 시간 186[s] | loss 5.15 | 퍼플렉서티 189.19\n",
      "| 에폭 1 | 반복 1181 | 시간 189[s] | loss 5.02 | 퍼플렉서티 184.96\n",
      "| 에폭 1 | 반복 1201 | 시간 193[s] | loss 5.13 | 퍼플렉서티 156.36\n",
      "| 에폭 1 | 반복 1221 | 시간 197[s] | loss 5.30 | 퍼플렉서티 152.74\n",
      "| 에폭 1 | 반복 1241 | 시간 200[s] | loss 5.36 | 퍼플렉서티 182.24\n",
      "| 에폭 1 | 반복 1261 | 시간 204[s] | loss 5.04 | 퍼플렉서티 165.50\n",
      "| 에폭 1 | 반복 1281 | 시간 207[s] | loss 5.38 | 퍼플렉서티 171.40\n",
      "| 에폭 1 | 반복 1301 | 시간 210[s] | loss 5.45 | 퍼플렉서티 216.64\n",
      "| 에폭 1 | 반복 1321 | 시간 213[s] | loss 5.39 | 퍼플렉서티 206.57\n"
     ]
    }
   ],
   "source": [
    "ppl_list = []\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "start_time = time.time()\n",
    "model.time_idx = 0\n",
    "for epoch in range(max_epoch):\n",
    "    for iters in range(max_iters):\n",
    "        batch_x, batch_t = model.get_batch(xs, ts, batch_size, time_size)\n",
    "\n",
    "        y, loss = model.forward(batch_x, batch_t)\n",
    "        model.backward(batch_t)\n",
    "        model.clip_grads(max_grad)\n",
    "        model.update(lr)\n",
    "        \n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "        if ((iters % eval_interval) == 0):\n",
    "            ppl = np.exp(total_loss / loss_count)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('| 에폭 %d | 반복 %d | 시간 %d[s] | loss %.2f | 퍼플렉서티 %.2f'\n",
    "                % (epoch + 1, iters + 1, elapsed_time, loss, ppl))\n",
    "            ppl_list.append(float(ppl))\n",
    "            total_loss, loss_count = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10200c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = np.arange(len(ppl_list))\n",
    "plt.plot(p, ppl_list, label='train')\n",
    "plt.xlabel('iters (x' + str(eval_interval) + ')')\n",
    "plt.ylabel('ppl')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
